,QuestionID,Question Text,SentenceID,Sentence,ToneSummary,JoyToneID,JoyScore,SadnessToneID,SadnessScore,FearToneID,FearScore,AngerToneID,AngerScore,AnalyticalToneID,AnalyticalScore,ConfidentToneID,ConfidentScore,TentativeToneID,TentativeScore,NoEmotion,QuestionText,SentenceText
0,45033467,,0,,"[{'score': 0.615354, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.841501, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.615354,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841501,FALSE,0,FALSE,0,FALSE,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""","""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images."
1,45033467,,1,,"[{'score': 0.747163, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747163,FALSE,0,FALSE,0,TRUE,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""","Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.."
2,45033467,,2,,"[{'score': 0.504579, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.504579,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.933436,FALSE,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""","But is there any other ready made solution on any of cloud providers ?"""
3,45546462,,0,,"[{'score': 0.702427, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.702427,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""","""I have problem with access to camera first time."
4,45546462,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""",I installed my app.
5,45546462,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""",Next go to scan to QR Code.
6,45546462,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""",I use to scan QR Code google vision.
7,45546462,,4,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""","App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open."
8,45546462,,5,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""","But I go back activity and go to activity which scan QR Code, camera open.my"
9,45546462,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""",AdnroidManifest.xmlmy
10,45546462,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""","class"""
11,49619897,,0,,"[{'score': 0.727113, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727113,FALSE,0,FALSE,0,TRUE,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""","""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a."
12,49619897,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""","Here is the DAG:But, the work step is throwing an exception."
13,49619897,,2,,"[{'score': 0.554483, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.592648, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.554483,FALSE,0,FALSE,0,TRUE,0.592648,FALSE,0,FALSE,0,FALSE,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""","Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong?"
14,49619897,,3,,"[{'score': 0.624599, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.624599,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,FALSE,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""",Maybe there is a problem withused in the.
15,49619897,,4,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.948146, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948146,FALSE,0,TRUE,0.647986,TRUE,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""","Or, dataFrame is not the way to go in order to handle query results.PS: result of"""
16,55388663,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""","""I try to use Google Cloud Vision API to detect text of an image."
17,55388663,,1,,"[{'score': 0.52908, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.52908,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,FALSE,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""","After detecting, I get 1 page and 17 Blocks."
18,55388663,,2,,"[{'score': 0.67159, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.67159,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,FALSE,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""","I am trying to get text in each blocks and save it in a list, but it does not work."
19,55388663,,3,,"[{'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,TRUE,0.775166,TRUE,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""",Here is my code:I would like to know is there any other way to get text.
20,55388663,,4,,"[{'score': 0.808709, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.808709,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,FALSE,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""","Thanks a lot."""
21,52833231,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How can i write a test case in Junit for amazon Rekognition.For the above program I wanted to write a Junit testcase. Kindly help me with the same""","""How can i write a test case in Junit for amazon Rekognition.For the above program I wanted to write a Junit testcase."
22,52833231,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How can i write a test case in Junit for amazon Rekognition.For the above program I wanted to write a Junit testcase. Kindly help me with the same""","Kindly help me with the same"""
23,41895608,,0,,"[{'score': 0.50109, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.50109,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""","""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The"
24,41895608,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""",type '' was not found.
25,41895608,,2,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,TRUE,0.80026,FALSE,0,TRUE,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""",Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The
26,41895608,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""",tag '' does not exist in XML namespace ''.
27,41895608,,4,,"[{'score': 0.87913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87913,FALSE,0,FALSE,0,TRUE,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""","Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded."""
28,55122015,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I am trying to use aws rekognition to compare faces but it will give an error saying check if the object and bucket exist in same regionwhile uploading the image i have set the content type to image/jpeg formatbut when i upload an image using aws console from computer the rekognition will work ! am i doing something wrong in this code""","""I am trying to use aws rekognition to compare faces but it will give an error saying check if the object and bucket exist in same regionwhile uploading the image i have set the content type to image/jpeg formatbut when i upload an image using aws console from computer the rekognition will work !"
29,55122015,,1,,"[{'score': 0.642926, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.642926,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,FALSE,"""I am trying to use aws rekognition to compare faces but it will give an error saying check if the object and bucket exist in same regionwhile uploading the image i have set the content type to image/jpeg formatbut when i upload an image using aws console from computer the rekognition will work ! am i doing something wrong in this code""","am i doing something wrong in this code"""
30,50145372,,0,,"[{'score': 0.624674, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.798791, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.624674,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.798791,FALSE,"""I'm pretty new to the google vision api and I am trying to make arequest.Currently I am reading from an image file, encoding it toand trying to pass it on to the requestBut I am getting aerror. Could anyone spot the error in the code?.""","""I'm pretty new to the google vision api and I am trying to make arequest.Currently"
31,50145372,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm pretty new to the google vision api and I am trying to make arequest.Currently I am reading from an image file, encoding it toand trying to pass it on to the requestBut I am getting aerror. Could anyone spot the error in the code?.""","I am reading from an image file, encoding it toand trying to pass it on to the requestBut I am getting aerror."
32,50145372,,2,,"[{'score': 0.725776, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.610552, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.725776,FALSE,0,FALSE,0,TRUE,0.610552,FALSE,0,TRUE,0.984352,FALSE,"""I'm pretty new to the google vision api and I am trying to make arequest.Currently I am reading from an image file, encoding it toand trying to pass it on to the requestBut I am getting aerror. Could anyone spot the error in the code?.""","Could anyone spot the error in the code?."""
33,56094441,,0,,"[{'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""","""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms."
34,56094441,,1,,"[{'score': 0.678385, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.678385,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""","The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python."
35,56094441,,2,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.940414, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.940414,FALSE,0,TRUE,0.525007,TRUE,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""","For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not."
36,56094441,,3,,"[{'score': 0.765293, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765293,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""","I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?"""
37,53815181,,0,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""","""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer."
38,53815181,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""",I have already set up aws cli.
39,53815181,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""",Here is the code.
40,53815181,,3,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""",My image is in some bucket 'bx' with name 's.jpg'.
41,53815181,,4,,"[{'score': 0.707102, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.707102,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""","Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :"""
42,54683853,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""","""On google cloud vision you get charged per request."
43,54683853,,1,,"[{'score': 0.776575, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.776575,FALSE,0,FALSE,0,TRUE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""","If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request."
44,54683853,,2,,"[{'score': 0.744019, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.816877, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.816877,FALSE,0,TRUE,0.744019,TRUE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP?
45,54683853,,3,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.989022, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.989022,FALSE,0,TRUE,0.968123,TRUE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",Any insight would be appreciated.
46,54683853,,4,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",Thanks.#########
47,54683853,,5,,"[{'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,TRUE,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""","Safe Search would look as follows"""
48,54122545,,0,,"[{'score': 0.609136, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.609136,FALSE,0,TRUE,0.681699,TRUE,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""","""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId."
49,54122545,,1,,"[{'score': 0.560988, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5687, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.560988,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5687,FALSE,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""","I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue."""
50,56065772,,0,,"[{'score': 0.786249, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786249,FALSE,0,FALSE,0,TRUE,"""I want to extract handwritten text from an application form using Google Vision API's text detection feature.  It greatly extracts the handwritten text but gives very unorganized JSON type response, which I don't know how to parse because I want to extract only the specific fields like name, contact number, email, etc. and store them into MySQL database.Code ():Response from API:""","""I want to extract handwritten text from an application form using Google Vision API's text detection feature."
51,56065772,,1,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I want to extract handwritten text from an application form using Google Vision API's text detection feature.  It greatly extracts the handwritten text but gives very unorganized JSON type response, which I don't know how to parse because I want to extract only the specific fields like name, contact number, email, etc. and store them into MySQL database.Code ():Response from API:""","It greatly extracts the handwritten text but gives very unorganized JSON type response, which I don't know how to parse because I want to extract only the specific fields like name, contact number, email, etc. and store them into MySQL database.Code ():Response from API:"""
52,50133223,,0,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""","""We are using Google Cloud Vision APIs to extract Invoice fields."
53,50133223,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""",We would like to know whether the APIs support detection of table of data?
54,50133223,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""","Or do we have to write custom code to detect tables?"""
55,38811303,,0,,"[{'score': 0.612596, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.77632, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.638545, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.612596,FALSE,0,FALSE,0,TRUE,0.77632,FALSE,0,TRUE,0.638545,FALSE,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""","""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here."
56,38811303,,1,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""","Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be"""
57,56285629,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am usingto detect Japanese texts in the image. The response from Google contains texts like this: ""text"": ""\u5065\u5eb7\u4fdd\u967a."" I don't know which ""encoder"" Google is using for encoding japanese texts, UTF-8 or Unicode?""","""I am usingto detect Japanese texts in the image."
58,56285629,,1,,"[{'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.743682, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743682,FALSE,0,TRUE,0.534455,TRUE,"""I am usingto detect Japanese texts in the image. The response from Google contains texts like this: ""text"": ""\u5065\u5eb7\u4fdd\u967a."" I don't know which ""encoder"" Google is using for encoding japanese texts, UTF-8 or Unicode?""","The response from Google contains texts like this: ""text"": ""\u5065\u5eb7\u4fdd\u967a."" I don't know which ""encoder"" Google is using for encoding japanese texts, UTF-8 or Unicode?"""
59,55590797,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android? Ifyes, what is it and how to implement it? And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False.""","""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia."
60,55590797,,1,,"[{'score': 0.780431, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.780431,FALSE,0,FALSE,0,TRUE,"""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android? Ifyes, what is it and how to implement it? And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False.""","In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android?"
61,55590797,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android? Ifyes, what is it and how to implement it? And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False.""","Ifyes, what is it and how to implement it?"
62,55590797,,3,,"[{'score': 0.542172, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.865281, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.542172,FALSE,0,FALSE,0,FALSE,0,TRUE,0.865281,FALSE,0,FALSE,0,FALSE,"""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android? Ifyes, what is it and how to implement it? And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False.""","And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False."""
63,55685353,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.838593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.838593,FALSE,0,TRUE,0.681699,TRUE,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""","""I'm trying to evaluate Google vision endpoint."
64,55685353,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""",my pom is configured like belowThere are no other google dependency added.
65,55685353,,2,,"[{'score': 0.627175, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.627175,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.845297,FALSE,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""",I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions.
66,55685353,,3,,"[{'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,FALSE,0,TRUE,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""","but got no idea which one to use and how to fix dependency issues within the same jar."""
67,48670839,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)""","""I'm trying to use the Google Vision API in C# for an image with text on multiple lines."
68,48670839,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)""","I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately?"
69,48670839,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)""","Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)"""
70,51336137,,0,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""","""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But"
71,51336137,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""",I can't figure out how to start the stream processor?
72,51336137,,2,,"[{'score': 0.767592, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,TRUE,0.767592,FALSE,0,TRUE,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""",Do I have to simply write the main method and callfunction?
73,51336137,,3,,"[{'score': 0.973144, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.973144,TRUE,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""","Or do I have to do something else: like the guide mentioned something as?"""
74,49575082,,0,,"[{'score': 0.695447, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,TRUE,0.695447,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition."
75,49575082,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition."
76,49575082,,2,,"[{'score': 0.821444, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821444,FALSE,0,FALSE,0,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","However, I'm trying to find out a way to stream live data into Rekognition."
77,49575082,,3,,"[{'score': 0.76423, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.76423,FALSE,0,FALSE,0,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this."
78,49575082,,4,,"[{'score': 0.740021, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.740021,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,FALSE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition."
79,49575082,,5,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer.
80,49575082,,6,,"[{'score': 0.677676, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677676,FALSE,0,FALSE,0,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis?"
81,49575082,,7,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming."
82,49575082,,8,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.765293, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765293,FALSE,0,TRUE,0.946222,TRUE,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""","But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated."""
83,49520402,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""","""I have a c# WinForms project with a picture box that contains a document with text."
84,49520402,,1,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""","I am gathering the OCR data for the document using the Google Cloud Vision API, which works great."
85,49520402,,2,,"[{'score': 0.709662, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.709662,FALSE,0,FALSE,0,TRUE,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""","Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word."
86,49520402,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""","What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it."""
87,47991101,,0,,"[{'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.909883,TRUE,"""Am trying to OCR around 50,000 documents(~20K size jpegs). Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracyBased on quotas published atmy calculation is600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.Has anyone done any bulk operations using google vision? We did some POCS where some of these bulk requests are taking ~5mins.""","""Am trying to OCR around 50,000 documents(~20K size jpegs)."
88,47991101,,1,,"[{'score': 0.701324, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.701324,FALSE,0,FALSE,0,TRUE,"""Am trying to OCR around 50,000 documents(~20K size jpegs). Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracyBased on quotas published atmy calculation is600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.Has anyone done any bulk operations using google vision? We did some POCS where some of these bulk requests are taking ~5mins.""",Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracyBased on quotas published atmy calculation is600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.Has anyone done any bulk operations using google vision?
89,47991101,,2,,"[{'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.933436,TRUE,"""Am trying to OCR around 50,000 documents(~20K size jpegs). Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracyBased on quotas published atmy calculation is600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.Has anyone done any bulk operations using google vision? We did some POCS where some of these bulk requests are taking ~5mins.""","We did some POCS where some of these bulk requests are taking ~5mins."""
90,52169264,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?""","""I'm having trouble saving the output given by the Google Vision API."
91,52169264,,1,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?""",I'm using Python and testing with a demo image.
92,52169264,,2,,"[{'score': 0.697089, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.869502, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.716804, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.697089,FALSE,0,FALSE,0,TRUE,0.716804,FALSE,0,TRUE,0.869502,FALSE,"""I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?""","I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it."
93,52169264,,3,,"[{'score': 0.999641, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999641,TRUE,"""I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?""","Anyone have any suggestions?"""
94,40921512,,0,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659112,TRUE,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""","""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something."
95,40921512,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""","It works but not as I expected, it isn't accurate at all."
96,40921512,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""","The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api?"
97,40921512,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""","what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:"""
98,47982254,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained."
99,47982254,,1,,"[{'score': 0.783676, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.783676,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received."
100,47982254,,2,,"[{'score': 0.881441, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.881441,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,FALSE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","Unable to parse.""."
101,47982254,,3,,"[{'score': 0.846863, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.846863,FALSE,0,FALSE,0,TRUE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier."
102,47982254,,4,,"[{'score': 0.914477, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.914477,FALSE,0,FALSE,0,TRUE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page."
103,47982254,,5,,"[{'score': 0.758593, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.758593,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,FALSE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""",However I still get the same error.
104,47982254,,6,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.898858, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.898858,FALSE,0,TRUE,0.525007,TRUE,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""","Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:"""
105,55606918,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""I am using below code to connect to Google Vision API. I have JSON from Google Vision.The code is giving me below error . Not Sure why..Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details.""","""I am using below code to connect to Google Vision API."
106,55606918,,1,,"[{'score': 0.659248, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.659248,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using below code to connect to Google Vision API. I have JSON from Google Vision.The code is giving me below error . Not Sure why..Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details.""",I have JSON from Google Vision.The code is giving me below error .
107,55606918,,2,,"[{'score': 0.980517, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.980517,TRUE,"""I am using below code to connect to Google Vision API. I have JSON from Google Vision.The code is giving me below error . Not Sure why..Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details.""",Not Sure why..Please suggest..
108,55606918,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using below code to connect to Google Vision API. I have JSON from Google Vision.The code is giving me below error . Not Sure why..Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details.""","It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details."""
109,52829583,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""","""I was trying to make the google vision OCR regex searchable."
110,52829583,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""",I have completed it and works pretty well when the document contains only English characters.
111,52829583,,2,,"[{'score': 0.565628, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.565628,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""","But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above."
112,52829583,,3,,"[{'score': 0.512243, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.805724, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.512243,FALSE,0,FALSE,0,TRUE,0.805724,FALSE,0,TRUE,0.511119,FALSE,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""","Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option."""
113,45095486,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have downloaded the google vision api from. And I tried Barcode detector example, When I try to scan a linear and 2D barcodes, the scanned area(purple shape) shows in wrong location on preview layer.Note: This issue only occurs when I hold the device horizontally at the top of barcode.Herewith I have attached the screenshot which reflects this issue.Thank you!""","""I have downloaded the google vision api from."
114,45095486,,1,,"[{'score': 0.775204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775204,FALSE,0,FALSE,0,TRUE,"""I have downloaded the google vision api from. And I tried Barcode detector example, When I try to scan a linear and 2D barcodes, the scanned area(purple shape) shows in wrong location on preview layer.Note: This issue only occurs when I hold the device horizontally at the top of barcode.Herewith I have attached the screenshot which reflects this issue.Thank you!""","And I tried Barcode detector example, When I try to scan a linear and 2D barcodes, the scanned area(purple shape) shows in wrong location on preview layer.Note: This issue only occurs when I hold the device horizontally at the top of barcode.Herewith I have attached the screenshot which reflects this issue.Thank you!"""
115,41285556,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""","""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image."
116,41285556,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,TRUE,0.751512,FALSE,0,TRUE,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",It still can return recognized text correctly.
117,41285556,,2,,"[{'score': 0.864115, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.864115,FALSE,0,FALSE,0,TRUE,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""","(see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation."
118,41285556,,3,,"[{'score': 0.971904, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.971904,FALSE,0,TRUE,0.727988,TRUE,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""","(document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)"""
119,55330723,,0,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","""I have a question for azure custom vision."
120,55330723,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",I have a custom vision project for object detection.
121,55330723,,2,,"[{'score': 0.651999, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.651999,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",And I use the python SDK to create the project (see that:).
122,55330723,,3,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,TRUE,0.822231,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",But I found something wrong in the process of uploading.
123,55330723,,4,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","For example, there is a picture that has 3 persons in this picture."
124,55330723,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",So I tag 3 same class  person  in this picture.
125,55330723,,6,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website."
126,55330723,,7,,"[{'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture."
127,55330723,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg."
128,55330723,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg"
129,55330723,,10,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",finally.
130,55330723,,11,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.856622,TRUE,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""","Is there anything solution that can solve this problem?"""
131,45122248,,0,,"[{'score': 0.5232, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5232,FALSE,0,FALSE,0,TRUE,"""I was using ruby client of Google Cloud Vision, to extract the vehicle information on Automobile Original Titles.Observations:When I used the client API, i was getting 171 words.But, when I used the google's API demo here:, I got 459 words. It has much of the information I was looking for.Can anyone please explain, how to get the most out of the API ?""","""I was using ruby client of Google Cloud Vision, to extract the vehicle information on Automobile Original Titles.Observations:When I used the client API, i was getting 171 words.But, when I used the google's API demo here:, I got 459 words."
132,45122248,,1,,"[{'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.63698, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,TRUE,0.63698,TRUE,"""I was using ruby client of Google Cloud Vision, to extract the vehicle information on Automobile Original Titles.Observations:When I used the client API, i was getting 171 words.But, when I used the google's API demo here:, I got 459 words. It has much of the information I was looking for.Can anyone please explain, how to get the most out of the API ?""","It has much of the information I was looking for.Can anyone please explain, how to get the most out of the API ?"""
133,49558215,,0,,"[{'score': 0.610082, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.610082,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""","""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 ."
134,49558215,,1,,"[{'score': 0.583315, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583315,FALSE,0,FALSE,0,TRUE,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""","But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'?"
135,49558215,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""","Currently I am passing the externalImageId through my java code like :Above code internally calls :"""
136,46516494,,0,,"[{'score': 0.978988, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.978988,FALSE,0,FALSE,0,TRUE,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""","""I know this is a frequently asked question, but I couldn't find an answer."
137,46516494,,1,,"[{'score': 0.538553, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.799071, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.538553,FALSE,0,FALSE,0,TRUE,0.799071,FALSE,0,FALSE,0,FALSE,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""","I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace."
138,46516494,,2,,"[{'score': 0.645185, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.743365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.645185,FALSE,0,FALSE,0,TRUE,0.743365,FALSE,0,FALSE,0,FALSE,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""","I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code."
139,46516494,,3,,"[{'score': 0.8766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8766,FALSE,0,FALSE,0,TRUE,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""","I don't know where else I'm using it that it is creating this issue.Here is my code:"""
140,50190527,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""","""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below."
141,50190527,,1,,"[{'score': 0.772709, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.772709,FALSE,0,FALSE,0,TRUE,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""","But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help."""
142,54634576,,0,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,TRUE,0.615352,TRUE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""","""To start, I'm quite inexperienced with APIs in general."
143,54634576,,1,,"[{'score': 0.74948, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.74948,FALSE,0,FALSE,0,TRUE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google.
144,54634576,,2,,"[{'score': 0.65296, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.65296,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874372,FALSE,0,FALSE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",I've built the project usingand it all works fine.
145,54634576,,3,,"[{'score': 0.621837, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.591614, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.621837,FALSE,0,FALSE,0,TRUE,0.591614,FALSE,0,FALSE,0,FALSE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""","However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all."
146,54634576,,4,,"[{'score': 0.864999, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.864999,TRUE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""","I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it."
147,54634576,,5,,"[{'score': 0.81876, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.81876,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,FALSE,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""","I really don't know what to do here, extremely thankful for any pointers."""
148,55665919,,0,,"[{'score': 0.712696, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.712696,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error. I have no idea what I am doing wrong.I tried to make this request via Postman and everything is working fine there but in c# it just won't work.Can you help me please?The C# request body looks like this:Whereas the Postman request body looks like this:""","""I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error."
149,55665919,,1,,"[{'score': 0.583146, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.583146,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,FALSE,0,FALSE,"""I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error. I have no idea what I am doing wrong.I tried to make this request via Postman and everything is working fine there but in c# it just won't work.Can you help me please?The C# request body looks like this:Whereas the Postman request body looks like this:""","I have no idea what I am doing wrong.I tried to make this request via Postman and everything is working fine there but in c# it just won't work.Can you help me please?The C# request body looks like this:Whereas the Postman request body looks like this:"""
150,55793092,,0,,"[{'score': 0.802462, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802462,FALSE,0,FALSE,0,TRUE,"""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.""","""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API."
151,55793092,,1,,"[{'score': 0.796123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.796123,FALSE,0,FALSE,0,TRUE,"""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.""",So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app.
152,55793092,,2,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659112,TRUE,"""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.""",It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app?
153,55793092,,3,,"[{'score': 0.606999, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.606999,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.856622,FALSE,"""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.""","I haven't found anything on the internet since yesterday."""
154,55276425,,0,,"[{'score': 0.561818, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.561818,FALSE,0,FALSE,0,TRUE,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""","""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good."
155,55276425,,1,,"[{'score': 0.718223, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.619651, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.718223,FALSE,0,FALSE,0,TRUE,0.619651,FALSE,0,FALSE,0,FALSE,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""","It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?"""
156,49732951,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I am testing few sample code for OCR using Google Cloud Vision API. I observed the APIs can able to detectEnglishlanguage very easily from an Image but  in other language likeHindi, the APIs are not able to detect.MyCode :Image :But the same image, i have tried in Google Drive, all the text from the image is easily detected.Please let me know, same image how can i use in the code to detect the text?""","""I am testing few sample code for OCR using Google Cloud Vision API."
157,49732951,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am testing few sample code for OCR using Google Cloud Vision API. I observed the APIs can able to detectEnglishlanguage very easily from an Image but  in other language likeHindi, the APIs are not able to detect.MyCode :Image :But the same image, i have tried in Google Drive, all the text from the image is easily detected.Please let me know, same image how can i use in the code to detect the text?""","I observed the APIs can able to detectEnglishlanguage very easily from an Image but  in other language likeHindi, the APIs are not able to detect.MyCode :Image :But the same image, i have tried in Google Drive, all the text from the image is easily detected.Please let me know, same image how can i use in the code to detect the text?"""
158,52848248,,0,,"[{'score': 0.596608, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.596608,FALSE,0,FALSE,0,TRUE,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""","""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image."
159,52848248,,1,,"[{'score': 0.676232, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.676232,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,FALSE,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""",but I am getting SparseArray of size 0. what can be the problem.
160,52848248,,2,,"[{'score': 0.570203, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.570203,FALSE,0,FALSE,0,TRUE,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""","Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array."""
161,45720587,,0,,"[{'score': 0.522484, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.522484,FALSE,0,FALSE,0,TRUE,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""","""I'm running app engine locally, with the Google vision API."
162,45720587,,1,,"[{'score': 0.707813, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.707813,FALSE,0,FALSE,0,TRUE,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",I'm using the application default credentials for OAuth and building the API client to do label detection.
163,45720587,,2,,"[{'score': 0.665056, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.665056,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,FALSE,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""","Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it."
164,45720587,,3,,"[{'score': 0.808132, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.808132,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""","When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?"
165,45720587,,4,,"[{'score': 0.571545, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.81197, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.571545,FALSE,0,FALSE,0,TRUE,0.81197,FALSE,0,FALSE,0,FALSE,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""","I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that."""
166,47436472,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am using the Socket IO-Swift library in my current project. After installing the Socket.IO-Client-Swift pod file into my project, my FMDB library path is not found, showing this error:I'm using the following pods in my podfile:""","""I am using the Socket IO-Swift library in my current project."
167,47436472,,1,,"[{'score': 0.606284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.606284,FALSE,0,FALSE,0,TRUE,"""I am using the Socket IO-Swift library in my current project. After installing the Socket.IO-Client-Swift pod file into my project, my FMDB library path is not found, showing this error:I'm using the following pods in my podfile:""","After installing the Socket.IO-Client-Swift pod file into my project, my FMDB library path is not found, showing this error:I'm using the following pods in my podfile:"""
168,48840806,,0,,"[{'score': 0.69934, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69934,FALSE,0,FALSE,0,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","""I am trying to implement face detection using Google Mobile Vision in an Android app."
169,48840806,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face."
170,48840806,,2,,"[{'score': 0.507073, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.507073,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.845297,FALSE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","After detecting the face, I am trying to draw a rectangle around the face."
171,48840806,,3,,"[{'score': 0.561231, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.64898, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.796123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.561231,FALSE,0,FALSE,0,TRUE,0.796123,FALSE,0,TRUE,0.64898,FALSE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face."
172,48840806,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",Here is the code in my MainActivity.java
173,48840806,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully""."
174,48840806,,6,,"[{'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849827,FALSE,0,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","So, I am assuming that all my code is running."
175,48840806,,7,,"[{'score': 0.779778, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.779778,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",But I am unable to see the rectangle.Please help me with displaying a rectangle first.
176,48840806,,8,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""","Later I will adjust it to display around the face."""
177,50331196,,0,,"[{'score': 0.72085, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.72085,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""","""In my Java project I'm using Google Cloud Vision API to extract text from images."
178,50331196,,1,,"[{'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713028,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""","For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size."
179,50331196,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""","Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL."
180,50331196,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""",Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage.
181,50331196,,4,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""",For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response.
182,50331196,,5,,"[{'score': 0.811344, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.811344,FALSE,0,FALSE,0,TRUE,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""","I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see)."""
183,46380748,,0,,"[{'score': 0.56798, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.56798,FALSE,0,FALSE,0,TRUE,0.713028,FALSE,0,FALSE,0,FALSE,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""","""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot."
184,46380748,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""","So, it should scan the bill left-to-right downwards the image."
185,46380748,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""","I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left."
186,46380748,,3,,"[{'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.698634, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.698634,TRUE,0.704642,FALSE,0,TRUE,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""","My question is, how do I set the hints, so the detection always has the direction?"""
187,40103531,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""","""I am using React Native's Image Picker component to capture images on my app."
188,40103531,,1,,"[{'score': 0.698634, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.698634,FALSE,0,FALSE,0,TRUE,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""",Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API.
189,40103531,,2,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""",I've been searching on components in React Native but no result.
190,40103531,,3,,"[{'score': 0.980865, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,TRUE,0.980865,TRUE,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""","Does anybody know if there is something around or if it can be done within React Native?"""
191,51381522,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""","""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery."
192,51381522,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""","Uploading image takes time, so I want the users of my app to have feedback about what's going on."
193,51381522,,2,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",Progress Bar with real time percentage of image upload is what I need.
194,51381522,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will
195,51381522,,4,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",triggerand there I will get percent of upload as parameter (i) and increment.
196,51381522,,5,,"[{'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727988,TRUE,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""","Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help"""
197,37908660,,0,,"[{'score': 0.656435, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.861593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.861593,FALSE,0,TRUE,0.656435,TRUE,"""I have a similar question toand, neither of which have an accepted solution.I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.suggests using, but (having tried and failed) I'm not sure how to integrate it into my app.Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from mywhich I want to use to be able to toggle the flashlight:""","""I have a similar question toand, neither of which have an accepted solution.I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.suggests"
198,37908660,,1,,"[{'score': 0.50818, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.50818,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a similar question toand, neither of which have an accepted solution.I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.suggests using, but (having tried and failed) I'm not sure how to integrate it into my app.Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from mywhich I want to use to be able to toggle the flashlight:""","using, but (having tried and failed) I'm not sure how to integrate it into my app.Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from mywhich I want to use to be able to toggle the flashlight:"""
199,54077087,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I tried to create a custom model for my IBM Watson Visual Recognition API, by following the IBM's docs. I'm stuck at this point.""","""I tried to create a custom model for my IBM Watson Visual Recognition API, by following the IBM's docs."
200,54077087,,1,,"[{'score': 0.645695, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.997482, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.645695,FALSE,0,FALSE,0,TRUE,0.997482,FALSE,0,FALSE,0,FALSE,"""I tried to create a custom model for my IBM Watson Visual Recognition API, by following the IBM's docs. I'm stuck at this point.""","I'm stuck at this point."""
201,47089134,,0,,"[{'score': 0.515205, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515205,FALSE,0,FALSE,0,TRUE,"""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this?""","""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image."
202,47089134,,1,,"[{'score': 0.815245, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.815245,TRUE,"""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this?""",so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate.
203,47089134,,2,,"[{'score': 0.961411, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961411,TRUE,"""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this?""","so is there any other option for this?"""
204,52126381,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.810144, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.810144,FALSE,0,TRUE,0.5538,TRUE,"""I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable). Check the package spelling or your minimum-stability. What do that mean?""","""I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable)."
205,52126381,,1,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,TRUE,"""I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable). Check the package spelling or your minimum-stability. What do that mean?""",Check the package spelling or your minimum-stability.
206,52126381,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable). Check the package spelling or your minimum-stability. What do that mean?""","What do that mean?"""
207,51817443,,0,,"[{'score': 0.917265, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.917265,FALSE,0,FALSE,0,TRUE,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""","""My question is a little bit general, we want to build a solution based on amazon rekognition."
208,51817443,,1,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,TRUE,0.80026,FALSE,0,TRUE,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""",But we want to make sure that amazon don't keep our data after the process is completed for example.
209,51817443,,2,,"[{'score': 0.577557, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.577557,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""","When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing?"
210,51817443,,3,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""","Is it automatically destroyed or amazon keeps it locally?"""
211,49974375,,0,,"[{'score': 0.70878, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.70878,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""","""I'm trying to install google's vision API on my system."
212,49974375,,1,,"[{'score': 0.659052, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.743682, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.659052,FALSE,0,FALSE,0,TRUE,0.743682,FALSE,0,FALSE,0,FALSE,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""",I'm using the commandOn executing this command the following error is shown:Please help.
213,49974375,,2,,"[{'score': 0.821444, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821444,FALSE,0,FALSE,0,TRUE,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""",How can I solve the problem?Note:-I had installed openCV through the .whl
214,49974375,,3,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""",file and it installed just fine.
215,49974375,,4,,"[{'score': 0.892717, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.892717,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,FALSE,0,FALSE,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""","But when I tried to install scikit-image through the same process, it shows the aforementioned error."""
216,52451363,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""","""I created a ruby gem."
217,52451363,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",And I made a change to it.
218,52451363,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",But I did not change any of its development or runtime dependencies.
219,52451363,,3,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""","After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else."
220,52451363,,4,,"[{'score': 0.777256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.777256,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""","However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud."
221,52451363,,5,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""","However, I did not update google cloud in my gem."
222,52451363,,6,,"[{'score': 0.75535, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75535,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",I just updated one line of code in my gem itself.
223,52451363,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""","Why is it updating other gems and how can I prevent this?"""
224,43628002,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""","""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"""
225,43628002,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""","at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS""."
226,43628002,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""","I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application."
227,43628002,,3,,"[{'score': 0.849456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849456,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""","But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi"""
228,40087567,,0,,"[{'score': 0.776414, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.776414,FALSE,0,FALSE,0,TRUE,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""","""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json"
229,40087567,,1,,"[{'score': 0.809841, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.809841,FALSE,0,FALSE,0,TRUE,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""",file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ?
230,40087567,,2,,"[{'score': 0.682143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682143,FALSE,0,FALSE,0,TRUE,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""",and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json
231,40087567,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""","fileBelow is theinThe below is the text inside cloudVisionInputFile"""
232,44826567,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!""","""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres"
233,44826567,,1,,"[{'score': 0.928936, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.928936,TRUE,"""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!""","my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!"""
234,41755822,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to convertto aso that I can use it in the. I've tried the following...... but I get the following error:Google's example code references so the deprecatedAPI but I am usingwhich is why I cannot use them for help.""","""I need to convertto aso that I can use it in the."
235,41755822,,1,,"[{'score': 0.501519, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.655013, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.501519,FALSE,0,FALSE,0,TRUE,0.655013,FALSE,0,FALSE,0,FALSE,"""I need to convertto aso that I can use it in the. I've tried the following...... but I get the following error:Google's example code references so the deprecatedAPI but I am usingwhich is why I cannot use them for help.""","I've tried the following...... but I get the following error:Google's example code references so the deprecatedAPI but I am usingwhich is why I cannot use them for help."""
236,53980744,,0,,"[{'score': 0.782808, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.782808,FALSE,0,FALSE,0,TRUE,"""i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document. My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can. Is this indeed the case.""","""i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document."
237,53980744,,1,,"[{'score': 0.69572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69572,FALSE,0,FALSE,0,TRUE,"""i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document. My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can. Is this indeed the case.""",My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can.
238,53980744,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.970936, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,TRUE,0.970936,FALSE,0,TRUE,"""i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document. My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can. Is this indeed the case.""","Is this indeed the case."""
239,47614963,,0,,"[{'score': 0.560944, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.875575, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.875575,TRUE,0.560944,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth)."
240,47614963,,1,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position."
241,47614963,,2,,"[{'score': 0.789807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.789807,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried."
242,47614963,,3,,"[{'score': 0.756237, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.756237,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types."
243,47614963,,4,,"[{'score': 0.836435, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.836435,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold."
244,47614963,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV?
245,47614963,,6,,"[{'score': 0.5449, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5449,FALSE,0,FALSE,0,TRUE,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""","Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?"""
246,43410910,,0,,"[{'score': 0.651957, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.599609, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.651957,FALSE,0,FALSE,0,TRUE,0.599609,FALSE,0,FALSE,0,FALSE,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""","""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work."
247,43410910,,1,,"[{'score': 0.644513, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.644513,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""","can someone point me to the correct direction other than jumping off the roof.thanks"""
248,42102280,,0,,"[{'score': 0.555008, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.770567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.555008,FALSE,0,FALSE,0,TRUE,0.770567,FALSE,0,FALSE,0,FALSE,"""I am trying to get simple functionality from the Microsoft Face API, using this example provided ():Whenever I execute the code, I get a 400 bad request, of which I cannot how to view the specific cause. This is how mine looks:""","""I am trying to get simple functionality from the Microsoft Face API, using this example provided ():Whenever I execute the code, I get a 400 bad request, of which I cannot how to view the specific cause."
249,42102280,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to get simple functionality from the Microsoft Face API, using this example provided ():Whenever I execute the code, I get a 400 bad request, of which I cannot how to view the specific cause. This is how mine looks:""","This is how mine looks:"""
250,53799283,,0,,"[{'score': 0.870269, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.870269,FALSE,0,FALSE,0,TRUE,"""Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen. This detection and placing ar node has to be on same activity and not two different activities. Is it possible to combine both (arcore and Vision) on same activity.""","""Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen."
251,53799283,,1,,"[{'score': 0.592138, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.592138,FALSE,0,TRUE,"""Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen. This detection and placing ar node has to be on same activity and not two different activities. Is it possible to combine both (arcore and Vision) on same activity.""",This detection and placing ar node has to be on same activity and not two different activities.
252,53799283,,2,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen. This detection and placing ar node has to be on same activity and not two different activities. Is it possible to combine both (arcore and Vision) on same activity.""","Is it possible to combine both (arcore and Vision) on same activity."""
253,45546546,,0,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""","""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved."
254,45546546,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""",Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results.
255,45546546,,2,,"[{'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.976993,TRUE,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""","Could you advise on why this may be happening?"""
256,46739009,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""","""I am trying to use Google Vision API in my WinForms (.NET) project."
257,46739009,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",I have signed up in Google Cloud Platform and enabled Vision API.
258,46739009,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format.
259,46739009,,3,,"[{'score': 0.834975, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.834975,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""","As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before)."
260,46739009,,4,,"[{'score': 0.730849, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.730849,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""","All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S."
261,46739009,,5,,"[{'score': 0.526696, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.526696,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",I have made a significant research on this topic.
262,46739009,,6,,"[{'score': 0.757214, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.757214,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,TRUE,0.873263,FALSE,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""","I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them."""
263,52276444,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""","""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But"
264,52276444,,1,,"[{'score': 0.685958, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.685958,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,FALSE,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""","when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?"""
265,54432180,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""","""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK."
266,54432180,,1,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""",I have succeed with GET method but I don't know what to do with POST method.
267,54432180,,2,,"[{'score': 0.517363, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.517363,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,FALSE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""","I have figure it out that the problem is in ""request.set_body"""
268,54432180,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""",method.
269,54432180,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""","I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website."
270,54432180,,5,,"[{'score': 0.598646, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.598646,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.946222,FALSE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""","If anyone knows about this problem, please help me."
271,54432180,,6,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""",Thank you.Here is the link of Face API:And here is the code.
272,54432180,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""","In this code, I try to post a picture from a website:"""
273,56302753,,0,,"[{'score': 0.778006, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.778006,FALSE,0,FALSE,0,TRUE,"""I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close). Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream. Although, I tried with OpenCV & Google Vision api, but they are not able to perform the task perfectly.Please suggest if you have any solution.""","""I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close)."
274,56302753,,1,,"[{'score': 0.543266, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.543266,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close). Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream. Although, I tried with OpenCV & Google Vision api, but they are not able to perform the task perfectly.Please suggest if you have any solution.""",Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream.
275,56302753,,2,,"[{'score': 0.738513, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.738513,FALSE,0,FALSE,0,TRUE,"""I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close). Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream. Although, I tried with OpenCV & Google Vision api, but they are not able to perform the task perfectly.Please suggest if you have any solution.""","Although, I tried with OpenCV & Google Vision api, but they are not able to perform the task perfectly.Please suggest if you have any solution."""
276,50515317,,0,,"[{'score': 0.961593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961593,FALSE,0,FALSE,0,TRUE,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""","""The similar question has been asked here:."
277,50515317,,1,,"[{'score': 0.621837, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.621837,FALSE,0,FALSE,0,TRUE,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""","However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app."
278,50515317,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""","What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity."
279,50515317,,3,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""","But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera."""
280,55151128,,0,,"[{'score': 0.528524, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.528524,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""","""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row."
281,55151128,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""","I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work."
282,55151128,,2,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""","I get all sorts of type problems, but with no real information to solve it with."
283,55151128,,3,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""","The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this?"
284,55151128,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""","This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string"""
285,36974179,,0,,"[{'score': 0.827233, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.778006, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.827233,FALSE,0,FALSE,0,TRUE,0.778006,FALSE,0,TRUE,0.727988,FALSE,"""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith. Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine. So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks""","""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith."
286,36974179,,1,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith. Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine. So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks""",Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine.
287,36974179,,2,,"[{'score': 0.697779, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.697779,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith. Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine. So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks""","So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks"""
288,55528723,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run a code that uses previous version of the google cloud vision api. How do I install the earlier version? Working on Ubuntu and using Ruby""","""I am trying to run a code that uses previous version of the google cloud vision api."
289,55528723,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run a code that uses previous version of the google cloud vision api. How do I install the earlier version? Working on Ubuntu and using Ruby""",How do I install the earlier version?
290,55528723,,2,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am trying to run a code that uses previous version of the google cloud vision api. How do I install the earlier version? Working on Ubuntu and using Ruby""","Working on Ubuntu and using Ruby"""
291,45468418,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text). I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better.Thank you.""","""I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text)."
292,45468418,,1,,"[{'score': 0.934911, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.824153, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824153,FALSE,0,TRUE,0.934911,TRUE,"""I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text). I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better.Thank you.""","I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better.Thank you."""
293,44812417,,0,,"[{'score': 0.658146, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.658146,FALSE,0,FALSE,0,TRUE,"""I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it? This way I can extract only the name.OBS: I open to using any kind o service that facilitates this""","""I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it?"
294,44812417,,1,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it? This way I can extract only the name.OBS: I open to using any kind o service that facilitates this""","This way I can extract only the name.OBS: I open to using any kind o service that facilitates this"""
295,56407381,,0,,"[{'score': 0.809841, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.809841,FALSE,0,TRUE,0.58393,TRUE,"""I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application. But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent).My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly.I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform.""","""I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application."
296,56407381,,1,,"[{'score': 0.876374, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.876374,FALSE,0,FALSE,0,TRUE,"""I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application. But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent).My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly.I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform.""","But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent).My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly.I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform."""
297,47308379,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","""I'm having troubles building my Unity project."
298,47308379,,1,,"[{'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,FALSE,0,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","I tried both a custom scene, created by me, and the example scenes located into."
299,47308379,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service."
300,47308379,,3,,"[{'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.97759,FALSE,0,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",All Mixed Reality settings have been configured correctly.
301,47308379,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",When I try to build the project I get lots of errors and they're all related to Watson SDK.
302,47308379,,5,,"[{'score': 0.951863, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.872611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.951863,FALSE,0,FALSE,0,TRUE,0.872611,FALSE,0,FALSE,0,FALSE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there."
303,47308379,,6,,"[{'score': 0.698904, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.698904,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else?"
304,47308379,,7,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.998976,TRUE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",Any ideas?
305,47308379,,8,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""","Thanks!"""
306,42122978,,0,,"[{'score': 0.747114, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747114,FALSE,0,FALSE,0,TRUE,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""","""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text)."
307,42122978,,1,,"[{'score': 0.846863, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.846863,FALSE,0,FALSE,0,TRUE,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""",Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?
308,42122978,,2,,"[{'score': 0.622571, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.622571,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""","There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call."""
309,54466934,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a folder with 100+ images. I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.Using following code:I am usingto choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..Usedbut getting below errorfound one post but that is in python  unable to replicate it in R""","""I have a folder with 100+ images."
310,54466934,,1,,"[{'score': 0.679341, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.577924, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.679341,FALSE,0,FALSE,0,TRUE,0.577924,FALSE,0,FALSE,0,FALSE,"""I have a folder with 100+ images. I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.Using following code:I am usingto choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..Usedbut getting below errorfound one post but that is in python  unable to replicate it in R""","I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.Using following code:I am usingto choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..Usedbut getting below errorfound one post but that is in python  unable to replicate it in R"""
311,41523004,,0,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,FALSE,0,TRUE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""","""I am trying to detect faces in an image using AWS Image Rekognition API."
312,41523004,,1,,"[{'score': 0.616478, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.616478,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""","But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"""
313,41523004,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""","exists in the AWS S3 Bucket ""bucket-name""."
314,41523004,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""",And the region Name is also correct.The Permissions for this object '001.jpg'
315,41523004,,4,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.618451,FALSE,0,TRUE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""",is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this.
316,41523004,,5,,"[{'score': 0.974201, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.974201,TRUE,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""","Any Suggestion to resolve this please ?Thanks,"""
317,33515465,,0,,"[{'score': 0.549699, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.549699,FALSE,0,FALSE,0,TRUE,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""","""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to."
318,33515465,,1,,"[{'score': 0.80429, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.553221, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.80429,FALSE,0,FALSE,0,FALSE,0,TRUE,0.553221,FALSE,0,FALSE,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""","What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView."
319,33515465,,2,,"[{'score': 0.872746, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.87232, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.590113, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.872746,FALSE,0,FALSE,0,TRUE,0.87232,FALSE,0,TRUE,0.590113,FALSE,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""","Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)"""
320,54790692,,0,,"[{'score': 0.529664, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.529664,FALSE,0,FALSE,0,TRUE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""","""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json"
321,54790692,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",afterwards.
322,54790692,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""","However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename."
323,54790692,,3,,"[{'score': 0.589161, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.589161,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""","So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution."
324,54790692,,4,,"[{'score': 0.96417, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.96417,TRUE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",I can't seem to find anything in the documentation about not appendingto outputs.
325,54790692,,5,,"[{'score': 0.868088, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.868088,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""","Extremely happy for any pointers!"""
326,26010360,,0,,"[{'score': 0.946586, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946586,FALSE,0,FALSE,0,TRUE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""","""I want to develop a web based face recognition API."
327,26010360,,1,,"[{'score': 0.867677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867677,FALSE,0,FALSE,0,TRUE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",The system that will process face recognition is in c# application(opencv).
328,26010360,,2,,"[{'score': 0.512013, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.512013,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,FALSE,0,FALSE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",My problem is how to pass data from php to c#?
329,26010360,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""","I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website."
330,26010360,,4,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",seefor similar result.
331,26010360,,5,,"[{'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,FALSE,0,TRUE,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""","However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)"""
332,40346408,,0,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.80026,FALSE,0,TRUE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""","""I am training a classifier for recognizing certain objects in an image."
333,40346408,,1,,"[{'score': 0.753872, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.753872,FALSE,0,FALSE,0,TRUE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g.
334,40346408,,2,,"[{'score': 0.519518, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.519518,FALSE,0,FALSE,0,TRUE,0.681699,FALSE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""","dogs.Before I train Watson, I can delete pictures that may throw things off."
335,40346408,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""","Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples."
336,40346408,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",Would cats and other small animals be good negative examples?
337,40346408,,5,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""","What else?"""
338,50763441,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have aws recognition code written in Python, and it run's by Node API, which works fine on Windows system but when I'm deploying it on Linux I'm facing this issue:-I have given both AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess access role to I'm user. Still I don't know how to get things going.Python code:-Node Code used to run Python script:-I have Python version 2.7 installed on my Ubuntu, pip version 10.0.1.""","""I have aws recognition code written in Python, and it run's by Node API, which works fine on Windows system but when I'm deploying it on Linux I'm facing this issue:-I have given both AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess access role to I'm user."
339,50763441,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have aws recognition code written in Python, and it run's by Node API, which works fine on Windows system but when I'm deploying it on Linux I'm facing this issue:-I have given both AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess access role to I'm user. Still I don't know how to get things going.Python code:-Node Code used to run Python script:-I have Python version 2.7 installed on my Ubuntu, pip version 10.0.1.""","Still I don't know how to get things going.Python code:-Node Code used to run Python script:-I have Python version 2.7 installed on my Ubuntu, pip version 10.0.1."""
340,54876804,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""","""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads."
341,54876804,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""","I know there are ways to allow users to ""flag as inappropriate""."
342,54876804,,2,,"[{'score': 0.844716, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.844716,FALSE,0,FALSE,0,TRUE,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition.
343,54876804,,3,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.856622,TRUE,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""","Has anybody successfully implemented a solution for this problem?"""
344,43735666,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""","""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android"
345,43735666,,1,,"[{'score': 0.508382, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.508382,TRUE,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""","project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime"
346,43735666,,2,,"[{'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.733853,TRUE,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""","i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has"
347,43735666,,3,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.976993,TRUE,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""","anyone got any idea how to handle this exception."""
348,41348880,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""","""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images."
349,41348880,,1,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""","However, Amazon has no Documentation on how to integrate their SDK with iOS."
350,41348880,,2,,"[{'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,FALSE,0,TRUE,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""","They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project."
351,41348880,,3,,"[{'score': 0.62033, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.62033,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""","Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!"""
352,55099418,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to integrate AWS Rekognition into my Rails app. After the user uploads his avatar via Active Storage, Rekognition should show some info about it.However, I get the errorHow can I get the image file property into AWS Rekognition?""","""I'm trying to integrate AWS Rekognition into my Rails app."
353,55099418,,1,,"[{'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,TRUE,0.775166,TRUE,"""I'm trying to integrate AWS Rekognition into my Rails app. After the user uploads his avatar via Active Storage, Rekognition should show some info about it.However, I get the errorHow can I get the image file property into AWS Rekognition?""","After the user uploads his avatar via Active Storage, Rekognition should show some info about it.However,"
354,55099418,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to integrate AWS Rekognition into my Rails app. After the user uploads his avatar via Active Storage, Rekognition should show some info about it.However, I get the errorHow can I get the image file property into AWS Rekognition?""","I get the errorHow can I get the image file property into AWS Rekognition?"""
355,51156207,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.822231,TRUE,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""","""I am using Rekognition'sandto analise a video with some people."
356,51156207,,1,,"[{'score': 0.592374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.592374,TRUE,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""","The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video."
357,51156207,,2,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""",The issue is that the gap between two detections can be quite large.
358,51156207,,3,,"[{'score': 0.804906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804906,FALSE,0,FALSE,0,TRUE,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""","Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs"""
359,35602395,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I want to do some Analytics on the image hosted on the cloud using IBM Watson Visual recognition. Currently I am downloading the image and storing it locally and then give it to the Watson visual Recognition service.I dont want to download the image locally.I am using JAVA""","""I want to do some Analytics on the image hosted on the cloud using IBM Watson Visual recognition."
360,35602395,,1,,"[{'score': 0.607667, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.607667,FALSE,0,FALSE,0,TRUE,"""I want to do some Analytics on the image hosted on the cloud using IBM Watson Visual recognition. Currently I am downloading the image and storing it locally and then give it to the Watson visual Recognition service.I dont want to download the image locally.I am using JAVA""","Currently I am downloading the image and storing it locally and then give it to the Watson visual Recognition service.I dont want to download the image locally.I am using JAVA"""
361,50782221,,0,,"[{'score': 0.524022, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.524022,FALSE,0,FALSE,0,TRUE,"""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question.""","""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose."
362,50782221,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.88939,TRUE,"""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question.""",Hence neither the question contains any code nor is an answer expected to contain any code.
363,50782221,,2,,"[{'score': 0.947344, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.947344,FALSE,0,FALSE,0,TRUE,"""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question.""","If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question."""
364,47413657,,0,,"[{'score': 0.754414, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.754414,FALSE,0,FALSE,0,TRUE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""","""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks."
365,47413657,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",It uses the same Google Vision API.
366,47413657,,2,,"[{'score': 0.802462, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802462,FALSE,0,FALSE,0,TRUE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision.
367,47413657,,3,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose.
368,47413657,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",But they are highly priced.
369,47413657,,5,,"[{'score': 0.7142, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.764732, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.7142,FALSE,0,FALSE,0,FALSE,0,TRUE,0.764732,FALSE,0,FALSE,0,FALSE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results.
370,47413657,,6,,"[{'score': 0.586917, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.719769, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.586917,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719769,FALSE,0,FALSE,0,FALSE,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""","But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry"""
371,51611493,,0,,"[{'score': 0.612561, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.612561,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,FALSE,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""","""I am having problems to return a promise from the Google Vision OCR."
372,51611493,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",Here is the sample code from Google:This will output the full text to the console.
373,51611493,,2,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,TRUE,0.647986,TRUE,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",If I put the above code into a function and return the variabledetectionsI get onlyundefinedback.
374,51611493,,3,,"[{'score': 0.841001, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841001,FALSE,0,FALSE,0,TRUE,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""","I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you."""
375,40837023,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""","""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g."
376,40837023,,1,,"[{'score': 0.651786, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.51916, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.651786,FALSE,0,TRUE,0.51916,TRUE,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""","quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision."""
377,49388341,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""","""I am trying to use the Azure Face API on android."
378,49388341,,1,,"[{'score': 0.69572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69572,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""",I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method.
379,49388341,,2,,"[{'score': 0.619589, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.606023, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.619589,FALSE,0,FALSE,0,TRUE,0.606023,FALSE,0,FALSE,0,FALSE,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""","I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range."
380,49388341,,3,,"[{'score': 0.70601, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.70601,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,FALSE,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""","I don't understand why it isn't working."""
381,47641619,,0,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""","""I'm using Node JS to call Google Vision Cloud API."
382,47641619,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.635197, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635197,FALSE,0,TRUE,0.647986,TRUE,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""",It's working fine but I can't understand how to process the returned object.Any clue?
383,47641619,,2,,"[{'score': 0.955584, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955584,FALSE,0,TRUE,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""",I have to readfullTextAnnotation.textkey.
384,47641619,,3,,"[{'score': 0.775384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775384,FALSE,0,FALSE,0,TRUE,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""","All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:"""
385,51991401,,0,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.870269, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.870269,FALSE,0,TRUE,0.659112,TRUE,"""I'm trying to recognize faces in a video stream using thesebut I couldn't find any help to implement  PutMedia operation using Python. I'm  using Ubuntu 16.04 and Python 3.6. Any hint please so I can implement it using Python.""","""I'm trying to recognize faces in a video stream using thesebut I couldn't find any help to implement  PutMedia operation using Python."
386,51991401,,1,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I'm trying to recognize faces in a video stream using thesebut I couldn't find any help to implement  PutMedia operation using Python. I'm  using Ubuntu 16.04 and Python 3.6. Any hint please so I can implement it using Python.""",I'm  using Ubuntu 16.04 and Python 3.6.
387,51991401,,2,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I'm trying to recognize faces in a video stream using thesebut I couldn't find any help to implement  PutMedia operation using Python. I'm  using Ubuntu 16.04 and Python 3.6. Any hint please so I can implement it using Python.""","Any hint please so I can implement it using Python."""
388,44519968,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I am accessing google vision api using requests.post method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""response i get is:please help""","""I am accessing google vision api using requests.post"
389,44519968,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.647986,TRUE,"""I am accessing google vision api using requests.post method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""response i get is:please help""",method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri.
390,44519968,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am accessing google vision api using requests.post method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""response i get is:please help""","I cannot specify local file name like ""/Users/pi/test.jpg""response"
391,44519968,,3,,"[{'score': 0.580279, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.580279,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am accessing google vision api using requests.post method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""response i get is:please help""","i get is:please help"""
392,52270452,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am facing issue while implementing Amazon Rekognition. The error I am getting is:AWSRekognition class, createStreamProcessor API call always through the following error:AWSKinesisRecorder class API submitAllRecords API call always through the following error:Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.Any help appreciated?""","""I am facing issue while implementing Amazon Rekognition."
393,52270452,,1,,"[{'score': 0.58382, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.837056, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.58382,FALSE,0,FALSE,0,TRUE,0.837056,FALSE,0,FALSE,0,FALSE,"""I am facing issue while implementing Amazon Rekognition. The error I am getting is:AWSRekognition class, createStreamProcessor API call always through the following error:AWSKinesisRecorder class API submitAllRecords API call always through the following error:Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.Any help appreciated?""","The error I am getting is:AWSRekognition class, createStreamProcessor API call always through the following error:AWSKinesisRecorder class API submitAllRecords API call always through the following error:Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.Any help appreciated?"""
394,45160721,,0,,"[{'score': 0.677676, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677676,FALSE,0,FALSE,0,TRUE,"""I am trying to do a simple face detect call using AWS Android SDK Reckognition 2.4.4.  Can someone point what is going wrong?I am getting the following errorHere is the code""","""I am trying to do a simple face detect call using AWS Android SDK Reckognition 2.4.4."
395,45160721,,1,,"[{'score': 0.705583, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.705583,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,FALSE,"""I am trying to do a simple face detect call using AWS Android SDK Reckognition 2.4.4.  Can someone point what is going wrong?I am getting the following errorHere is the code""","Can someone point what is going wrong?I am getting the following errorHere is the code"""
396,43425391,,0,,"[{'score': 0.585652, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.585652,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""","""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."""
397,43425391,,1,,"[{'score': 0.864142, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.522484, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.864142,FALSE,0,FALSE,0,TRUE,0.522484,FALSE,0,TRUE,0.822231,FALSE,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""","How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid"
398,43425391,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""","image URL.""}}Whole code looks like below."""
399,47392166,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.""","""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification."
400,47392166,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.""",So it needs two times 10 images in a zip-folder (binary buffer of the zip folder).
401,47392166,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.""","Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format."""
402,50452142,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""","""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera)."
403,50452142,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""","Then, I want to send this to the Azure api with that same app."
404,50452142,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""",Is there any support for this?
405,50452142,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""","I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance."""
406,50536717,,0,,"[{'score': 0.79213, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.635961, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.79213,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635961,FALSE,0,FALSE,0,FALSE,"""What is the best and simple way to search and get images list from S3 by using English keywords. Or do I have to use the Rekognition to store all the image metadatas into database?My development is using Php.""","""What is the best and simple way to search and get images list from S3 by using English keywords."
407,50536717,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""What is the best and simple way to search and get images list from S3 by using English keywords. Or do I have to use the Rekognition to store all the image metadatas into database?My development is using Php.""","Or do I have to use the Rekognition to store all the image metadatas into database?My development is using Php."""
408,47557888,,0,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""","""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg."
409,47557888,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",The image that I am using has text that is approximately size 10-12 or a regular letter page.
410,47557888,,2,,"[{'score': 0.804906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804906,FALSE,0,FALSE,0,TRUE,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""","However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often?"
411,47557888,,3,,"[{'score': 0.561098, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.74948, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.561098,FALSE,0,FALSE,0,FALSE,0,TRUE,0.74948,FALSE,0,TRUE,0.594263,FALSE,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""","Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?"""
412,42431787,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""","""Google Natural Language API has been working in my iOS app up until yesterday."
413,42431787,,1,,"[{'score': 0.647033, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.647033,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,FALSE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""","The API started returning ""permission denied"" errors as of this morning."
414,42431787,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",E.g:Example request:Billing is enabled for the account (with a balance of $0).
415,42431787,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard.
416,42431787,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""","I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues."
417,42431787,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message.
418,42431787,,6,,"[{'score': 0.622787, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.622787,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,FALSE,0,FALSE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",Calls from thedo work however.Edit:The error is also happening on the.
419,42431787,,7,,"[{'score': 0.721102, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.825633, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.721102,FALSE,0,FALSE,0,TRUE,0.825633,FALSE,0,FALSE,0,FALSE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""","Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page."
420,42431787,,8,,"[{'score': 0.590824, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.910842, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.590824,FALSE,0,FALSE,0,TRUE,0.910842,FALSE,0,TRUE,0.615352,FALSE,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""","The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:"""
421,55585979,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""I am new to Google Vision, and I want create code to receive an asynchronous response. For example, create a JSON file to response and later load the JSON file and continue with de recognizer.I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.This is how I save the response to a JSON file:This is how I try to read and use the JSON file:but it does not work, it says""","""I am new to Google Vision, and I want create code to receive an asynchronous response."
422,55585979,,1,,"[{'score': 0.724581, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.724581,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,FALSE,"""I am new to Google Vision, and I want create code to receive an asynchronous response. For example, create a JSON file to response and later load the JSON file and continue with de recognizer.I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.This is how I save the response to a JSON file:This is how I try to read and use the JSON file:but it does not work, it says""","For example, create a JSON file to response and later load the JSON file and continue with de recognizer.I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.This is how I save the response to a JSON file:This is how I try to read and use the JSON file:but it does not work, it says"""
423,45979638,,0,,"[{'score': 0.508482, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.947344, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.508482,FALSE,0,FALSE,0,FALSE,0,TRUE,0.947344,FALSE,0,FALSE,0,FALSE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""","""The goal is to make an app which can recognize egg markings, for example."
424,45979638,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",I tried bothand theon the following images.
425,45979638,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI
426,45979638,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI
427,45979638,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""","color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV."
428,45979638,,5,,"[{'score': 0.950868, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.950868,FALSE,0,FALSE,0,TRUE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""","These are the results so far:However, there's still lots of noise."
429,45979638,,6,,"[{'score': 0.514187, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.908301, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.514187,FALSE,0,FALSE,0,TRUE,0.908301,FALSE,0,FALSE,0,FALSE,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""","I'm struggling to adjust the parameters so that it works well across different images."""
430,37515812,,0,,"[{'score': 0.939116, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.939116,FALSE,0,FALSE,0,TRUE,"""I am using the google cloud vision api to analyze pictures. Is there a list of all the possible responses for the labelAnnotations method?""","""I am using the google cloud vision api to analyze pictures."
431,37515812,,1,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I am using the google cloud vision api to analyze pictures. Is there a list of all the possible responses for the labelAnnotations method?""","Is there a list of all the possible responses for the labelAnnotations method?"""
432,49936444,,0,,"[{'score': 0.772316, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.772316,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,FALSE,0,FALSE,"""I am unable to save the image inbut can save in the same rekognition directory. How do I save a snapshot from camera to static folder?My OpenCV code is running in post method of.In:""","""I am unable to save the image inbut can save in the same rekognition directory."
433,49936444,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am unable to save the image inbut can save in the same rekognition directory. How do I save a snapshot from camera to static folder?My OpenCV code is running in post method of.In:""","How do I save a snapshot from camera to static folder?My OpenCV code is running in post method of.In:"""
434,38147675,,0,,"[{'score': 0.581336, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.889429, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.581336,FALSE,0,FALSE,0,FALSE,0,TRUE,0.889429,FALSE,0,FALSE,0,FALSE,"""I have integrated google vision in my project as shown in below post:Everything looks fine except the camera view brightness . The camera view here is very dark when comparing with my actual android camera app.Please let me know if i can increase the brightness of the camera and turn on any low light settings. Thanks .Pictures :,""","""I have integrated google vision in my project as shown in below post:Everything looks fine except the camera view brightness ."
435,38147675,,1,,"[{'score': 0.756947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.756947,FALSE,0,FALSE,0,TRUE,"""I have integrated google vision in my project as shown in below post:Everything looks fine except the camera view brightness . The camera view here is very dark when comparing with my actual android camera app.Please let me know if i can increase the brightness of the camera and turn on any low light settings. Thanks .Pictures :,""",The camera view here is very dark when comparing with my actual android camera app.Please let me know if i can increase the brightness of the camera and turn on any low light settings.
436,38147675,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have integrated google vision in my project as shown in below post:Everything looks fine except the camera view brightness . The camera view here is very dark when comparing with my actual android camera app.Please let me know if i can increase the brightness of the camera and turn on any low light settings. Thanks .Pictures :,""","Thanks .Pictures :,"""
437,52647919,,0,,"[{'score': 0.828505, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828505,FALSE,0,FALSE,0,TRUE,"""I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text. For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR. Is it possible?Also is there any other way to not get jumbled up text when we have 3 columns of text in image?""","""I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text."
438,52647919,,1,,"[{'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,FALSE,0,TRUE,"""I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text. For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR. Is it possible?Also is there any other way to not get jumbled up text when we have 3 columns of text in image?""","For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR."
439,52647919,,2,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text. For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR. Is it possible?Also is there any other way to not get jumbled up text when we have 3 columns of text in image?""","Is it possible?Also is there any other way to not get jumbled up text when we have 3 columns of text in image?"""
440,54195144,,0,,"[{'score': 0.72441, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.72441,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,FALSE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""","""I have set up a simple, monolitic jHipster project from the generator with oAuth support."
441,54195144,,1,,"[{'score': 0.596228, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.596228,FALSE,0,FALSE,0,TRUE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""","After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project."
442,54195144,,2,,"[{'score': 0.52488, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.52488,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end.
443,54195144,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""","This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties"
444,54195144,,4,,"[{'score': 0.721675, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.721675,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.909883,FALSE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""","looks like that:Besides those errors, everything seems to work fine."
445,54195144,,5,,"[{'score': 0.906379, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.906379,FALSE,0,FALSE,0,TRUE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Requesting to analyze an image throws another error:but the results returned from google are correct.
446,54195144,,6,,"[{'score': 0.541268, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.867767, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.541268,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867767,FALSE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues.
447,54195144,,7,,"[{'score': 0.811765, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.811765,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""","And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue."""
448,53794638,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""","""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP."
449,53794638,,1,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""","I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'."
450,53794638,,2,,"[{'score': 0.608856, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.609892, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.608856,FALSE,0,FALSE,0,TRUE,0.609892,FALSE,0,TRUE,0.647986,FALSE,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""","So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner."
451,53794638,,3,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""","And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!"""
452,48266531,,0,,"[{'score': 0.784247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784247,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API.""","""I am using the Google Cloud Vision API for Python on a small program I'm using."
453,48266531,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API.""","The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API."""
454,47849128,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""","""I am developing VR tool in .NET framework using IBM watson visual recognition service."
455,47849128,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",_visualRecognition.
456,47849128,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",Classify () method was working fine for the custom classifiers before a week.
457,47849128,,3,,"[{'score': 0.593962, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.593962,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,FALSE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""","Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes."
458,47849128,,4,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.547677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.547677,FALSE,0,TRUE,0.716301,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",It's working as default classify method even after passing classifierID's and Owner Id's.
459,47849128,,5,,"[{'score': 0.790954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.790954,FALSE,0,FALSE,0,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",It's work as default classify methodCode:Before same code returning below result.
460,47849128,,6,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""","Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:"""
461,44996852,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to integrate the Google Vision API for face detection.But at creation of object FaceDetector, through FaceDetector.Builder the exception works and the application takes off.The activity in which FaceDetector is created:AndroidManifest.xml:dependencies:Full Error Log:p.s: When creating a FaceDetector object in an empty application, there were no errors or exceptions.""","""I'm trying to integrate the Google Vision API for face detection.But at creation of object FaceDetector, through FaceDetector.Builder the exception works and the application takes off.The activity in which FaceDetector is created:AndroidManifest.xml:dependencies:Full Error Log:p.s:"
462,44996852,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I'm trying to integrate the Google Vision API for face detection.But at creation of object FaceDetector, through FaceDetector.Builder the exception works and the application takes off.The activity in which FaceDetector is created:AndroidManifest.xml:dependencies:Full Error Log:p.s: When creating a FaceDetector object in an empty application, there were no errors or exceptions.""","When creating a FaceDetector object in an empty application, there were no errors or exceptions."""
463,50164690,,0,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""If you try Google Vision API with follwoing demo-image.jpgshown in, you will get a record with empty description and score of 0.7024 in   . Why!?""","""If you try Google Vision API with follwoing demo-image.jpgshown"
464,50164690,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""If you try Google Vision API with follwoing demo-image.jpgshown in, you will get a record with empty description and score of 0.7024 in   . Why!?""","in, you will get a record with empty description and score of 0.7024 in   ."
465,50164690,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""If you try Google Vision API with follwoing demo-image.jpgshown in, you will get a record with empty description and score of 0.7024 in   . Why!?""","Why!?"""
466,46151909,,0,,"[{'score': 0.974201, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974201,TRUE,"""I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers.1) how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?""","""I'm doing some investigations about AWS Rekognition."
467,46151909,,1,,"[{'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,TRUE,"""I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers.1) how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?""",There are two issues I need to know but didn't get the answers.1)
468,46151909,,2,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.783887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.783887,FALSE,0,TRUE,0.525007,TRUE,"""I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers.1) how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?""","how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?"""
469,48607548,,0,,"[{'score': 0.841001, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841001,FALSE,0,FALSE,0,TRUE,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""","""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room."
470,48607548,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""",But the API is returning only 10 responses.In order to get more responses I added the fieldin.
471,48607548,,2,,"[{'score': 0.61571, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.608261, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.61571,FALSE,0,FALSE,0,TRUE,0.608261,FALSE,0,FALSE,0,FALSE,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""","After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system."
472,48607548,,3,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""",It is only accepting images present on the internet with an image address like in the piece of code below.
473,48607548,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""","Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:"""
474,44532633,,0,,"[{'score': 0.694921, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.694921,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what.""","""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step."
475,44532633,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what.""","I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I"
476,44532633,,2,,"[{'score': 0.658567, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.658567,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,TRUE,0.647986,FALSE,"""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what.""","am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what."""
477,52536703,,0,,"[{'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,FALSE,0,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""","""I'm creating barcode reading application using google vision and it consists a flash on/off function also."
478,52536703,,1,,"[{'score': 0.952145, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.952145,FALSE,0,FALSE,0,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""","i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but"
479,52536703,,2,,"[{'score': 0.713632, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.713632,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""","when camera screen is on, flash light is not working.when"
480,52536703,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""","camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working."
481,52536703,,4,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",but i want to flash light on/off with out regarding the camera view is on or stop.i
482,52536703,,5,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem.
483,52536703,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""","thanks in advance."""
484,52746720,,0,,"[{'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,TRUE,"""In Azure Cognitive Image processing the returned json have a ""caption"" field which summarizes the content of the image. However, I didn't find anything similar in AWS.In Amazon Rekognition for image processing how do I get the caption for an image?""","""In Azure Cognitive Image processing the returned json have a ""caption"" field which summarizes the content of the image."
485,52746720,,1,,"[{'score': 0.716804, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716804,FALSE,0,FALSE,0,TRUE,"""In Azure Cognitive Image processing the returned json have a ""caption"" field which summarizes the content of the image. However, I didn't find anything similar in AWS.In Amazon Rekognition for image processing how do I get the caption for an image?""","However, I didn't find anything similar in AWS.In Amazon Rekognition for image processing how do I get the caption for an image?"""
486,49840929,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""","""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:)."
487,49840929,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""",How can I use this with?I need a model (pb file) and weights (json file).
488,49840929,,2,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""",However Azure gives me a .pb
489,49840929,,3,,"[{'score': 0.676008, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.676008,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""","and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the."
490,49840929,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""",This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format.
491,49840929,,5,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""","However I need to fill in ""output_node_names"" (how do I get these?)."
492,49840929,,6,,"[{'score': 0.917181, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.917181,TRUE,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""","I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point."""
493,55369637,,0,,"[{'score': 0.69934, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69934,FALSE,0,FALSE,0,TRUE,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""","""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json"
494,55369637,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.5538,TRUE,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""","file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example"
495,55369637,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""",:filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI
496,55369637,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""","want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1"""
497,51444352,,0,,"[{'score': 0.795273, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.795273,FALSE,0,FALSE,0,TRUE,"""I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the errorAny idea of what I can do in this situation? I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allowsand it still doesn't work.""","""I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the errorAny idea of what I can do in this situation?"
498,51444352,,1,,"[{'score': 0.599254, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.599254,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,FALSE,"""I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the errorAny idea of what I can do in this situation? I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allowsand it still doesn't work.""","I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allowsand it still doesn't work."""
499,49615202,,0,,"[{'score': 0.547054, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.547054,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""When trying out google cloud vision with the drag and drop, the last tab has raw JSON.  What parameter do we need to pass to get that data?I'm currently doing DOCUMENT_TEXT_DETECTION but it only gives data at the level of words and not of individual characters.Edit: I modified this codeand changed the feature ...and the printing to ...I'm only seeing textAnnotations in the output.""","""When trying out google cloud vision with the drag and drop, the last tab has raw JSON."
500,49615202,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When trying out google cloud vision with the drag and drop, the last tab has raw JSON.  What parameter do we need to pass to get that data?I'm currently doing DOCUMENT_TEXT_DETECTION but it only gives data at the level of words and not of individual characters.Edit: I modified this codeand changed the feature ...and the printing to ...I'm only seeing textAnnotations in the output.""","What parameter do we need to pass to get that data?I'm currently doing DOCUMENT_TEXT_DETECTION but it only gives data at the level of words and not of individual characters.Edit: I modified this codeand changed the feature ...and the printing to ...I'm only seeing textAnnotations in the output."""
501,46095355,,0,,"[{'score': 0.536938, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.536938,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""","""Hi i am new to google vision apis."
502,46095355,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""","I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces."
503,46095355,,2,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""",but vision api returning only 10 faces Detection.
504,46095355,,3,,"[{'score': 0.71364, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.71364,FALSE,0,FALSE,0,TRUE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""",Is there any way to detect all the faces using this Vision api.
505,46095355,,4,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""",please refer.and
506,46095355,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""","you can take this image as refHere is my code"""
507,56224197,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.677381, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677381,FALSE,0,TRUE,0.681699,TRUE,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""","""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing."
508,56224197,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""",it says a lot about the picture but doesn't say what the whole picture is of (food or something like that).
509,56224197,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""","I contacted the support team but didn't get an answer."""
510,47532783,,0,,"[{'score': 0.714615, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.714615,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.the result of vertices""","""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but"
511,47532783,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.the result of vertices""",its returns the max size of bitmap image I have.the
512,47532783,,2,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.the result of vertices""","result of vertices"""
513,45296021,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.""","""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects."
514,45296021,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.""",Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library.
515,45296021,,2,,"[{'score': 0.671499, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.671499,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.""","would be of great help."""
516,43494736,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""","""Thestates that formethod, theandcan takeor."
517,43494736,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""","I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded)."
518,43494736,,2,,"[{'score': 0.538928, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.538928,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.647986,FALSE,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""",Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)?
519,43494736,,3,,"[{'score': 0.990161, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.990161,TRUE,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""","or any JavaScript examples usingparam would help."""
520,45775358,,0,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.618451,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""","""I create a web service and I have to insert multi nested object to the database."
521,45775358,,1,,"[{'score': 0.596024, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.596024,FALSE,0,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way.
522,45775358,,2,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",I implemented Onion Architecture in my solution and I add each object by other service.
523,45775358,,3,,"[{'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849827,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",Is this a correct way?
524,45775358,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",The object which I want to insert is AwsRekognitionResponse.
525,45775358,,5,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""","I would like to know which way is the most optimal and correct."""
526,47571678,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""","""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people."
527,47571678,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""","the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run"
528,47571678,,2,,"[{'score': 0.523045, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.520556, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.523045,FALSE,0,FALSE,0,FALSE,0,TRUE,0.520556,FALSE,0,FALSE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""","index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception."
529,47571678,,3,,"[{'score': 0.87232, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87232,FALSE,0,FALSE,0,TRUE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""",I am very new to this and have tried to find the solution to the problem however i have nothing yet.
530,47571678,,4,,"[{'score': 0.540444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.540444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""",Please help.
531,47571678,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""",I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help.
532,47571678,,6,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""","Thank you!"""
533,52343909,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""","""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs?"
534,52343909,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.75152,TRUE,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""","As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file."
535,52343909,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""","So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer."
536,52343909,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.928936, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.928936,TRUE,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""","So far, I've been using Software like OCRKit or ABBYY FineReader."
537,52343909,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""","I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead."""
538,43687962,,0,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""","""I am using Google Cloud Vision API for OCR purpose."
539,43687962,,1,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",I am able to connect to the API and getting JSON result back as expected.
540,43687962,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""","What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image."
541,43687962,,3,,"[{'score': 0.812219, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.864115, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.864115,FALSE,0,TRUE,0.812219,TRUE,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",Pl. let me know what could be the case.
542,43687962,,4,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""","Sample code is attached."""
543,56266707,,0,,"[{'score': 0.777256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.777256,FALSE,0,FALSE,0,TRUE,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""","""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file."
544,56266707,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""","There's one node that contains test, but it's no longer formatted."
545,56266707,,2,,"[{'score': 0.687909, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.934054, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.687909,FALSE,0,FALSE,0,TRUE,0.934054,FALSE,0,FALSE,0,FALSE,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""",Only line breaks are delineated with.
546,56266707,,3,,"[{'score': 0.557881, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.557881,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""","I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted?"
547,56266707,,4,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""","Are there any libraries that would work with GCP to enhance JSON output?"""
548,37796918,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to Google Cloud Vision API. I am doing OCR on images primarily for bills and receipts.For a few images it is working fine, but when I try some other images it gives me this error:This is my code:""","""I am new to Google Cloud Vision API."
549,37796918,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to Google Cloud Vision API. I am doing OCR on images primarily for bills and receipts.For a few images it is working fine, but when I try some other images it gives me this error:This is my code:""","I am doing OCR on images primarily for bills and receipts.For a few images it is working fine, but when I try some other images it gives me this error:This is my code:"""
550,54560404,,0,,"[{'score': 0.507116, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.507116,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","""I have a SQL Server 2017 database and a series of .net"
551,54560404,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",APIs that consume the stored procedures exposed from the database.
552,54560404,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",This works fine for posting data from a website/mobile app and reading that data back.
553,54560404,,3,,"[{'score': 0.534632, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.534632,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,FALSE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","However, one of my pages, a registration page, needs to do more."
554,54560404,,4,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","It takes the usual, name, email, telephone etc and this is all good."
555,54560404,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",It also takes a document as an image.
556,54560404,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",The image uploads fine and is stored in the database.
557,54560404,,7,,"[{'score': 0.903207, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.903207,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence."
558,54560404,,8,,"[{'score': 0.886438, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.886438,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image."
559,54560404,,9,,"[{'score': 0.900604, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.900604,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting."
560,54560404,,10,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","If it matches, it's a pass."
561,54560404,,11,,"[{'score': 0.598418, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.598418,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,FALSE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected."
562,54560404,,12,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e."
563,54560404,,13,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net"
564,54560404,,14,,"[{'score': 0.679906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.679906,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution?"
565,54560404,,15,,"[{'score': 0.799123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.799123,FALSE,0,FALSE,0,TRUE,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""","Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears."""
566,48381832,,0,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""","""I am using using google vision API to detect face from bitmap."
567,48381832,,1,,"[{'score': 0.515876, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.942582, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.515876,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.942582,FALSE,0,FALSE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""",But it is always returning false.
568,48381832,,2,,"[{'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,FALSE,0,TRUE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""",It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways
569,48381832,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""",returning false.
570,48381832,,4,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""",I checked in OPPO(5.1.1)
571,48381832,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""","and Moto(6.0)TIA"""
572,49123683,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""how can i use google cloud vision with python django rest api? My task is that i have a picture,i have to find similer picture from an another picture.is there any other solution to do this task?""","""how can i use google cloud vision with python django rest api?"
573,49123683,,1,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""how can i use google cloud vision with python django rest api? My task is that i have a picture,i have to find similer picture from an another picture.is there any other solution to do this task?""","My task is that i have a picture,i have to find similer picture from an another picture.is"
574,49123683,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,TRUE,0.88939,TRUE,"""how can i use google cloud vision with python django rest api? My task is that i have a picture,i have to find similer picture from an another picture.is there any other solution to do this task?""","there any other solution to do this task?"""
575,56285264,,0,,"[{'score': 0.624641, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.624641,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""","""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language."
576,56285264,,1,,"[{'score': 0.822095, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.822095,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,FALSE,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""","Can I config API return other language or multi language?P/S: Sorry for my bad English :("""
577,45559285,,0,,"[{'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,FALSE,0,TRUE,"""I have integrated Google Cloud Vision API in my java application for text recognition from complex formatted documents. One of my colleague suggested to use ""Tesseract API"".Can anyone please give difference between these two API's.And which is better in terms of accuracy or have any advantage over other.TIA""","""I have integrated Google Cloud Vision API in my java application for text recognition from complex formatted documents."
578,45559285,,1,,"[{'score': 0.861422, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.676008, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.676008,FALSE,0,TRUE,0.861422,TRUE,"""I have integrated Google Cloud Vision API in my java application for text recognition from complex formatted documents. One of my colleague suggested to use ""Tesseract API"".Can anyone please give difference between these two API's.And which is better in terms of accuracy or have any advantage over other.TIA""","One of my colleague suggested to use ""Tesseract API"".Can anyone please give difference between these two API's.And which is better in terms of accuracy or have any advantage over other.TIA"""
579,52857016,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""","""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet."
580,52857016,,1,,"[{'score': 0.508625, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.508625,FALSE,0,FALSE,0,TRUE,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""","I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows."
581,52857016,,2,,"[{'score': 0.802757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802757,FALSE,0,TRUE,0.647986,TRUE,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""","How can I handle this situation and generate an excel sheet with best possible accuracy?"""
582,41166264,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""","""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition."
583,41166264,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""","However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly?"
584,41166264,,2,,"[{'score': 0.908148, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.965322, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.908148,FALSE,0,TRUE,0.965322,TRUE,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""","Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas?"
585,41166264,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""","Thanks!"""
586,55929206,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Gettingwhen trying to detect text in a part (numpy array rectangle out) of an opencv image. When I try to convert the resulting base64 string online it works without issuesI foundon the topic but why would I need to do that?Any ideas what I could be doing wrong?""","""Gettingwhen trying to detect text in a part (numpy array rectangle out) of an opencv image."
587,55929206,,1,,"[{'score': 0.736977, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.709321, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.736977,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.709321,FALSE,"""Gettingwhen trying to detect text in a part (numpy array rectangle out) of an opencv image. When I try to convert the resulting base64 string online it works without issuesI foundon the topic but why would I need to do that?Any ideas what I could be doing wrong?""","When I try to convert the resulting base64 string online it works without issuesI foundon the topic but why would I need to do that?Any ideas what I could be doing wrong?"""
588,40136543,,0,,"[{'score': 0.662817, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.662817,FALSE,0,FALSE,0,TRUE,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""","""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API?"
589,40136543,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""",What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that.
590,40136543,,2,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""",IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google.
591,40136543,,3,,"[{'score': 0.900799, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.900799,FALSE,0,FALSE,0,TRUE,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""","I am open to other services as well, if prices ares below Google's"""
592,38211578,,0,,"[{'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,FALSE,0,TRUE,"""I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage. After that, I want to analyze the photo using Google Cloud Vision APIs.Uploading works fine.To analyze the photo, I'm specifying it usingThe problem is that I get the following errorDo you have any suggestion w.r.t. what permissions I need to set?Thanks!""","""I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage."
593,38211578,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage. After that, I want to analyze the photo using Google Cloud Vision APIs.Uploading works fine.To analyze the photo, I'm specifying it usingThe problem is that I get the following errorDo you have any suggestion w.r.t. what permissions I need to set?Thanks!""","After that, I want to analyze the photo using Google Cloud Vision APIs.Uploading works fine.To analyze the photo, I'm specifying it usingThe problem is that I get the following errorDo you have any suggestion w.r.t."
594,38211578,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage. After that, I want to analyze the photo using Google Cloud Vision APIs.Uploading works fine.To analyze the photo, I'm specifying it usingThe problem is that I get the following errorDo you have any suggestion w.r.t. what permissions I need to set?Thanks!""","what permissions I need to set?Thanks!"""
595,35823073,,0,,"[{'score': 0.664771, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.664771,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""","""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received."
596,35823073,,1,,"[{'score': 0.866387, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.866387,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,FALSE,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Unable to parse number.
597,35823073,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",--------------------
598,35823073,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""","^"","
599,35823073,,4,,"[{'score': 0.699647, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.94715, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.699647,FALSE,0,FALSE,0,TRUE,0.94715,FALSE,0,FALSE,0,FALSE,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""","""status"": ""INVALID_ARGUMENT"" } }I was following this example:"""
600,43520997,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have implemented microsoft face api, with the help of the library:The script is working perfectly on my local machine and I can identify the faces and store them on microsoft db. But when I upload it to my server, it is showing the below error and api call won't process:I have then installed the corresponding files with composer and place the folder with the 'Net' having URL.php..but then it shows the following error.Let me know if you need any further information...Thanks...""","""I have implemented microsoft face api, with the help of the library:The script is working perfectly on my local machine and I can identify the faces and store them on microsoft db."
601,43520997,,1,,"[{'score': 0.715189, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.715189,FALSE,0,FALSE,0,TRUE,"""I have implemented microsoft face api, with the help of the library:The script is working perfectly on my local machine and I can identify the faces and store them on microsoft db. But when I upload it to my server, it is showing the below error and api call won't process:I have then installed the corresponding files with composer and place the folder with the 'Net' having URL.php..but then it shows the following error.Let me know if you need any further information...Thanks...""","But when I upload it to my server, it is showing the below error and api call won't process:I have then installed the corresponding files with composer and place the folder with the 'Net' having URL.php..but then it shows the following error.Let me know if you need any further information...Thanks..."""
602,54061460,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""","""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API."
603,54061460,,1,,"[{'score': 0.892124, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.892124,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""",When i do this i am catching an error but it only logs as true.
604,54061460,,2,,"[{'score': 0.842576, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.842576,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""","Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?"""
605,52326351,,0,,"[{'score': 0.725032, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.725032,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help""","""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working."
606,52326351,,1,,"[{'score': 0.567807, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.567807,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help""","It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function""."
607,52326351,,2,,"[{'score': 0.540444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.540444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help""","Please help"""
608,51479671,,0,,"[{'score': 0.595347, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.595347,FALSE,0,FALSE,0,TRUE,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""","""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3."
609,51479671,,1,,"[{'score': 0.501319, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.501319,FALSE,0,TRUE,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""",I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning).
610,51479671,,2,,"[{'score': 0.68932, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.68932,TRUE,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""","Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda."
611,51479671,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""","Also note my S3 bucket is in the same region as my Lambda function:"""
612,54218010,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I  want to use the Microsoft Emotion API but program it to detect an emotion it currently does not. Is it possible to do this?""","""I  want to use the Microsoft Emotion API but program it to detect an emotion it currently does not."
613,54218010,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I  want to use the Microsoft Emotion API but program it to detect an emotion it currently does not. Is it possible to do this?""","Is it possible to do this?"""
614,35748095,,0,,"[{'score': 0.576155, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.576155,FALSE,0,FALSE,0,TRUE,"""Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ? For unfamiliar ppl via some learning/look-alike mechanism ?Thanks.""","""Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ?"
615,35748095,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ? For unfamiliar ppl via some learning/look-alike mechanism ?Thanks.""","For unfamiliar ppl via some learning/look-alike mechanism ?Thanks."""
616,41803160,,0,,"[{'score': 0.855825, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855825,FALSE,0,FALSE,0,TRUE,"""I am looking for a Google Cloud API that can do both face recognition and identification. My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.Is there any Google Cloud API that can do face recognition?""","""I am looking for a Google Cloud API that can do both face recognition and identification."
617,41803160,,1,,"[{'score': 0.777784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.777784,FALSE,0,FALSE,0,TRUE,"""I am looking for a Google Cloud API that can do both face recognition and identification. My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.Is there any Google Cloud API that can do face recognition?""","My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.Is there any Google Cloud API that can do face recognition?"""
618,42536697,,0,,"[{'score': 0.551348, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.615741, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.551348,FALSE,0,FALSE,0,TRUE,0.615741,FALSE,0,FALSE,0,FALSE,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""","""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line."
619,42536697,,1,,"[{'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,FALSE,0,TRUE,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""",From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle
620,42536697,,2,,"[{'score': 0.731593, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.731593,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""",hasHas anyone had success with the examples in Android Studio?
621,42536697,,3,,"[{'score': 0.928187, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.928187,FALSE,0,FALSE,0,TRUE,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""",I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ.
622,42536697,,4,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""","Thanks"""
623,44783626,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""","""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError."
624,44783626,,1,,"[{'score': 0.607137, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.525275, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.544052, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.607137,FALSE,0,FALSE,0,TRUE,0.525275,FALSE,0,TRUE,0.544052,FALSE,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""","The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not."
625,44783626,,2,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.57374,TRUE,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""","So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company."""
626,45720763,,0,,"[{'score': 0.799163, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.799163,FALSE,0,FALSE,0,TRUE,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""","""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL?"
627,45720763,,1,,"[{'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.681087, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681087,TRUE,0.874372,FALSE,0,TRUE,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""","It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK."""
628,51609428,,0,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""","""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether."
629,51609428,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket.
630,51609428,,2,,"[{'score': 0.784773, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784773,FALSE,0,TRUE,0.716301,TRUE,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people.
631,51609428,,3,,"[{'score': 0.509959, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509959,FALSE,0,FALSE,0,TRUE,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""","This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?"""
632,49167306,,0,,"[{'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,FALSE,0,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""","""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN."
633,49167306,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""",The company's infrastructure dictates that accessing external services must be done through a forward proxy.
634,49167306,,2,,"[{'score': 0.716147, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716147,FALSE,0,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""","Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to."
635,49167306,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""","I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS."
636,49167306,,4,,"[{'score': 0.824794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824794,FALSE,0,FALSE,0,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""",I can't figure out how to set the scheme.
637,49167306,,5,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.968123,TRUE,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""","Any help would be appreciated."""
638,49249692,,0,,"[{'score': 0.796749, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.796749,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3. I am getting the following error:Any ideas what I need to change here.""","""I am trying to use AWS Rekognition, detect_text API."
639,49249692,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3. I am getting the following error:Any ideas what I need to change here.""",I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3.
640,49249692,,2,,"[{'score': 0.669038, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.669038,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,TRUE,0.681699,FALSE,"""I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3. I am getting the following error:Any ideas what I need to change here.""","I am getting the following error:Any ideas what I need to change here."""
641,49126860,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.Stacktrace:""","""I am getting the below error while calling Watson Visual Recognition API through Java."
642,49126860,,1,,"[{'score': 0.712179, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.712179,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,FALSE,"""I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.Stacktrace:""","Any help will be highly appreciated.Stacktrace:"""
643,54897189,,0,,"[{'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638987,FALSE,0,TRUE,"""All of Amazon documentation on their Video Rekognition API are examples of videos that are stored in S3 bucket. Is there anyone out there who have tried using the API without storing the videos in S3 i.e. on local machine or GCS?""","""All of Amazon documentation on their Video Rekognition API are examples of videos that are stored in S3 bucket."
644,54897189,,1,,"[{'score': 0.836129, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.836129,TRUE,"""All of Amazon documentation on their Video Rekognition API are examples of videos that are stored in S3 bucket. Is there anyone out there who have tried using the API without storing the videos in S3 i.e. on local machine or GCS?""","Is there anyone out there who have tried using the API without storing the videos in S3 i.e. on local machine or GCS?"""
645,50722472,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""","""I have aofFace."
646,50722472,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""",I usedto findEyes Landmark.
647,50722472,,2,,"[{'score': 0.534382, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.534382,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,FALSE,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""","Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance"""
648,52440902,,0,,"[{'score': 0.709662, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.709662,FALSE,0,FALSE,0,TRUE,"""I'm using the ""DOCUMENT_TEXT_DETECTION"" option from the Google Cloud Vision API.It seems that it's returning correct text value, but incorrect coordinates bounding box.Why this problem occurred?Thank you.raw picturedraw bounding box picturereturningappendixdraw bounding box words and overall""","""I'm using the ""DOCUMENT_TEXT_DETECTION"" option from the Google Cloud Vision API.It seems that it's returning correct text value, but incorrect coordinates bounding box.Why this problem occurred?Thank you.raw"
649,52440902,,1,,"[{'score': 0.665764, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.665764,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.5538,FALSE,"""I'm using the ""DOCUMENT_TEXT_DETECTION"" option from the Google Cloud Vision API.It seems that it's returning correct text value, but incorrect coordinates bounding box.Why this problem occurred?Thank you.raw picturedraw bounding box picturereturningappendixdraw bounding box words and overall""","picturedraw bounding box picturereturningappendixdraw bounding box words and overall"""
650,48115069,,0,,"[{'score': 0.737292, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.737292,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""","""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException""."
651,48115069,,1,,"[{'score': 0.85465, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.85465,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""","Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:"""
652,50894208,,0,,"[{'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,TRUE,"""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.""","""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services."
653,50894208,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.""","I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor."
654,50894208,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.""","All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second."""
655,56126281,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""","""I have a .pb"
656,56126281,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",/ .tflite
657,56126281,,2,,"[{'score': 0.802757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802757,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",file from which I would like to construct the original model architecture in Tensorflow code.
658,56126281,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",What would be the easiest way to do it?
659,56126281,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",(The .pb
660,56126281,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",/ .tflite
661,56126281,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""","file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described."
662,56126281,,7,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.867767, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.867767,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""","The names were not really helpful, but Netron revealed something about the layers."
663,56126281,,8,,"[{'score': 0.765977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765977,FALSE,0,TRUE,0.58393,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",I just don't see what is the easiest way to construct the TF layers in TF code.
664,56126281,,9,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""","Also, I don't know which layers where frozen during the training process."""
665,56193287,,0,,"[{'score': 0.889102, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.889102,FALSE,0,FALSE,0,TRUE,"""I need to develop a face recognition system in using Angular with Azure Face API. However, the documentation for Azure Face API is in C#. Could anyone help me rewrite it to typescript?is the guildline for face recognition in Azure Face API""","""I need to develop a face recognition system in using Angular with Azure Face API."
666,56193287,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I need to develop a face recognition system in using Angular with Azure Face API. However, the documentation for Azure Face API is in C#. Could anyone help me rewrite it to typescript?is the guildline for face recognition in Azure Face API""","However, the documentation for Azure Face API is in C#."
667,56193287,,2,,"[{'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,TRUE,0.873263,TRUE,"""I need to develop a face recognition system in using Angular with Azure Face API. However, the documentation for Azure Face API is in C#. Could anyone help me rewrite it to typescript?is the guildline for face recognition in Azure Face API""","Could anyone help me rewrite it to typescript?is the guildline for face recognition in Azure Face API"""
668,55447535,,0,,"[{'score': 0.677069, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677069,FALSE,0,FALSE,0,TRUE,"""I want to implement the google cloud vision textDetection using a google cloud vision.I have install the composer from google cloud vision to the thirdparty vendor in codeignier.What my setup in construct is :and my function to call the OCR is :But before process the text detection i have run into an error :Which is this line :$imageAnnotator = new ImageAnnotatorClient([What could possible cause the error ? From the construct above i already include or require_once the Path to the Class.Is there something that i have missed in here ?Thank You""","""I want to implement the google cloud vision textDetection using a google cloud vision.I have install the composer from google cloud vision to the thirdparty vendor in codeignier.What my setup in construct is :and my function to call the OCR is :But before process the text detection i have run into an error :Which is this line :$imageAnnotator = new ImageAnnotatorClient([What could possible cause the error ?"
669,55447535,,1,,"[{'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,TRUE,0.733853,TRUE,"""I want to implement the google cloud vision textDetection using a google cloud vision.I have install the composer from google cloud vision to the thirdparty vendor in codeignier.What my setup in construct is :and my function to call the OCR is :But before process the text detection i have run into an error :Which is this line :$imageAnnotator = new ImageAnnotatorClient([What could possible cause the error ? From the construct above i already include or require_once the Path to the Class.Is there something that i have missed in here ?Thank You""","From the construct above i already include or require_once the Path to the Class.Is there something that i have missed in here ?Thank You"""
670,42188322,,0,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?""","""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am"
671,42188322,,1,,"[{'score': 0.690522, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.690522,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,FALSE,"""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?""",I missing something?
672,42188322,,2,,"[{'score': 0.542988, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.542988,FALSE,0,FALSE,0,TRUE,"""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?""","Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?"""
673,49231034,,0,,"[{'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.909883,TRUE,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""","""I try using custom vision service that could read bank notes."
674,49231034,,1,,"[{'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,FALSE,0,TRUE,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results.
675,49231034,,2,,"[{'score': 0.719382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719382,FALSE,0,FALSE,0,TRUE,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""","However, I want to build an android app.How am I be able to get the tag result and its prediction?"
676,49231034,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""","Thank you in advance."""
677,49912384,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""","""I am making a call to Google's Vision API using Ajax."
678,49912384,,1,,"[{'score': 0.660207, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660207,FALSE,0,TRUE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""",I have completed billing and received an API key.
679,49912384,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""","However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource."
680,49912384,,3,,"[{'score': 0.974578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974578,FALSE,0,FALSE,0,TRUE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""",Origin 'null' is therefore not allowed access.
681,49912384,,4,,"[{'score': 0.906122, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.906122,FALSE,0,FALSE,0,TRUE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""","The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension."
682,49912384,,5,,"[{'score': 0.740887, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.740887,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,FALSE,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""","If anybody can help that would be excellent."""
683,48733647,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working a Xamarin.Forms App that uses the Azure Face API. With this API you retrieve a JSON response (See Below).I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.This is how I set the JSON data to a string.""","""I am working a Xamarin.Forms App that uses the Azure Face API."
684,48733647,,1,,"[{'score': 0.690068, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.690068,FALSE,0,FALSE,0,TRUE,"""I am working a Xamarin.Forms App that uses the Azure Face API. With this API you retrieve a JSON response (See Below).I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.This is how I set the JSON data to a string.""","With this API you retrieve a JSON response (See Below).I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.This is how I set the JSON data to a string."""
685,51561234,,0,,"[{'score': 0.690365, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.690365,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""","""there seems to be very little to no documentation for AWS iOS text recognition inside an image."
686,51561234,,1,,"[{'score': 0.811324, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.811324,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""","I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?"
687,51561234,,2,,"[{'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.704683,TRUE,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""","I did not see that option when the json file was being created on the AWS web site...Any ideas?"""
688,41397369,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""","""I'm working with the Watson Visual Recognition service using Cygwin and curl."
689,41397369,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""","I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier."
690,41397369,,2,,"[{'score': 0.559677, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.874319, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.559677,FALSE,0,FALSE,0,TRUE,0.874319,FALSE,0,TRUE,0.716301,FALSE,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""",Other questions like this on Stack Overflow have suggested an internal service error.
691,41397369,,3,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""","I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing?"
692,41397369,,4,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""","How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:"""
693,49881417,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""in Watson Studio I am writing code in a Jupyter Notebook to use a Watson Visual Recognition custom model.It works ok with external images.I haven't been able yet to refer to an image I have uploaded to the Assets of my project. The url of the asset gets to a full page not the image only:Thank you""","""in Watson Studio I am writing code in a Jupyter Notebook to use a Watson Visual Recognition custom model.It works ok with external images.I haven't been able yet to refer to an image I have uploaded to the Assets of my project."
694,49881417,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""in Watson Studio I am writing code in a Jupyter Notebook to use a Watson Visual Recognition custom model.It works ok with external images.I haven't been able yet to refer to an image I have uploaded to the Assets of my project. The url of the asset gets to a full page not the image only:Thank you""","The url of the asset gets to a full page not the image only:Thank you"""
695,47749413,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,TRUE,0.92125,FALSE,0,TRUE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""","""I have used Google's Vision OCR a lot and it is really very accurate."
696,47749413,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.88939,TRUE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",I was wondering if I can do the OCR on a video file or video stream.
697,47749413,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""","Say, I have some surveillance video and I want to get all the text throughout that video."
698,47749413,,3,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""","In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision."
699,47749413,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""","I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done."
700,47749413,,5,,"[{'score': 0.827087, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.793846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.827087,FALSE,0,FALSE,0,FALSE,0,TRUE,0.793846,FALSE,0,FALSE,0,FALSE,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""","There might not be a perfect solution, but even if we get 50% of it, it's better than nothing."""
701,46189464,,0,,"[{'score': 0.817805, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.817805,FALSE,0,FALSE,0,TRUE,"""I have question on Watson Visual recognition Service of bluemix?Is 50 Images a minimum requirement to recognize a face of a person?What would happen if we train with less than 50 images? What would be the consistency of the output in terms of facial recognition?Requirement is, Retrieve the employee id of an employee by his facial(visual) recognition.Is it achievable with Watson visual recognition Service?In real time, it may be little hard to have 50 images of an employee or a person.?Thanks,Priyanka""","""I have question on Watson Visual recognition Service of bluemix?Is 50 Images a minimum requirement to recognize a face of a person?What would happen if we train with less than 50 images?"
702,46189464,,1,,"[{'score': 0.666746, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.666746,FALSE,0,FALSE,0,TRUE,"""I have question on Watson Visual recognition Service of bluemix?Is 50 Images a minimum requirement to recognize a face of a person?What would happen if we train with less than 50 images? What would be the consistency of the output in terms of facial recognition?Requirement is, Retrieve the employee id of an employee by his facial(visual) recognition.Is it achievable with Watson visual recognition Service?In real time, it may be little hard to have 50 images of an employee or a person.?Thanks,Priyanka""","What would be the consistency of the output in terms of facial recognition?Requirement is, Retrieve the employee id of an employee by his facial(visual) recognition.Is it achievable with Watson visual recognition Service?In real time, it may be little hard to have 50 images of an employee or a person.?Thanks,Priyanka"""
703,55894824,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""","""I'm working for the first time with IBM Watson Visual Recognition."
704,55894824,,1,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",My Python app needs to pass images that it's managing in memory to the service.
705,55894824,,2,,"[{'score': 0.777256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.777256,FALSE,0,FALSE,0,TRUE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""","However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files."
706,55894824,,3,,"[{'score': 0.534025, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.595823, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.534025,FALSE,0,FALSE,0,FALSE,0,TRUE,0.595823,FALSE,0,FALSE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call.
707,55894824,,4,,"[{'score': 0.860123, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.860123,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""","I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param."
708,55894824,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",When I added a mock filename (e.g.
709,55894824,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",'xyz123.jpg')
710,55894824,,7,,"[{'score': 0.66412, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.912528, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.66412,FALSE,0,FALSE,0,TRUE,0.912528,FALSE,0,FALSE,0,FALSE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",I get back the following error:Can I make calls to the analysis API using an image from memory?
711,55894824,,8,,"[{'score': 0.773049, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.773049,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""","If so, how?EDIT:This is essentially what I'm trying to do:Thanks"""
712,54425585,,0,,"[{'score': 0.516465, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.516465,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,FALSE,"""I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing.""","""I have some images of faces which I need to determine the rough age of the faces."
713,54425585,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing.""",Does Google Cloud Vision API have this feature to determine the age?
714,54425585,,2,,"[{'score': 0.812219, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.812219,TRUE,"""I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing.""","From the documentation, I don't see any such feature."
715,54425585,,3,,"[{'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.534455,TRUE,"""I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing.""","Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing."""
716,44184869,,0,,"[{'score': 0.627829, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.627829,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,FALSE,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""","""I created a GCP project to play around with the video-intelligence API."
717,44184869,,1,,"[{'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,FALSE,0,TRUE,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""","I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account."
718,44184869,,2,,"[{'score': 0.913043, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.99997, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.913043,FALSE,0,FALSE,0,FALSE,0,TRUE,0.99997,FALSE,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""","Baffled."""
719,56016618,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am implementing google vision API to convert pdf to text. I am at the end everything works fine but getting an error at the endI have usedOutput file is showing Output files:But after that getting.""","""I am implementing google vision API to convert pdf to text."
720,56016618,,1,,"[{'score': 0.67827, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.67827,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am implementing google vision API to convert pdf to text. I am at the end everything works fine but getting an error at the endI have usedOutput file is showing Output files:But after that getting.""","I am at the end everything works fine but getting an error at the endI have usedOutput file is showing Output files:But after that getting."""
721,56051643,,0,,"[{'score': 0.800316, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.800316,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,FALSE,"""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine""","""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public)."
722,56051643,,1,,"[{'score': 0.605831, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.605831,FALSE,0,FALSE,0,TRUE,"""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine""","Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine"""
723,40281166,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.This is how I am fetching the credentials, from the examples that the documentation provided. Is there something wrong?""","""Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.This is how I am fetching the credentials, from the examples that the documentation provided."
724,40281166,,1,,"[{'score': 0.508527, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.508527,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,FALSE,"""Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.This is how I am fetching the credentials, from the examples that the documentation provided. Is there something wrong?""","Is there something wrong?"""
725,53486685,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""**help me this a simple script to implement google vision API in python .i installed all requirements i need but still see that error**AttributeError: module 'google.cloud.vision' has no attribute 'Client'""","""**help me this a simple script to implement google vision API in python .i"
726,53486685,,1,,"[{'score': 0.916771, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.727798, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.916771,TRUE,0.727798,FALSE,0,TRUE,"""**help me this a simple script to implement google vision API in python .i installed all requirements i need but still see that error**AttributeError: module 'google.cloud.vision' has no attribute 'Client'""",installed all requirements i need but still see that error**AttributeError: module 'google.cloud.vision'
727,53486685,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""**help me this a simple script to implement google vision API in python .i installed all requirements i need but still see that error**AttributeError: module 'google.cloud.vision' has no attribute 'Client'""","has no attribute 'Client'"""
728,49752955,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but thereturns null only, And My 2nd Question is how to do brightness option...""","""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image."
729,49752955,,1,,"[{'score': 0.510724, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.510724,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but thereturns null only, And My 2nd Question is how to do brightness option...""","Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but"
730,49752955,,2,,"[{'score': 0.874319, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874319,FALSE,0,FALSE,0,TRUE,"""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but thereturns null only, And My 2nd Question is how to do brightness option...""","thereturns null only, And My 2nd Question is how to do brightness option..."""
731,48077116,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I use Google Cloud Vision Document OCR API. The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?""","""I use Google Cloud Vision Document OCR API."
732,48077116,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I use Google Cloud Vision Document OCR API. The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?""","The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?"""
733,46693204,,0,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713028,TRUE,0.618451,FALSE,0,TRUE,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""","""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons."
734,46693204,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""","So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way."
735,46693204,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""",The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front.
736,46693204,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""","With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web."
737,46693204,,4,,"[{'score': 0.833824, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.833824,TRUE,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""","We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?"""
738,46760602,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""","""I only manage to use the Emotion API subscription key for pictures but never for videos."
739,46760602,,1,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659112,TRUE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""",It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7.
740,46760602,,2,,"[{'score': 0.696951, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.696951,FALSE,0,FALSE,0,TRUE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""","In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains."
741,46760602,,3,,"[{'score': 0.611168, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.607897, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.611168,FALSE,0,FALSE,0,TRUE,0.607897,FALSE,0,FALSE,0,FALSE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""","I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet."
742,46760602,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""","I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me."
743,46760602,,5,,"[{'score': 0.518701, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.518701,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.994446,FALSE,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""","Does anyone have any idea what could be wrong?"""
744,42125608,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""","""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating)."
745,42125608,,1,,"[{'score': 0.718038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718038,FALSE,0,FALSE,0,TRUE,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project?
746,42125608,,2,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,FALSE,0,TRUE,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",At the moment I am using only the API key while I experiment with Google Vision.
747,42125608,,3,,"[{'score': 0.563171, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.756336, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.756336,FALSE,0,TRUE,0.563171,TRUE,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""","I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case."
748,42125608,,4,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.968123,TRUE,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""","Not used ACLs yet, or service accounts.Any help appreciated"""
749,48445901,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""","""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture?"
750,48445901,,1,,"[{'score': 0.521709, 'tone_id': 'anger', 'tone_name': 'Anger'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.521709,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""","As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings."""
751,38048320,,0,,"[{'score': 0.767076, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767076,FALSE,0,FALSE,0,TRUE,"""I am working on the python example for Cloud Vision API from.I have already setup the project and activated the service account with its key. I have also called theand entered my credentials.Here is my code (as derived from the python example of Vision API text detection):This is the error message I am getting:I want to be able to use my own project for the example and not the default (google.com:cloudsdktool).""","""I am working on the python example for Cloud Vision API from.I have already setup the project and activated the service account with its key."
752,38048320,,1,,"[{'score': 0.684254, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.684254,FALSE,0,FALSE,0,TRUE,"""I am working on the python example for Cloud Vision API from.I have already setup the project and activated the service account with its key. I have also called theand entered my credentials.Here is my code (as derived from the python example of Vision API text detection):This is the error message I am getting:I want to be able to use my own project for the example and not the default (google.com:cloudsdktool).""","I have also called theand entered my credentials.Here is my code (as derived from the python example of Vision API text detection):This is the error message I am getting:I want to be able to use my own project for the example and not the default (google.com:cloudsdktool)."""
753,41126381,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""","""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this?"
754,41126381,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""","I am properly encoded."""
755,51646568,,0,,"[{'score': 0.573563, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.717043, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.573563,FALSE,0,FALSE,0,TRUE,0.717043,FALSE,0,FALSE,0,FALSE,"""The release of Google Cloud Vision API had been on July 24, but what else was there later?There was no problem when using Document Text Detection of Vision API on July 25, but when I did the same thing on Augst 1, it became a error response ""Bad image data"".Not all images causes this error.  Several images causes ""Bad image data"" response.Using the not Document Text Detection but Text Detection API, I can get the correct response.Does anyone else have the same problem?My code is following.Response is following.""","""The release of Google Cloud Vision API had been on July 24, but what else was there later?There was no problem when using Document Text Detection of Vision API on July 25, but when I did the same thing on Augst 1, it became a error response ""Bad image data"".Not all images causes this error."
756,51646568,,1,,"[{'score': 0.556911, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.875324, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.556911,FALSE,0,FALSE,0,TRUE,0.875324,FALSE,0,FALSE,0,FALSE,"""The release of Google Cloud Vision API had been on July 24, but what else was there later?There was no problem when using Document Text Detection of Vision API on July 25, but when I did the same thing on Augst 1, it became a error response ""Bad image data"".Not all images causes this error.  Several images causes ""Bad image data"" response.Using the not Document Text Detection but Text Detection API, I can get the correct response.Does anyone else have the same problem?My code is following.Response is following.""","Several images causes ""Bad image data"" response.Using the not Document Text Detection but Text Detection API, I can get the correct response.Does anyone else have the same problem?My code is following.Response is following."""
757,47574353,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""","""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft."
758,47574353,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""","It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)"""
759,44543092,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""","""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots."
760,44543092,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""","For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !"""
761,42420031,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""","""Testing thecall of the Azure Computer Vision API from PHP."
762,42420031,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.875801, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,TRUE,0.875801,FALSE,0,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""","I have been able to get it to operate, but the images being saved locally are very, very poor quality."
763,42420031,,2,,"[{'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849827,FALSE,0,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""","Highly pixelated, very blurry, etc."
764,42420031,,3,,"[{'score': 0.693439, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.737379, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.693439,FALSE,0,TRUE,0.737379,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""","They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process?"
765,42420031,,4,,"[{'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,FALSE,0,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""",I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714.
766,42420031,,5,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""",Trying to create 115x115 thumbnail.Code at the moment.
767,42420031,,6,,"[{'score': 0.522652, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.541591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.522652,FALSE,0,FALSE,0,TRUE,0.541591,FALSE,0,FALSE,0,FALSE,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""","Have tried it with smartCropping set to both True and False."""
768,43277815,,0,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,TRUE,0.751512,FALSE,0,TRUE,"""We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you.""","""We are sending requests to Microsoft emotion API to find the emotions every second."
769,43277815,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you.""","But in a single go, we are able to send only 300 requests that is for 5 minutes."
770,43277815,,2,,"[{'score': 0.515512, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.515512,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you.""","After 5 minutes, it stops sending the responses."
771,43277815,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you.""","If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you."""
772,53886444,,0,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.926894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926894,TRUE,0.751512,FALSE,0,TRUE,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""","""I am building an OCR based solution to extract information from certain financial documents."
773,53886444,,1,,"[{'score': 0.524192, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.524192,FALSE,0,FALSE,0,TRUE,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""","As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep."
774,53886444,,2,,"[{'score': 0.897416, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.897416,FALSE,0,FALSE,0,TRUE,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""","He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated."
775,53886444,,3,,"[{'score': 0.538698, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.695447, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.538698,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.695447,FALSE,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""","I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer."""
776,50553795,,0,,"[{'score': 0.716569, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716569,FALSE,0,FALSE,0,TRUE,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""","""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE""."
777,50553795,,1,,"[{'score': 0.524901, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.524901,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775166,FALSE,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""",These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos?
778,50553795,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""",I don't see anything in the Places API.
779,50553795,,3,,"[{'score': 0.518058, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.518058,FALSE,0,FALSE,0,TRUE,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""","The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to."""
780,53043003,,0,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I want to compare two photos. When I connected to AWS I try to connect to:But have this error:Full code for getting information about photos:What is wrong? What I did wrong in this code?""","""I want to compare two photos."
781,53043003,,1,,"[{'score': 0.797427, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.702145, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.797427,FALSE,0,FALSE,0,TRUE,0.702145,FALSE,0,FALSE,0,FALSE,"""I want to compare two photos. When I connected to AWS I try to connect to:But have this error:Full code for getting information about photos:What is wrong? What I did wrong in this code?""",When I connected to AWS I try to connect to:But have this error:Full code for getting information about photos:What is wrong?
782,53043003,,2,,"[{'score': 0.730235, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.730235,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to compare two photos. When I connected to AWS I try to connect to:But have this error:Full code for getting information about photos:What is wrong? What I did wrong in this code?""","What I did wrong in this code?"""
783,53117918,,0,,"[{'score': 0.517371, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.566724, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.566724,FALSE,0,TRUE,0.517371,FALSE,0,FALSE,0,TRUE,0.681699,FALSE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""","""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake."
784,53117918,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.968123,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Just can't figure it out.
785,53117918,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""","I am working on a project that is available of two platforms, Android & iOS."
786,53117918,,3,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Its sort of a facial recognition app.
787,53117918,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""","When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections."
788,53117918,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms.
789,53117918,,6,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it.
790,53117918,,7,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Nothing else.
791,53117918,,8,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Just this code.
792,53117918,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",You can see that one is showing 3 collections while other is showing two.
793,53117918,,10,,"[{'score': 0.700439, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700439,FALSE,0,FALSE,0,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same.
794,53117918,,11,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Is there anything I need to change?
795,53117918,,12,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,TRUE,0.58393,TRUE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Is there any additional setup I need to do to make it work?Please let me know.
796,53117918,,13,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""","Thanks."""
797,36125830,,0,,"[{'score': 0.839221, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839221,FALSE,0,TRUE,0.58393,TRUE,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""","""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits?"
798,36125830,,1,,"[{'score': 0.904038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904038,FALSE,0,FALSE,0,TRUE,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""",I think it will greatly improve my result.I cannot find any result or hint on the internet at all.
799,36125830,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.984352,TRUE,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""","Any help is appreciated."""
800,39551502,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""","""I'm trying to send a binary image file to test the Microsoft Face API."
801,39551502,,1,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,TRUE,0.849827,FALSE,0,TRUE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",Using POSTMAN works perfectly and I get back aas expected.
802,39551502,,2,,"[{'score': 0.541591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.541591,FALSE,0,FALSE,0,TRUE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""","However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help."
803,39551502,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",Here's my code for sending requests.
804,39551502,,4,,"[{'score': 0.738988, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.738988,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,FALSE,0,FALSE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working.
805,39551502,,5,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.998976,TRUE,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""","Any ideas?"""
806,52308804,,0,,"[{'score': 0.504139, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.504139,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""","""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible."
807,52308804,,1,,"[{'score': 0.639168, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.639168,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""","It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field?"
808,52308804,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""",Has Cognitive a limit of fields?
809,52308804,,3,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""",Do I need to include any param to increase the number of fields to return?
810,52308804,,4,,"[{'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,TRUE,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""","Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)"""
811,54469189,,0,,"[{'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,FALSE,0,TRUE,"""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?Heres my request:""","""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need."
812,54469189,,1,,"[{'score': 0.815943, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.815943,TRUE,"""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?Heres my request:""","I just want the text from the menu, not all the coordinates that come with the response."
813,54469189,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.955608, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.955608,TRUE,"""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?Heres my request:""",I can't find anything about narrowing down the response in any documentation i've read.
814,54469189,,3,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,TRUE,0.615352,TRUE,"""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?Heres my request:""","Does someone know how to specify what fields get returned in the response?Heres my request:"""
815,54979768,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose. When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?""","""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose."
816,54979768,,1,,"[{'score': 0.583091, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583091,FALSE,0,FALSE,0,TRUE,"""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose. When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?""",When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over.
817,54979768,,2,,"[{'score': 0.857967, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.857967,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose. When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?""",It's simple and fine.
818,54979768,,3,,"[{'score': 0.998159, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.998159,TRUE,"""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose. When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?""","Anyone could suggest some way to investigate it?"""
819,54048657,,0,,"[{'score': 0.586193, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586193,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""","""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb"
820,54048657,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",file?
821,54048657,,2,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",I know we can get that via API calls to the azure custom vision service.
822,54048657,,3,,"[{'score': 0.937816, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.937816,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""","Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb"
823,54048657,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",file as there are tensors present.
824,54048657,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",Can we do the same for custom vision's model.pb
825,54048657,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs.
826,54048657,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",Thetakes a tensor of shapeand theoutputs a tensor of shape.
827,54048657,,8,,"[{'score': 0.682029, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.679542, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.682029,FALSE,0,FALSE,0,TRUE,0.679542,FALSE,0,FALSE,0,FALSE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""","If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong?"
828,54048657,,9,,"[{'score': 0.613464, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.996505, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.613464,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.996505,FALSE,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""","Any suggestions are welcome."""
829,52659767,,0,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""I have to try AWS Rekognition API's. And am new for PHP. So, Now am using PHP code for SandBox.Now, I got following Error,Note: my sandbox test cases URL:""","""I have to try AWS Rekognition API's."
830,52659767,,1,,"[{'score': 0.801295, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.801295,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have to try AWS Rekognition API's. And am new for PHP. So, Now am using PHP code for SandBox.Now, I got following Error,Note: my sandbox test cases URL:""",And am new for PHP.
831,52659767,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have to try AWS Rekognition API's. And am new for PHP. So, Now am using PHP code for SandBox.Now, I got following Error,Note: my sandbox test cases URL:""","So, Now am using PHP code for SandBox.Now, I got following Error,Note: my sandbox test cases URL:"""
832,55568129,,0,,"[{'score': 0.629992, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.629992,FALSE,0,FALSE,0,TRUE,"""I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly. How to use this API more appropriately so that I can extract MICR accurately.""","""I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly."
833,55568129,,1,,"[{'score': 0.882442, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.931276, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.931276,TRUE,0.882442,FALSE,0,TRUE,"""I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly. How to use this API more appropriately so that I can extract MICR accurately.""","How to use this API more appropriately so that I can extract MICR accurately."""
834,44167057,,0,,"[{'score': 0.543321, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543321,FALSE,0,FALSE,0,TRUE,"""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.""","""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back)."
835,44167057,,1,,"[{'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,FALSE,0,TRUE,"""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.""",There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed?
836,44167057,,2,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.""",Is there a different tag I need to use to get this information?
837,44167057,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.""","The images are also not horizontally flipped when viewing in the gallery."""
838,51109673,,0,,"[{'score': 0.878702, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.878702,TRUE,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""","""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services."
839,51109673,,1,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.90303, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.90303,FALSE,0,TRUE,0.525007,TRUE,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""","I want to run label detection, but have the same error while attempting the others, such as celebrity face detection."
840,51109673,,2,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance?
841,51109673,,3,,"[{'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,TRUE,0.845297,TRUE,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""","Is there somewhere in the AWS Documentation that explains what this error might indicate?"""
842,52365814,,0,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.75152,TRUE,"""Is possible to modify the maximum detections that Amazon Rekognition textDetection has? It only detects the first 50 occurences, but we need more (at least 60).If not, do you have any idea of how to make a workaround?Thanks!""","""Is possible to modify the maximum detections that Amazon Rekognition textDetection has?"
843,52365814,,1,,"[{'score': 0.698045, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.698045,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,FALSE,0,FALSE,"""Is possible to modify the maximum detections that Amazon Rekognition textDetection has? It only detects the first 50 occurences, but we need more (at least 60).If not, do you have any idea of how to make a workaround?Thanks!""","It only detects the first 50 occurences, but we need more (at least 60).If not, do you have any idea of how to make a workaround?Thanks!"""
844,53259815,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,How can I get that JSON from annotation structure? Is it possible?""","""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call."
845,53259815,,1,,"[{'score': 0.944551, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.944551,FALSE,0,FALSE,0,TRUE,"""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,How can I get that JSON from annotation structure? Is it possible?""","For example,How can I get that JSON from annotation structure?"
846,53259815,,2,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,How can I get that JSON from annotation structure? Is it possible?""","Is it possible?"""
847,51973564,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am creating an app in app inventor for that I need to detection emotions. So I used API (Google Vision API) to make my work easier. But got sucked in the screen when I access the url.Here is the screen shot""","""I am creating an app in app inventor for that I need to detection emotions."
848,51973564,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am creating an app in app inventor for that I need to detection emotions. So I used API (Google Vision API) to make my work easier. But got sucked in the screen when I access the url.Here is the screen shot""",So I used API (Google Vision API) to make my work easier.
849,51973564,,2,,"[{'score': 0.623139, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.623139,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am creating an app in app inventor for that I need to detection emotions. So I used API (Google Vision API) to make my work easier. But got sucked in the screen when I access the url.Here is the screen shot""","But got sucked in the screen when I access the url.Here is the screen shot"""
850,53865532,,0,,"[{'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,FALSE,0,TRUE,"""As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces. But its giving me error :I tried earlier using S3 bucket images and it worked properly. But right now I am trying to send the images without using S3 bucket.I am using a Lambda function, and I referred toMy code (edited version) :And the error that I am getting in the Cloudwatch is :Where am I going wrong?, as thethat I am passing is of theformat.""","""As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces."
851,53865532,,1,,"[{'score': 0.587665, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.587665,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,FALSE,"""As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces. But its giving me error :I tried earlier using S3 bucket images and it worked properly. But right now I am trying to send the images without using S3 bucket.I am using a Lambda function, and I referred toMy code (edited version) :And the error that I am getting in the Cloudwatch is :Where am I going wrong?, as thethat I am passing is of theformat.""",But its giving me error :I tried earlier using S3 bucket images and it worked properly.
852,53865532,,2,,"[{'score': 0.544189, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.544189,FALSE,0,FALSE,0,TRUE,"""As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces. But its giving me error :I tried earlier using S3 bucket images and it worked properly. But right now I am trying to send the images without using S3 bucket.I am using a Lambda function, and I referred toMy code (edited version) :And the error that I am getting in the Cloudwatch is :Where am I going wrong?, as thethat I am passing is of theformat.""","But right now I am trying to send the images without using S3 bucket.I am using a Lambda function, and I referred toMy code (edited version) :And the error that I am getting in the Cloudwatch is :Where am I going wrong?, as thethat I am passing is of theformat."""
853,42984821,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""","""I want to use the Microsoft Face API from an application in C++."
854,42984821,,1,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.615352,TRUE,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",The cpprest sdk allows me to send an url of image or binary data of image.
855,42984821,,2,,"[{'score': 0.601126, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.601126,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",The problem is that my image is not a file in disk but a cv::Mat in memory.
856,42984821,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""","I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image."
857,42984821,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""","How can I send to server the opencv Mat image through the client?"""
858,44832036,,0,,"[{'score': 0.537251, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.537251,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""","""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full"
859,44832036,,1,,"[{'score': 0.718921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718921,FALSE,0,FALSE,0,TRUE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",stack trace:API call makes the error:I am using the last version of javax.servlet-api
860,44832036,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""","(3.1.0),"
861,44832036,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",GAE (1.9.52) and Java 8.
862,44832036,,4,,"[{'score': 0.552234, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.511454, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.552234,FALSE,0,FALSE,0,TRUE,0.511454,FALSE,0,FALSE,0,FALSE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error.
863,44832036,,5,,"[{'score': 0.601841, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.601841,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,FALSE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy
864,44832036,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",I don't need extra thread for my servlet.
865,44832036,,7,,"[{'score': 0.590179, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.520224, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.590179,FALSE,0,FALSE,0,TRUE,0.520224,FALSE,0,FALSE,0,FALSE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture.
866,44832036,,8,,"[{'score': 0.853486, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.853486,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,FALSE,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""","Thank you for any advices."""
867,46061561,,0,,"[{'score': 0.820679, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.820679,FALSE,0,FALSE,0,TRUE,"""I am trying to do translate a document with google translate from the package google.cloudI already did:and the result was:then I called the package in Spyder (Python 3.5):I obtained this error:""","""I am trying to do translate a document with google translate from the package google.cloudI"
868,46061561,,1,,"[{'score': 0.688696, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.688696,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,FALSE,"""I am trying to do translate a document with google translate from the package google.cloudI already did:and the result was:then I called the package in Spyder (Python 3.5):I obtained this error:""","already did:and the result was:then I called the package in Spyder (Python 3.5):I obtained this error:"""
869,41711704,,0,,"[{'score': 0.808152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.808152,FALSE,0,TRUE,0.716301,TRUE,"""Is it possible to detect faces using Camera2 with Google Vision API only ? I could not find a way to integrate it.""","""Is it possible to detect faces using Camera2 with Google Vision API only ?"
870,41711704,,1,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,TRUE,0.856622,TRUE,"""Is it possible to detect faces using Camera2 with Google Vision API only ? I could not find a way to integrate it.""","I could not find a way to integrate it."""
871,49471062,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""how am I able to extract just the ""roll"" from this list which I got from Amazon Kinesis/Rekognition using Python 2.7?{u'FaceSearchResponse': [{u'DetectedFace': {u'BoundingBox': {u'Width': 0.10875, u'Top': 0.08555555, u'Left': 0.775, u'Height': 0.19333333}, u'Confidence': 99.82224, u'Pose': {u'Yaw': 39.53371, u'Roll': 10.791267, u'Pitch': -1.0082194}, u'Quality': {u'Sharpness': 99.93052, u'Brightness': 44.374504}, u'Landmarks': [{u'Y': 0.17006741, u'X': 0.81887186, u'Type': u'eyeLeft'}, {u'Y': 0.18348174, u'X': 0.8479081, u'Type': u'eyeRight'}, {u'Y': 0.21523575, u'X': 0.8444541, u'Type': u'nose'}, {u'Y': 0.2389706, u'X': 0.81935763, u'Type': u'mouthLeft'}, {u'Y': 0.2415149, u'X': 0.83268094, u'Type': u'mouthRight'}]}, u'MatchedFaces': []}], u'StreamProcessorInformation': {u'Status': u'RUNNING'}, u'InputInformation': {u'KinesisVideo': {u'ServerTimestamp': 1521934266.557, u'FrameOffsetInSeconds': 0.035999998450279236, u'StreamArn': u'arn:aws:kinesisvideo:us-east-1:086906171606:stream/AmazonRekognitionVS/1520802835146', u'FragmentNumber': u'91343852333181789275940108114159018792280348730', u'ProducerTimestamp': 1521934266.294}}}""","""how am I able to extract just the ""roll"" from this list which I got from Amazon Kinesis/Rekognition using Python 2.7?{u'FaceSearchResponse': [{u'DetectedFace': {u'BoundingBox': {u'Width': 0.10875, u'Top': 0.08555555, u'Left': 0.775, u'Height': 0.19333333}, u'Confidence': 99.82224, u'Pose': {u'Yaw': 39.53371, u'Roll': 10.791267, u'Pitch': -1.0082194}, u'Quality': {u'Sharpness': 99.93052, u'Brightness': 44.374504}, u'Landmarks': [{u'Y': 0.17006741, u'X': 0.81887186, u'Type': u'eyeLeft'}, {u'Y': 0.18348174, u'X': 0.8479081, u'Type': u'eyeRight'}, {u'Y': 0.21523575, u'X': 0.8444541, u'Type': u'nose'}, {u'Y': 0.2389706, u'X': 0.81935763, u'Type': u'mouthLeft'}, {u'Y': 0.2415149, u'X': 0.83268094, u'Type': u'mouthRight'}]}, u'MatchedFaces': []}], u'StreamProcessorInformation': {u'Status': u'RUNNING'}, u'InputInformation': {u'KinesisVideo': {u'ServerTimestamp': 1521934266.557,"
872,49471062,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""how am I able to extract just the ""roll"" from this list which I got from Amazon Kinesis/Rekognition using Python 2.7?{u'FaceSearchResponse': [{u'DetectedFace': {u'BoundingBox': {u'Width': 0.10875, u'Top': 0.08555555, u'Left': 0.775, u'Height': 0.19333333}, u'Confidence': 99.82224, u'Pose': {u'Yaw': 39.53371, u'Roll': 10.791267, u'Pitch': -1.0082194}, u'Quality': {u'Sharpness': 99.93052, u'Brightness': 44.374504}, u'Landmarks': [{u'Y': 0.17006741, u'X': 0.81887186, u'Type': u'eyeLeft'}, {u'Y': 0.18348174, u'X': 0.8479081, u'Type': u'eyeRight'}, {u'Y': 0.21523575, u'X': 0.8444541, u'Type': u'nose'}, {u'Y': 0.2389706, u'X': 0.81935763, u'Type': u'mouthLeft'}, {u'Y': 0.2415149, u'X': 0.83268094, u'Type': u'mouthRight'}]}, u'MatchedFaces': []}], u'StreamProcessorInformation': {u'Status': u'RUNNING'}, u'InputInformation': {u'KinesisVideo': {u'ServerTimestamp': 1521934266.557, u'FrameOffsetInSeconds': 0.035999998450279236, u'StreamArn': u'arn:aws:kinesisvideo:us-east-1:086906171606:stream/AmazonRekognitionVS/1520802835146', u'FragmentNumber': u'91343852333181789275940108114159018792280348730', u'ProducerTimestamp': 1521934266.294}}}""","u'FrameOffsetInSeconds': 0.035999998450279236, u'StreamArn': u'arn:aws:kinesisvideo:us-east-1:086906171606:stream/AmazonRekognitionVS/1520802835146', u'FragmentNumber': u'91343852333181789275940108114159018792280348730', u'ProducerTimestamp': 1521934266.294}}}"""
873,44804442,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""","""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7)."
874,44804442,,1,,"[{'score': 0.841134, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841134,FALSE,0,FALSE,0,TRUE,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""","Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt"
875,44804442,,2,,"[{'score': 0.650949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.650949,FALSE,0,FALSE,0,TRUE,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""","file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed."
876,44804442,,3,,"[{'score': 0.999739, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999739,TRUE,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Could anyone help?
877,44804442,,4,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""","Thanks!"""
878,52805384,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am receiving this error message when trying to upload to an AWS Lambda.  This is from the- exampleIn particular it says IAM is not authorized to perform iam:ListRoles nor iam:ListPolicies.I checked my IAM user's AWS Lambda ListFunctions in the AWS policy simulator which says it is working , although I do not know if this is relevant to my problem.thanks""","""I am receiving this error message when trying to upload to an AWS Lambda."
879,52805384,,1,,"[{'score': 0.672133, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672133,FALSE,0,FALSE,0,TRUE,"""I am receiving this error message when trying to upload to an AWS Lambda.  This is from the- exampleIn particular it says IAM is not authorized to perform iam:ListRoles nor iam:ListPolicies.I checked my IAM user's AWS Lambda ListFunctions in the AWS policy simulator which says it is working , although I do not know if this is relevant to my problem.thanks""","This is from the- exampleIn particular it says IAM is not authorized to perform iam:ListRoles nor iam:ListPolicies.I checked my IAM user's AWS Lambda ListFunctions in the AWS policy simulator which says it is working , although I do not know if this is relevant to my problem.thanks"""
880,43534783,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Can someone tryand see if it works?Here's the.All I get when I try running it is a black screen (after fixing). I don't get any errors in the logs either.""","""Can someone tryand see if it works?Here's the.All I get when I try running it is a black screen (after fixing)."
881,43534783,,1,,"[{'score': 0.792918, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.792918,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.856622,FALSE,"""Can someone tryand see if it works?Here's the.All I get when I try running it is a black screen (after fixing). I don't get any errors in the logs either.""","I don't get any errors in the logs either."""
882,44794681,,0,,"[{'score': 0.682143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682143,FALSE,0,FALSE,0,TRUE,"""I am working on Google Vision Api and I know how to send a request by using any image uri below.I want to use send request by using Php Client Library. When I look at the sample codes on Google Vision Documentation, it shows only sample for Local images of Remote images that work on Google Cloud Storage.As you can see my json request, I need to use Php Client library with remote url but I couldn't.Thank you.""","""I am working on Google Vision Api and I know how to send a request by using any image uri below.I want to use send request by using Php Client Library."
883,44794681,,1,,"[{'score': 0.600499, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.600499,FALSE,0,FALSE,0,TRUE,"""I am working on Google Vision Api and I know how to send a request by using any image uri below.I want to use send request by using Php Client Library. When I look at the sample codes on Google Vision Documentation, it shows only sample for Local images of Remote images that work on Google Cloud Storage.As you can see my json request, I need to use Php Client library with remote url but I couldn't.Thank you.""","When I look at the sample codes on Google Vision Documentation, it shows only sample for Local images of Remote images that work on Google Cloud Storage.As you can see my json request, I need to use Php Client library with remote url but I couldn't.Thank you."""
884,43910100,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?""","""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition."
885,43910100,,1,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?""",something has been changed in the type of calls between the 2 services.
886,43910100,,2,,"[{'score': 0.720099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.720099,FALSE,0,FALSE,0,TRUE,"""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?""","My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?"""
887,43844506,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results."
888,43844506,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg"
889,43844506,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Website: www.algen.comsg
890,43844506,,3,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR"
891,43844506,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang."
892,43844506,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Steve.
893,43844506,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Eric@alphabiofuels.sg
894,43844506,,7,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","www.alphabiofuels.sg""Result"
895,43844506,,8,,"[{'score': 0.555866, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.555866,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""","from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;"""
896,38322210,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""","""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java"
897,38322210,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""","serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS"
898,38322210,,2,,"[{'score': 0.776795, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.776795,FALSE,0,FALSE,0,TRUE,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""","MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a)"
899,38322210,,3,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",poster xml result:b) .java
900,38322210,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""","server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:"""
901,48145425,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am Deploying Google cloud vision Ocr in My angular2 webapp.but i am getting many of the errors when i add this code in my webapp code.please help me to sort out this.When I run this code ,it gives me this output:""","""I am Deploying Google cloud vision Ocr in My angular2 webapp.but"
902,48145425,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am Deploying Google cloud vision Ocr in My angular2 webapp.but i am getting many of the errors when i add this code in my webapp code.please help me to sort out this.When I run this code ,it gives me this output:""",i am getting many of the errors when i add this code in my webapp code.please
903,48145425,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am Deploying Google cloud vision Ocr in My angular2 webapp.but i am getting many of the errors when i add this code in my webapp code.please help me to sort out this.When I run this code ,it gives me this output:""","help me to sort out this.When I run this code ,it gives me this output:"""
904,51145859,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When an image gets captured it defaults to left orientation. So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions""","""When an image gets captured it defaults to left orientation."
905,51145859,,1,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""When an image gets captured it defaults to left orientation. So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions""","So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right)."
906,51145859,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When an image gets captured it defaults to left orientation. So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions""","I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions"""
907,48709133,,0,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.859009, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.859009,TRUE,0.80026,FALSE,0,TRUE,"""I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?""","""I'm using Google's Vision API to identify certain features in an image."
908,48709133,,1,,"[{'score': 0.568262, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568262,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?""","I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen."
909,48709133,,2,,"[{'score': 0.583315, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583315,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?""","It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?"""
910,45849380,,0,,"[{'score': 0.821036, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821036,FALSE,0,FALSE,0,TRUE,"""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?""","""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage."
911,45849380,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?""","My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here."
912,45849380,,2,,"[{'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,FALSE,0,TRUE,"""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?""","Is there a way I can check what my service client has in the way of permissions?"""
913,51778072,,0,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I am using the OCR functionality of mobile google vision. I used the sample example where there is a box surrounding the text being detected.I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text. How can  I improve the performance so that the box is more realtime when the camera is moving ?Thank you""","""I am using the OCR functionality of mobile google vision."
914,51778072,,1,,"[{'score': 0.618258, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.618258,FALSE,0,FALSE,0,TRUE,"""I am using the OCR functionality of mobile google vision. I used the sample example where there is a box surrounding the text being detected.I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text. How can  I improve the performance so that the box is more realtime when the camera is moving ?Thank you""","I used the sample example where there is a box surrounding the text being detected.I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text."
915,51778072,,2,,"[{'score': 0.706526, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.706526,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,FALSE,0,FALSE,"""I am using the OCR functionality of mobile google vision. I used the sample example where there is a box surrounding the text being detected.I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text. How can  I improve the performance so that the box is more realtime when the camera is moving ?Thank you""","How can  I improve the performance so that the box is more realtime when the camera is moving ?Thank you"""
916,36944481,,0,,"[{'score': 0.663387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.663387,FALSE,0,FALSE,0,TRUE,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""","""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue."
917,36944481,,1,,"[{'score': 0.629858, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.629858,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""","I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine."
918,36944481,,2,,"[{'score': 0.565134, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.565134,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,FALSE,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""",I have billing enabled and other curl requests work without issue.
919,36944481,,3,,"[{'score': 0.690931, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.690931,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,FALSE,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""","Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently."
920,36944481,,4,,"[{'score': 0.855393, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.86005, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.855393,FALSE,0,FALSE,0,TRUE,0.86005,FALSE,0,FALSE,0,FALSE,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""","I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host."""
921,50785134,,0,,"[{'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727988,TRUE,"""I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:""","""I am trying to perform OCR on images with some regional language which are supported by Google Vision API."
922,50785134,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:""","However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi."
923,50785134,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:""","Below given is my code:"""
924,55514176,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?""","""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature)."
925,55514176,,1,,"[{'score': 0.82973, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.82973,FALSE,0,FALSE,0,TRUE,"""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?""","This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well."
926,55514176,,2,,"[{'score': 0.772283, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.772283,FALSE,0,FALSE,0,TRUE,"""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?""","The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed""."
927,55514176,,3,,"[{'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.753251, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.753251,FALSE,0,TRUE,0.534455,TRUE,"""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?""","Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?"""
928,45563012,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""","""I'm trying develop a facial recognition system."
929,45563012,,1,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""",But the issue is it can be by-passed by a photo.
930,45563012,,2,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,FALSE,0,TRUE,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""",I'm using google vision api to detect faces.
931,45563012,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""",Is there a way to avoid detecting faces in a photo?
932,45563012,,4,,"[{'score': 0.797389, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.797389,FALSE,0,FALSE,0,TRUE,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""","Just want to know if there is a real person standing in front of the camera."""
933,33482245,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to develop a face tracking app using the Google Vision API ()This is my manifest:This is my code:This is the error in Logcat:Why does this happen (...on an Xperia Z3 compact 5.1)?UPDATE:I spotted a new error. I think it might be the reason why my code is not working.How can I resolve this problem?""","""I am trying to develop a face tracking app using the Google Vision API ()This is my manifest:This is my code:This is the error in Logcat:Why does this happen (...on an Xperia Z3 compact 5.1)?UPDATE:I spotted a new error."
934,33482245,,1,,"[{'score': 0.73524, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.892152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.73524,FALSE,0,FALSE,0,TRUE,0.892152,FALSE,0,TRUE,0.525007,FALSE,"""I am trying to develop a face tracking app using the Google Vision API ()This is my manifest:This is my code:This is the error in Logcat:Why does this happen (...on an Xperia Z3 compact 5.1)?UPDATE:I spotted a new error. I think it might be the reason why my code is not working.How can I resolve this problem?""","I think it might be the reason why my code is not working.How can I resolve this problem?"""
935,50780962,,0,,"[{'score': 0.610552, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.610552,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""","""I'm using google cloud vision api python to scan document to read the text from it."
936,50780962,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",Document is an invoice which has customer details and tables.
937,50780962,,2,,"[{'score': 0.759698, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.759698,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.92125,FALSE,0,FALSE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",Document to text data conversion works perfect.
938,50780962,,3,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",However the data is not sorted.
939,50780962,,4,,"[{'score': 0.765293, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765293,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",I'm not able to find a way how to sort the data because I need to extract few values from it.
940,50780962,,5,,"[{'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""","And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents."
941,50780962,,6,,"[{'score': 0.639002, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.639002,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""","How to structure it and read the data.Thanks in advance."""
942,55252358,,0,,"[{'score': 0.767921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767921,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","""I am using Google Cloud Vision to detect faces within images."
943,55252358,,1,,"[{'score': 0.898327, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.898327,FALSE,0,TRUE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","Earlier today, my code was working perfectly fine."
944,55252358,,2,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception."
945,55252358,,3,,"[{'score': 0.62991, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.62991,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,FALSE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","However, now it is giving me an error message that I am finding hard to debug."
946,55252358,,4,,"[{'score': 0.698522, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.698522,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,TRUE,0.909883,FALSE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue?"
947,55252358,,5,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""","This is my first time using Google Cloud Vision."""
948,49207943,,0,,"[{'score': 0.796123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.796123,FALSE,0,FALSE,0,TRUE,"""I am using Google cloud vision OCR to detect text. The displayed text is always 1. detected text, 2. each of the detected words. I only want to display the detected text.I am using the code fromwhere I set the type to Text Detectioninmethod.I also modified themethod to:This is my gradle:The detected text ofthat was displayed is:But I want it to only displayHow can I do it?""","""I am using Google cloud vision OCR to detect text."
949,49207943,,1,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.751512,FALSE,0,TRUE,"""I am using Google cloud vision OCR to detect text. The displayed text is always 1. detected text, 2. each of the detected words. I only want to display the detected text.I am using the code fromwhere I set the type to Text Detectioninmethod.I also modified themethod to:This is my gradle:The detected text ofthat was displayed is:But I want it to only displayHow can I do it?""","The displayed text is always 1. detected text, 2. each of the detected words."
950,49207943,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google cloud vision OCR to detect text. The displayed text is always 1. detected text, 2. each of the detected words. I only want to display the detected text.I am using the code fromwhere I set the type to Text Detectioninmethod.I also modified themethod to:This is my gradle:The detected text ofthat was displayed is:But I want it to only displayHow can I do it?""","I only want to display the detected text.I am using the code fromwhere I set the type to Text Detectioninmethod.I also modified themethod to:This is my gradle:The detected text ofthat was displayed is:But I want it to only displayHow can I do it?"""
951,55950028,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""","""I'm trying to get Google Vision API to work with my project but having trouble."
952,55950028,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""","I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json"
953,55950028,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""",file.
954,55950028,,3,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""",I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json
955,55950028,,4,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.926173, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926173,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""","file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow."""
956,54131993,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""","""Let's say I have the following document:I am sending it to Google Vision."
957,54131993,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""","What I get back, is the words and their boundingPoly."
958,54131993,,2,,"[{'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,TRUE,0.839577,TRUE,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""",What I would like is if I could somehow group the result by the rectangles shown on the image.
959,54131993,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""",Is there a way to detect those boxes?
960,54131993,,4,,"[{'score': 0.50654, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.50654,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,FALSE,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""","Canbe used somehow?"""
961,38679651,,0,,"[{'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,FALSE,0,TRUE,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""","""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e."
962,38679651,,1,,"[{'score': 0.679906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.679906,FALSE,0,FALSE,0,TRUE,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""","black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect?"
963,38679651,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""","Should I avoid using ""-"" in class names?"
964,38679651,,3,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""","Are there any other rules for the value of a classname?"""
965,40512241,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""","""I am working with Watson Visual Recognition and have successfully created a custom classifier."
966,40512241,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json
967,40512241,,2,,"[{'score': 0.674843, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.609997, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.885885, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.674843,FALSE,0,FALSE,0,TRUE,0.885885,FALSE,0,TRUE,0.609997,FALSE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious.
968,40512241,,3,,"[{'score': 0.783168, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.783168,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.946222,FALSE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",Any ideas on what I've missed?
969,40512241,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners-
970,40512241,,5,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""","An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold-"
971,40512241,,6,,"[{'score': 0.773486, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.593611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.593611,TRUE,0.773486,FALSE,0,TRUE,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""","A floating point value that specifies the minimum score a class must have to be displayed in the response."""
972,41562347,,0,,"[{'score': 0.708394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708394,FALSE,0,FALSE,0,TRUE,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""","""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions."
973,41562347,,1,,"[{'score': 0.658408, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.658408,FALSE,0,FALSE,0,TRUE,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""","This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results"""
974,45376533,,0,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.57374,TRUE,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""","""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities?"
975,45376533,,1,,"[{'score': 0.604739, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.755898, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.755898,FALSE,0,TRUE,0.604739,TRUE,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""","Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here."
976,45376533,,2,,"[{'score': 0.938429, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.588596, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.588596,FALSE,0,TRUE,0.938429,TRUE,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""","I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close."""
977,47430886,,0,,"[{'score': 0.67049, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.67049,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,FALSE,"""That is my code which I'm running on terminal, I tried but don't know why is that error coming. What changes should I do??""","""That is my code which I'm running on terminal, I tried but don't know why is that error coming."
978,47430886,,1,,"[{'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}]",FALSE,0,FALSE,0,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""That is my code which I'm running on terminal, I tried but don't know why is that error coming. What changes should I do??""","What changes should I do??"""
979,41388926,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,TRUE,0.525007,TRUE,"""I'd like to try, but I don't see a full example of the syntax for using the HTTP API. Assuming I have two images, how would I call this API from Python to retrieve a similarity score?""","""I'd like to try, but I don't see a full example of the syntax for using the HTTP API."
980,41388926,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I'd like to try, but I don't see a full example of the syntax for using the HTTP API. Assuming I have two images, how would I call this API from Python to retrieve a similarity score?""","Assuming I have two images, how would I call this API from Python to retrieve a similarity score?"""
981,52414541,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""","""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class."
982,52414541,,1,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""",I am following the example atunder theheader.
983,52414541,,2,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""",I am using C# and trying to build a classic console application.
984,52414541,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""","I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget."
985,52414541,,4,,"[{'score': 0.537636, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.537636,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""","I get a compile errorThe below is my code:I get error at the line below:Any hints please?"""
986,40156561,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to call the Google Vision REST API from a Xamarin.Forms app. I have the following code:-But this is returning me aerror message:""","""I'm trying to call the Google Vision REST API from a Xamarin.Forms app."
987,40156561,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to call the Google Vision REST API from a Xamarin.Forms app. I have the following code:-But this is returning me aerror message:""","I have the following code:-But this is returning me aerror message:"""
988,44362421,,0,,"[{'score': 0.849456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849456,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""","""We have a customer requirement to search similar images in a collection using Watson Visual Recognition."
989,44362421,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""",The documentation mentions that each collection can contain 1 million images.
990,44362421,,2,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""","Thus, I have the following questions:a) What is the maximum size of the image?b)"
991,44362421,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""",Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day.
992,44362421,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""","So, can only 25k images added to the collection/day?c)"
993,44362421,,5,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""",The customer has about 2 million images.
994,44362421,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""",How can we upload the images faster?d)
995,44362421,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""","Is there a separate plan available for bulk volumes?"""
996,50829647,,0,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.65875, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.65875,FALSE,0,TRUE,0.57374,TRUE,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""","""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled."
997,50829647,,1,,"[{'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.6731, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.6731,FALSE,0,TRUE,0.733853,TRUE,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""","Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible."""
998,53938721,,0,,"[{'score': 0.912032, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.912032,FALSE,0,FALSE,0,TRUE,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""","""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion."
999,53938721,,1,,"[{'score': 0.733193, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.733193,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""","The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions."
1000,53938721,,2,,"[{'score': 0.773968, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.773968,FALSE,0,FALSE,0,TRUE,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""","Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later."
1001,53938721,,3,,"[{'score': 0.556627, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.556627,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""","Initially, I hadand that was why it was giving me an error when trying to determine emotions."
1002,53938721,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""","The following code goes in themethod:"""
1003,55617828,,0,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:""","""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing."
1004,55617828,,1,,"[{'score': 0.660017, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.660017,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:""",That part was easy!
1005,55617828,,2,,"[{'score': 0.802215, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802215,FALSE,0,FALSE,0,TRUE,"""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:""","Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:"""
1006,54015677,,0,,"[{'score': 0.634637, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.634637,FALSE,0,FALSE,0,TRUE,"""I am using the following code for Rekognition.It is taking almost 1 min to finish the face detection between the 2 images.Is this normal? I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrongThank you""","""I am using the following code for Rekognition.It is taking almost 1 min to finish the face detection between the 2 images.Is this normal?"
1007,54015677,,1,,"[{'score': 0.632275, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.878702, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.632275,FALSE,0,TRUE,0.878702,TRUE,"""I am using the following code for Rekognition.It is taking almost 1 min to finish the face detection between the 2 images.Is this normal? I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrongThank you""","I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrongThank you"""
1008,45366479,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""","""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided."
1009,45366479,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""","This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g."
1010,45366479,,2,,"[{'score': 0.711374, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.64763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.711374,FALSE,0,FALSE,0,FALSE,0,TRUE,0.64763,TRUE,0.543112,FALSE,0,FALSE,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""","""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine."
1011,45366479,,3,,"[{'score': 0.580867, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.580867,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""",So maybe the size of the dataset is the problem.
1012,45366479,,4,,"[{'score': 0.664718, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.915522, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.915522,FALSE,0,TRUE,0.664718,TRUE,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""","However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?"""
1013,42631231,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""","""I want to do reverse image search in my android app."
1014,42631231,,1,,"[{'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804675,TRUE,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""",I need some api as powerful as google reverse image search.Is there any google reverse image search api for android?
1015,42631231,,2,,"[{'score': 0.708515, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708515,TRUE,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""",weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app?
1016,42631231,,3,,"[{'score': 0.726148, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.726148,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""","I foundbut seems google has detected and closed that server."""
1017,47924385,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""","""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output."
1018,47924385,,1,,"[{'score': 0.500545, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.500545,FALSE,0,FALSE,0,TRUE,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""","My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5)."
1019,47924385,,2,,"[{'score': 0.590922, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.59968, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.590922,FALSE,0,FALSE,0,TRUE,0.59968,FALSE,0,FALSE,0,FALSE,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""","On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)"""
1020,47779841,,0,,"[{'score': 0.87232, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87232,FALSE,0,FALSE,0,TRUE,"""I am using Amazon rekognition to compare faces in php (AWS SDK for phpthe documentation.The code belowI don't know php. How can I get the confidence of the picture?I try some things but I cant get it.""","""I am using Amazon rekognition to compare faces in php (AWS SDK for phpthe documentation.The code belowI don't know php."
1021,47779841,,1,,"[{'score': 0.534813, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.534813,FALSE,0,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,FALSE,0,FALSE,"""I am using Amazon rekognition to compare faces in php (AWS SDK for phpthe documentation.The code belowI don't know php. How can I get the confidence of the picture?I try some things but I cant get it.""","How can I get the confidence of the picture?I try some things but I cant get it."""
1022,49293605,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""","""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client."
1023,49293605,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""","On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition',"
1024,49293605,,2,,"[{'score': 0.590113, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.590113,TRUE,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""","region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!"""
1025,47885650,,0,,"[{'score': 0.657939, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.657939,FALSE,0,FALSE,0,TRUE,"""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?""","""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015."
1026,47885650,,1,,"[{'score': 0.738047, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.830561, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.738047,FALSE,0,FALSE,0,TRUE,0.830561,FALSE,0,TRUE,0.786991,FALSE,"""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?""","Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?"""
1027,40077320,,0,,"[{'score': 0.617758, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.617758,FALSE,0,FALSE,0,TRUE,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""","""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was"
1028,40077320,,1,,"[{'score': 0.775204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775204,FALSE,0,FALSE,0,TRUE,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""","far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up."
1029,40077320,,2,,"[{'score': 0.754976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.754976,TRUE,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""","Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require."
1030,40077320,,3,,"[{'score': 0.630779, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.624557, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.669292, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.630779,FALSE,0,FALSE,0,TRUE,0.669292,FALSE,0,TRUE,0.624557,FALSE,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""","Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?"""
1031,50417917,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS). Is internet connection required to use Amazon rekognition?""","""I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS)."
1032,50417917,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS). Is internet connection required to use Amazon rekognition?""","Is internet connection required to use Amazon rekognition?"""
1033,45727079,,0,,"[{'score': 0.616426, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.616426,FALSE,0,FALSE,0,TRUE,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""","""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests."
1034,45727079,,1,,"[{'score': 0.583707, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583707,FALSE,0,FALSE,0,TRUE,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""","The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone."
1035,45727079,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""","Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int"
1036,45727079,,3,,"[{'score': 0.748503, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.748503,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""","idUser):detectSafeSearch(String path):"""
1037,47671289,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""","""I'm working on image processing."
1038,47671289,,1,,"[{'score': 0.51225, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.812188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.51225,FALSE,0,FALSE,0,FALSE,0,TRUE,0.812188,FALSE,0,FALSE,0,FALSE,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""","So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos."
1039,47671289,,2,,"[{'score': 0.60618, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.60618,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,TRUE,0.91961,FALSE,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""","Is there any solution for this problem ?"""
1040,49589780,,0,,"[{'score': 0.512886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512886,FALSE,0,FALSE,0,TRUE,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""","""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I"
1041,49589780,,1,,"[{'score': 0.90888, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.90888,FALSE,0,TRUE,0.839577,TRUE,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""","am using the following links to guide me:Remember that I am not installing any library or using Composer."""
1042,49657006,,0,,"[{'score': 0.746076, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.746076,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""","""Hi I am new in python and I would love to convert the following data into a json file."
1043,49657006,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",I obtained this data set from Google Vision API.
1044,49657006,,2,,"[{'score': 0.944551, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.944551,FALSE,0,FALSE,0,TRUE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""","However, I have no idea how to approach this problem."
1045,49657006,,3,,"[{'score': 0.540444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.540444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Please help.
1046,49657006,,4,,"[{'score': 0.572005, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.572005,FALSE,0,FALSE,0,TRUE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated.
1047,49657006,,5,,"[{'score': 0.753368, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.753368,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""","Each four is split with commas.Thank you so much."""
1048,52534025,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to implement AWS Rekognition. Be default API allows only one image.I want to upload multiple image for face recognition. Is it possible?""","""I am trying to implement AWS Rekognition."
1049,52534025,,1,,"[{'score': 0.891685, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.891685,FALSE,0,FALSE,0,TRUE,"""I am trying to implement AWS Rekognition. Be default API allows only one image.I want to upload multiple image for face recognition. Is it possible?""",Be default API allows only one image.I want to upload multiple image for face recognition.
1050,52534025,,2,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""I am trying to implement AWS Rekognition. Be default API allows only one image.I want to upload multiple image for face recognition. Is it possible?""","Is it possible?"""
1051,44652637,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""","""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own."
1052,44652637,,1,,"[{'score': 0.734198, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.74948, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.734198,FALSE,0,FALSE,0,TRUE,0.74948,FALSE,0,FALSE,0,FALSE,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""","In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error."""
1053,52274071,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI? I am in need of your help.Thank You""","""I am currently trying to use Google's cloud vision API for my project."
1054,52274071,,1,,"[{'score': 0.69538, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69538,FALSE,0,FALSE,0,TRUE,"""I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI? I am in need of your help.Thank You""",The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination.
1055,52274071,,2,,"[{'score': 0.534147, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.534147,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI? I am in need of your help.Thank You""","But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI?"
1056,52274071,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI? I am in need of your help.Thank You""","I am in need of your help.Thank You"""
1057,53735551,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""","""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code."
1058,53735551,,1,,"[{'score': 0.561842, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.735509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.561842,FALSE,0,FALSE,0,TRUE,0.735509,FALSE,0,FALSE,0,FALSE,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""","It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge!"
1059,53735551,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""","Thanks.Here is my code."""
1060,50739245,,0,,"[{'score': 0.717675, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.628634, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.717675,TRUE,0.628634,FALSE,0,TRUE,"""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case . ATM using this""","""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table ."
1061,50739245,,1,,"[{'score': 0.841134, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841134,FALSE,0,FALSE,0,TRUE,"""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case . ATM using this""",like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case .
1062,50739245,,2,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case . ATM using this""","ATM using this"""
1063,38869448,,0,,"[{'score': 0.60985, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.908301, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.60985,FALSE,0,FALSE,0,TRUE,0.908301,FALSE,0,FALSE,0,FALSE,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""","""I've been aroundGoogle Vision APIbut I have a problem I can't really solve."
1064,38869448,,1,,"[{'score': 0.799144, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.799144,FALSE,0,FALSE,0,TRUE,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""","This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before?"
1065,38869448,,2,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""","I cannot figure out how to solve this problem..."""
1066,46090578,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""","""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them."
1067,46090578,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""",I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using.
1068,46090578,,2,,"[{'score': 0.63751, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.80288, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.63751,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80288,FALSE,0,FALSE,0,FALSE,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""","Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it."
1069,46090578,,3,,"[{'score': 0.582154, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.950868, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.582154,FALSE,0,FALSE,0,TRUE,0.950868,FALSE,0,FALSE,0,FALSE,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""","Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead."
1070,46090578,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""","Sorry andthank youall for taking the time to help me out."""
1071,51935317,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""","""Please I need an accurate and efficient sdk/api to extract text from images of any type."
1072,51935317,,1,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.682143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682143,FALSE,0,TRUE,0.615352,TRUE,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""","I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated."""
1073,49527406,,0,,"[{'score': 0.65274, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.710563, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.710563,FALSE,0,TRUE,0.65274,TRUE,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""","""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012)."
1074,49527406,,1,,"[{'score': 0.83831, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.745634, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.745634,FALSE,0,TRUE,0.83831,TRUE,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""","This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible?"
1075,49527406,,2,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.890872, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890872,FALSE,0,TRUE,0.786991,TRUE,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""",Is it possible to train the google vision with example images?
1076,49527406,,3,,"[{'score': 0.586752, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.586752,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,TRUE,0.968123,FALSE,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""","Or are there any other solutions that may be able to do this?Thank you!"""
1077,53464278,,0,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.564476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.564476,FALSE,0,TRUE,0.75152,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""","""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run."
1078,53464278,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Do I run this in Jupyter notebooks and would I need to install extra things?
1079,53464278,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Do I run it in the Anaconda Prompt?
1080,53464278,,3,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",I've tried both.
1081,53464278,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""","In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors."
1082,53464278,,5,,"[{'score': 0.790954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.790954,FALSE,0,FALSE,0,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",I've installed AWS already and curious whether there is more to install.
1083,53464278,,6,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.93534, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93534,FALSE,0,TRUE,0.5538,TRUE,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""","It would be greatly appreciated if anyone helped me."""
1084,47297666,,0,,"[{'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,FALSE,0,TRUE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""","""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud."
1085,47297666,,1,,"[{'score': 0.824794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824794,FALSE,0,FALSE,0,TRUE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Google takes 3 seconds to return a label response.
1086,47297666,,2,,"[{'score': 0.802462, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802462,FALSE,0,FALSE,0,TRUE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow
1087,47297666,,3,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",can I improve the speed of Google Cloud Vision's response?
1088,47297666,,4,,"[{'score': 0.733521, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.733521,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision.
1089,47297666,,5,,"[{'score': 0.816249, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.816249,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,FALSE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Results are much better!edit
1090,47297666,,6,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""","2:For reference, I moved the timer code to start at the beginning each python file."
1091,47297666,,7,,"[{'score': 0.519025, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.519025,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,FALSE,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""","This gives me a better idea of the response speed:"""
1092,53658135,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""","""I am working on Xamarin Forms to create a native application."
1093,53658135,,1,,"[{'score': 0.518879, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.518879,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",I am new to Xamarin and as well as google cloud vision API.
1094,53658135,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method.
1095,53658135,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""","However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly?"
1096,53658135,,4,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",I am not sure if I am doing it correctly.
1097,53658135,,5,,"[{'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.878702, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,TRUE,0.878702,TRUE,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""","If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib"""
1098,46752575,,0,,"[{'score': 0.599703, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.599703,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""","""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC."
1099,46752575,,1,,"[{'score': 0.710004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.816398, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.710004,FALSE,0,TRUE,0.816398,TRUE,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""","although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which"
1100,46752575,,2,,"[{'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,TRUE,0.822231,TRUE,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise?
1101,46752575,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""","(i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation"
1102,46752575,,4,,"[{'score': 0.92611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.698904, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.92611,FALSE,0,TRUE,0.698904,TRUE,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""","does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated."""
1103,53613704,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm triying to multiple language text detection with google cloud vision. But I have a problem. If I send the request text detection api endpoint this url;and this body;I'm getting the this error code;}What is a problem?""","""I'm triying to multiple language text detection with google cloud vision."
1104,53613704,,1,,"[{'score': 0.916667, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.916667,TRUE,0.931034,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,FALSE,"""I'm triying to multiple language text detection with google cloud vision. But I have a problem. If I send the request text detection api endpoint this url;and this body;I'm getting the this error code;}What is a problem?""",But I have a problem.
1105,53613704,,2,,"[{'score': 0.734478, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.734478,FALSE,0,FALSE,0,TRUE,"""I'm triying to multiple language text detection with google cloud vision. But I have a problem. If I send the request text detection api endpoint this url;and this body;I'm getting the this error code;}What is a problem?""","If I send the request text detection api endpoint this url;and this body;I'm getting the this error code;}What is a problem?"""
1106,48608334,,0,,"[{'score': 0.899126, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.899126,FALSE,0,TRUE,0.615352,TRUE,"""I am trying to develop android face recognition app. Starting from-how can I get camera preview FPS value?I understand that I can set requested FPS onwith, but I wantactualFPS value updated every second or every new frame. Is it possible to get or calculate it?""","""I am trying to develop android face recognition app."
1107,48608334,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to develop android face recognition app. Starting from-how can I get camera preview FPS value?I understand that I can set requested FPS onwith, but I wantactualFPS value updated every second or every new frame. Is it possible to get or calculate it?""","Starting from-how can I get camera preview FPS value?I understand that I can set requested FPS onwith, but I wantactualFPS value updated every second or every new frame."
1108,48608334,,2,,"[{'score': 0.514782, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.514782,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,FALSE,"""I am trying to develop android face recognition app. Starting from-how can I get camera preview FPS value?I understand that I can set requested FPS onwith, but I wantactualFPS value updated every second or every new frame. Is it possible to get or calculate it?""","Is it possible to get or calculate it?"""
1109,40372942,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""","""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images."
1110,40372942,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""",The text images are quite clean and it works 80% of the times.
1111,40372942,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""","The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening?"
1112,40372942,,3,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""","Is it the API or my image format could use some formatting?"""
1113,41903995,,0,,"[{'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,FALSE,0,TRUE,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""","""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database."
1114,41903995,,1,,"[{'score': 0.627032, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.695447, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.627032,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.695447,FALSE,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""","I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks"""
1115,44465669,,0,,"[{'score': 0.784247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784247,FALSE,0,FALSE,0,TRUE,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""","""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron."
1116,44465669,,1,,"[{'score': 0.716578, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.716578,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""","I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400."
1117,44465669,,2,,"[{'score': 0.698941, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.698941,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""","Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful."
1118,44465669,,3,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""","I have tried using and omitting the optional userData field, with a string and an image file."""
1119,56406581,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""","""Have 3 pages PDF which has scanned Id card."
1120,56406581,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""","Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file"""
1121,50893836,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to microsoft face API. Is it possible to use it in linux environment?I could not find any documentation about it in their website.Thanks.""","""I am new to microsoft face API."
1122,50893836,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am new to microsoft face API. Is it possible to use it in linux environment?I could not find any documentation about it in their website.Thanks.""","Is it possible to use it in linux environment?I could not find any documentation about it in their website.Thanks."""
1123,49256697,,0,,"[{'score': 0.623814, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.623814,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-""","""I'm new to android."
1124,49256697,,1,,"[{'score': 0.765599, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765599,FALSE,0,FALSE,0,TRUE,"""I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-""",I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen.
1125,49256697,,2,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.822231,TRUE,"""I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-""",If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible?
1126,49256697,,3,,"[{'score': 0.89289, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.89289,FALSE,0,FALSE,0,TRUE,"""I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-""","Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-"""
1127,43302771,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?""","""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API."
1128,43302771,,1,,"[{'score': 0.522246, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.522246,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?""","I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels."
1129,43302771,,2,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,TRUE,0.91961,TRUE,"""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?""","Does anyone know what's the minimum requirement?"""
1130,52475518,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""","""I have connected my python program with Google-cloud-vision through API."
1131,52475518,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""","I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image."
1132,52475518,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""","In both Python and JSON output I am successfully getting English Text, but No Bangla text."
1133,52475518,,3,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.796123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.796123,FALSE,0,TRUE,0.822231,TRUE,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""",Could you please help how to solve Bangla detection part.
1134,52475518,,4,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""","So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API."""
1135,44166894,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","""I've got a Watson Visual Recognition service bound to a Bluemix Application."
1136,44166894,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service.
1137,44166894,,2,,"[{'score': 0.564595, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.702543, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.564595,TRUE,0.702543,FALSE,0,FALSE,0,FALSE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails."
1138,44166894,,3,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.80026,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","The wait to become valid is documented, but it's causing a headache."
1139,44166894,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid.
1140,44166894,,5,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.618451,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key."
1141,44166894,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls."
1142,44166894,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application."
1143,44166894,,8,,"[{'score': 0.625096, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.515711, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.732192, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.625096,FALSE,0,FALSE,0,TRUE,0.732192,FALSE,0,TRUE,0.515711,FALSE,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""","This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?"""
1144,52172303,,0,,"[{'score': 0.71364, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.71364,FALSE,0,FALSE,0,TRUE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""","""So using Google's Vision API I'm trying to convert this table using Nodejs."
1145,52172303,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",It would be best if the result would be an array like.
1146,52172303,,2,,"[{'score': 0.561818, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.561818,FALSE,0,FALSE,0,TRUE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image.
1147,52172303,,3,,"[{'score': 0.515873, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.515873,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,TRUE,0.946222,FALSE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Using some kind of hacky solution I managed to merge the words.
1148,52172303,,4,,"[{'score': 0.915522, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.665938, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.915522,FALSE,0,TRUE,0.665938,TRUE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""","For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem?"
1149,52172303,,5,,"[{'score': 0.716757, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.722704, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.504017, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.716757,FALSE,0,FALSE,0,TRUE,0.722704,FALSE,0,TRUE,0.504017,FALSE,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""","Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)"""
1150,44856326,,0,,"[{'score': 0.67103, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67103,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""","""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result."
1151,44856326,,1,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized.
1152,44856326,,2,,"[{'score': 0.716804, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716804,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""","But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses."
1153,44856326,,3,,"[{'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",I'm authenticating using a API token.Why are the responses different?
1154,44856326,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",The requests are slightly different but not in a way which should affect the output.
1155,44856326,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""","Right?"""
1156,55500377,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""","""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project."
1157,55500377,,1,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""","As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested."
1158,55500377,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""","I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"":"
1159,55500377,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""","[{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},"""
1160,55514812,,0,,"[{'score': 0.720541, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.720541,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.see below errorAnd my code is""","""I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.see"
1161,55514812,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.see below errorAnd my code is""","below errorAnd my code is"""
1162,44910023,,0,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I am using below code to call google cloud vision api. not able to find out how can I set response timeout for the request in case I do not get response within a set timeout.""","""I am using below code to call google cloud vision api."
1163,44910023,,1,,"[{'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,TRUE,"""I am using below code to call google cloud vision api. not able to find out how can I set response timeout for the request in case I do not get response within a set timeout.""","not able to find out how can I set response timeout for the request in case I do not get response within a set timeout."""
1164,43124732,,0,,"[{'score': 0.871934, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.871934,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.704683,FALSE,"""Im trying to install thepython package, and I encounter the foloowing meesage whenn the installation fails:what could cause this? I installed other packages without problems.""","""Im trying to install thepython package, and I encounter the foloowing meesage whenn the installation fails:what could cause this?"
1165,43124732,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""Im trying to install thepython package, and I encounter the foloowing meesage whenn the installation fails:what could cause this? I installed other packages without problems.""","I installed other packages without problems."""
1166,40258634,,0,,"[{'score': 0.662817, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.662817,FALSE,0,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""","""I am trying to implement Microsoft emotion api in C# using code available on github."
1167,40258634,,1,,"[{'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.92125,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""",I followed all the steps given in.
1168,40258634,,2,,"[{'score': 0.805876, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.805876,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""","I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'."
1169,40258634,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""",Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'.
1170,40258634,,4,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""","Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance."""
1171,53081398,,0,,"[{'score': 0.520966, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.520966,FALSE,0,FALSE,0,TRUE,"""Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?Apparently there is new functionality released, described here:Improved detection models are now available for the following features:Logo DetectionText Detection (OCR)Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.We'll support both the current model and the new model the next 90 days. After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.This is what my code looks like now:""","""Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?Apparently there is new functionality released, described here:Improved detection models are now available for the following features:Logo DetectionText Detection (OCR)Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.We'll support both the current model and the new model the next 90 days."
1172,53081398,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?Apparently there is new functionality released, described here:Improved detection models are now available for the following features:Logo DetectionText Detection (OCR)Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.We'll support both the current model and the new model the next 90 days. After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.This is what my code looks like now:""","After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.This is what my code looks like now:"""
1173,54345710,,0,,"[{'score': 0.517659, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.517659,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""I'm facing an issue in the Azure Face API. It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same.""","""I'm facing an issue in the Azure Face API."
1174,54345710,,1,,"[{'score': 0.599484, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.606023, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.606023,FALSE,0,TRUE,0.599484,TRUE,"""I'm facing an issue in the Azure Face API. It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same.""","It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same."""
1175,44096947,,0,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80026,FALSE,0,TRUE,"""Android Dev withI see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Androidwe use thedifferent from. Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:""","""Android Dev withI see that they are all libraries of the translator."
1176,44096947,,1,,"[{'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,FALSE,0,TRUE,"""Android Dev withI see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Androidwe use thedifferent from. Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:""",The cloud-vision has two libraries as well but in the Androidwe use thedifferent from.
1177,44096947,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Android Dev withI see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Androidwe use thedifferent from. Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:""","Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:"""
1178,49669981,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision OCR API to read text from image.var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")""","""I am trying to use google cloud vision OCR API to read text from image.var"
1179,49669981,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision OCR API to read text from image.var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")""",response = client.DetectText(image);
1180,49669981,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision OCR API to read text from image.var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")""","This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")"""
1181,47838580,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte? How can I get a bitmap in ruby?""","""The google vision API requires a bitmap sent as an argument."
1182,47838580,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte? How can I get a bitmap in ruby?""",I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte?
1183,47838580,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte? How can I get a bitmap in ruby?""","How can I get a bitmap in ruby?"""
1184,35532645,,0,,"[{'score': 0.635197, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635197,FALSE,0,FALSE,0,TRUE,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""","""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message."
1185,35532645,,1,,"[{'score': 0.50118, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.50118,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""","But the ""Cloud Vision API"" is enabled for the project."
1186,35532645,,2,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.963382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.963382,FALSE,0,TRUE,0.822231,TRUE,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""","Any help is appreciated.Error Details from Google APIs Explorer"""
1187,56391486,,0,,"[{'score': 0.660937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660937,FALSE,0,FALSE,0,TRUE,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""","""i have a google vision api object localization request that returns a response fine ."
1188,56391486,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.856622,TRUE,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines .
1189,56391486,,2,,"[{'score': 0.742832, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.561417, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.742832,FALSE,0,FALSE,0,FALSE,0,TRUE,0.561417,FALSE,0,FALSE,0,FALSE,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""","The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)"""
1190,53744481,,0,,"[{'score': 0.51418, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.51418,FALSE,0,FALSE,0,TRUE,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:Note :img_stris a byte arrayFirst error: sendall() argument 1 must be string or buffer, not dictReason in my understanding: { ""Bytes"" : image } is a Json and NOT a stringMy Solution: Make the json a string ( not sure whether I can concatenate img_str ( a byte array )Now error: Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)QuestionHow do I concatenate a byte array (img_str) with strings without losing the array ?Can i convertimagevariable to string WITHOUTgetting the can't decode byte 0xffexception ? orCan we do something else to overcome this issue ?Thanks in advance guys !!""","""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:Note :img_stris a byte arrayFirst error: sendall() argument 1 must be string or buffer, not dictReason in my understanding: { ""Bytes"" : image } is a Json and NOT a stringMy Solution: Make the json a string ( not sure whether I can concatenate img_str ( a byte array )Now error: Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)QuestionHow do I concatenate a byte array (img_str) with strings without losing the array ?Can i convertimagevariable to string WITHOUTgetting the can't decode byte 0xffexception ?"
1191,53744481,,1,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:Note :img_stris a byte arrayFirst error: sendall() argument 1 must be string or buffer, not dictReason in my understanding: { ""Bytes"" : image } is a Json and NOT a stringMy Solution: Make the json a string ( not sure whether I can concatenate img_str ( a byte array )Now error: Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)QuestionHow do I concatenate a byte array (img_str) with strings without losing the array ?Can i convertimagevariable to string WITHOUTgetting the can't decode byte 0xffexception ? orCan we do something else to overcome this issue ?Thanks in advance guys !!""","orCan we do something else to overcome this issue ?Thanks in advance guys !!"""
1192,38984901,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been testing the Microsoft Computer Vision API with some pictures I took and it is not able to properly identify what I am uploading. Is there a way I could teach it what I am uploading is?My tests have been using.A sample image:It should include ""bottle"" in the tags at the very least.""","""I have been testing the Microsoft Computer Vision API with some pictures I took and it is not able to properly identify what I am uploading."
1193,38984901,,1,,"[{'score': 0.802309, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802309,FALSE,0,FALSE,0,TRUE,"""I have been testing the Microsoft Computer Vision API with some pictures I took and it is not able to properly identify what I am uploading. Is there a way I could teach it what I am uploading is?My tests have been using.A sample image:It should include ""bottle"" in the tags at the very least.""","Is there a way I could teach it what I am uploading is?My tests have been using.A sample image:It should include ""bottle"" in the tags at the very least."""
1194,36655630,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0,"
1195,36655630,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","Google.Apis.Auth v1.10.0,"
1196,36655630,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",Google.Apis.Vision.v1
1197,36655630,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",v1.12.0.45Performing
1198,36655630,,4,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.698904, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.698904,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected..."
1199,36655630,,5,,"[{'score': 0.846863, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.846863,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results.."
1200,36655630,,6,,"[{'score': 0.786249, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786249,FALSE,0,TRUE,0.58393,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses."
1201,36655630,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response."
1202,36655630,,8,,"[{'score': 0.504894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.670352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.504894,FALSE,0,TRUE,0.670352,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?"
1203,36655630,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""","Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16"""
1204,51389440,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""","""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image."
1205,51389440,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",The image is about 2000x3000 pixels and is a picture of a contract.
1206,51389440,,2,,"[{'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,TRUE,0.825035,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",I want to get all the text and the bounding boxes.
1207,51389440,,3,,"[{'score': 0.949606, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,TRUE,0.949606,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",The image DPI is over 300 and it's quality is very clear.
1208,51389440,,4,,"[{'score': 0.624803, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.624803,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.
1209,51389440,,5,,"[{'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",This time it recognized text that it did not recognize before.
1210,51389440,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",Why would it do this?
1211,51389440,,7,,"[{'score': 0.786514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786514,FALSE,0,FALSE,0,TRUE,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""","If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?"""
1212,54847537,,0,,"[{'score': 0.85437, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85437,TRUE,"""Is it possibile restrict the search of Google vision api filtering by specific countries or geographic zones ? By language ?""","""Is it possibile restrict the search of Google vision api filtering by specific countries or geographic zones ?"
1213,54847537,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is it possibile restrict the search of Google vision api filtering by specific countries or geographic zones ? By language ?""","By language ?"""
1214,50715542,,0,,"[{'score': 0.82515, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.82515,FALSE,0,FALSE,0,TRUE,"""Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.I am getting error in below line. And tried to open this url is not working:Below is my design code.Below is my code which worte in button click for display in labelI used below example url:I created service key account also in google for json file.""","""Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.I am getting error in below line."
1215,50715542,,1,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.I am getting error in below line. And tried to open this url is not working:Below is my design code.Below is my code which worte in button click for display in labelI used below example url:I created service key account also in google for json file.""","And tried to open this url is not working:Below is my design code.Below is my code which worte in button click for display in labelI used below example url:I created service key account also in google for json file."""
1216,53701338,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""","""I am trying to develop C# Google Vision API function.the"
1217,53701338,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",code is supposed to compile into dll and it should run to do the following steps.get
1218,53701338,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""","the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error."
1219,53701338,,3,,"[{'score': 0.618003, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.893636, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.618003,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,TRUE,0.893636,FALSE,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this?
1220,53701338,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""","I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file..."""
1221,52219324,,0,,"[{'score': 0.752043, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.752043,FALSE,0,FALSE,0,TRUE,"""I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.How can I convert it to human-readable text with utf-8 format?Thanks all of your answer, may be I post my code is more easily for all of you.Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success.try:    code = requests.post(''+GOOGLE_API_KEY, data=params,headers=headers)except requests.exceptions.ConnectionError:    print('Request error')Thank you""","""I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.How can I convert it to human-readable text with utf-8 format?Thanks all of your answer, may be I post my code is more easily for all of you.Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success.try:"
1222,52219324,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.How can I convert it to human-readable text with utf-8 format?Thanks all of your answer, may be I post my code is more easily for all of you.Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success.try:    code = requests.post(''+GOOGLE_API_KEY, data=params,headers=headers)except requests.exceptions.ConnectionError:    print('Request error')Thank you""","code = requests.post(''+GOOGLE_API_KEY,"
1223,52219324,,2,,"[{'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,FALSE,0,TRUE,"""I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.How can I convert it to human-readable text with utf-8 format?Thanks all of your answer, may be I post my code is more easily for all of you.Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success.try:    code = requests.post(''+GOOGLE_API_KEY, data=params,headers=headers)except requests.exceptions.ConnectionError:    print('Request error')Thank you""","data=params,headers=headers)except requests.exceptions.ConnectionError:    print('Request error')Thank you"""
1224,39540741,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""","""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX."
1225,39540741,,1,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""","I used Google's Cloud Vision API Text Recognition, but the results were unreliable."
1226,39540741,,2,,"[{'score': 0.659886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659886,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""","Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results."
1227,39540741,,3,,"[{'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.540225, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.540225,FALSE,0,TRUE,0.511119,TRUE,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""","The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk."
1228,39540741,,4,,"[{'score': 0.922131, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.922131,FALSE,0,FALSE,0,TRUE,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""","Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?"""
1229,56184573,,0,,"[{'score': 0.58533, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.700747, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.58533,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700747,FALSE,0,FALSE,0,FALSE,"""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the sample photos i put in the faces folder. thank you !""","""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the"
1230,56184573,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the sample photos i put in the faces folder. thank you !""",sample photos i put in the faces folder.
1231,56184573,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the sample photos i put in the faces folder. thank you !""","thank you !"""
1232,55766327,,0,,"[{'score': 0.827997, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827997,FALSE,0,FALSE,0,TRUE,"""I have videos stored at amazon s3 cloud and i need to analyze it using google vision cloud and nodejs. Please help me.""","""I have videos stored at amazon s3 cloud and i need to analyze it using google vision cloud and nodejs."
1233,55766327,,1,,"[{'score': 0.540444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.540444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have videos stored at amazon s3 cloud and i need to analyze it using google vision cloud and nodejs. Please help me.""","Please help me."""
1234,44119233,,0,,"[{'score': 0.597086, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.597086,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to run thebut I'm getting this error:Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;These are the characteristics of my project:1-POM.xml file2- Class3- Google Cloud SDKI used the command: gcloud auth application-default loginCan anybody help me?Thanks.""","""I'm trying to run thebut I'm getting this error:Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;These"
1235,44119233,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run thebut I'm getting this error:Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;These are the characteristics of my project:1-POM.xml file2- Class3- Google Cloud SDKI used the command: gcloud auth application-default loginCan anybody help me?Thanks.""",are the characteristics of my project:1-POM.xml
1236,44119233,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run thebut I'm getting this error:Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;These are the characteristics of my project:1-POM.xml file2- Class3- Google Cloud SDKI used the command: gcloud auth application-default loginCan anybody help me?Thanks.""","file2- Class3- Google Cloud SDKI used the command: gcloud auth application-default loginCan anybody help me?Thanks."""
1237,55683585,,0,,"[{'score': 0.797389, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.797389,FALSE,0,FALSE,0,TRUE,"""I am trying to access image rekognition service of AWS using google-App-script, for this, I am trying to generate an AWS signature for API call, but response showing an error message.when I am trying with postman using authorization (AWS signature) it is working fine.below is my google app script code in which I am calling the main functionI am not able to figure out what I am doing wrong in the code, or it is the wrong method of generating the signature, please help.""","""I am trying to access image rekognition service of AWS using google-App-script, for this, I am trying to generate an AWS signature for API call, but response showing an error message.when"
1238,55683585,,1,,"[{'score': 0.591444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.509723, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.591444,FALSE,0,FALSE,0,TRUE,0.509723,FALSE,0,FALSE,0,FALSE,"""I am trying to access image rekognition service of AWS using google-App-script, for this, I am trying to generate an AWS signature for API call, but response showing an error message.when I am trying with postman using authorization (AWS signature) it is working fine.below is my google app script code in which I am calling the main functionI am not able to figure out what I am doing wrong in the code, or it is the wrong method of generating the signature, please help.""","I am trying with postman using authorization (AWS signature) it is working fine.below is my google app script code in which I am calling the main functionI am not able to figure out what I am doing wrong in the code, or it is the wrong method of generating the signature, please help."""
1239,50210568,,0,,"[{'score': 0.660937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660937,FALSE,0,FALSE,0,TRUE,"""I want to build a deep learning chatbot application which accepts image as input. I have built a lambda function integrating AWS rekognition that accepts image.Now, i want to extend this lambda function, and connect it to Amazon Lex bot , where user can upload the image for analysis.""","""I want to build a deep learning chatbot application which accepts image as input."
1240,50210568,,1,,"[{'score': 0.631014, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.631014,FALSE,0,FALSE,0,TRUE,"""I want to build a deep learning chatbot application which accepts image as input. I have built a lambda function integrating AWS rekognition that accepts image.Now, i want to extend this lambda function, and connect it to Amazon Lex bot , where user can upload the image for analysis.""","I have built a lambda function integrating AWS rekognition that accepts image.Now, i want to extend this lambda function, and connect it to Amazon Lex bot , where user can upload the image for analysis."""
1241,43771382,,0,,"[{'score': 0.602762, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.602762,FALSE,0,FALSE,0,TRUE,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""","""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json'"
1242,43771382,,1,,"[{'score': 0.67103, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67103,FALSE,0,FALSE,0,TRUE,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""","and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated"""
1243,43586009,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""","""I have access to aws account with username."
1244,43586009,,1,,"[{'score': 0.850128, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.850128,FALSE,0,FALSE,0,TRUE,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""","I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using)."
1245,43586009,,2,,"[{'score': 0.756308, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.756308,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.733853,FALSE,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""","It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than."
1246,43586009,,3,,"[{'score': 0.777772, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681087, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.777772,FALSE,0,FALSE,0,TRUE,0.681087,FALSE,0,FALSE,0,FALSE,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""","But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have."
1247,43586009,,4,,"[{'score': 0.543113, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.598602, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.543113,FALSE,0,FALSE,0,FALSE,0,TRUE,0.598602,FALSE,0,FALSE,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""","All I have is username and password to aws account.When I check the UI users page, I have restricted access"""
1248,48896074,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file usingand now the file shows strange characters.shows(after sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?""","""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR)."
1249,48896074,,1,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file usingand now the file shows strange characters.shows(after sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?""",I did some processing on the file usingand now the file shows strange characters.shows(after
1250,48896074,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file usingand now the file shows strange characters.shows(after sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?""","sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?"""
1251,44556228,,0,,"[{'score': 0.612109, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.612109,FALSE,0,FALSE,0,TRUE,"""I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:I getting a bad request error. Need help with this. The format of my json object (request) is as follows:""","""I want to send a json object while by using the http POST method to the Google Vision API."
1252,44556228,,1,,"[{'score': 0.729019, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.729019,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:I getting a bad request error. Need help with this. The format of my json object (request) is as follows:""",I am using the following code:I getting a bad request error.
1253,44556228,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:I getting a bad request error. Need help with this. The format of my json object (request) is as follows:""",Need help with this.
1254,44556228,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:I getting a bad request error. Need help with this. The format of my json object (request) is as follows:""","The format of my json object (request) is as follows:"""
1255,32558923,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""","""I'm using Google's Vision API BarcodeScanner on my project."
1256,32558923,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""",I would like to interrupt scanning once a code has been scanned and store the content in another activity.
1257,32558923,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""",How can i do that ?
1258,32558923,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""","There are so many classes and 'interconnections' :xThanks !"""
1259,51183169,,0,,"[{'score': 0.904882, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,TRUE,0.904882,TRUE,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""","""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition?"
1260,51183169,,1,,"[{'score': 0.575046, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.782501, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.782501,FALSE,0,TRUE,0.575046,TRUE,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""","I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome."""
1261,43070014,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Iam doing a project with nodejs and aws.I am using WebRTC and taking photo.After i am taking photos base64 data and posting nodejs and i am putting it my aws console and i am using it for detectfaces but its giving error.But i am adding photo from my aws console manually detect faces not giving a error.My codes here : MY WEBCAM JS : this is giving a base64 for me.and i am trying post with POSTMAN CHROME EXTENSION to my nodejs i can put it well but i can't  using a detect faces.My nodejs :MY ERROR :How can i do this please help me i could not fint anything.Thanks for help.""","""Iam doing a project with nodejs and aws.I am using WebRTC and taking photo.After i am taking photos base64 data and posting nodejs and i am putting it my aws console and i am using it for detectfaces but its giving error.But i am adding photo from my aws console manually detect faces not giving a error.My codes here : MY WEBCAM JS : this is giving a base64 for me.and"
1262,43070014,,1,,"[{'score': 0.59947, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.577798, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.515981, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.59947,FALSE,0,FALSE,0,TRUE,0.515981,FALSE,0,TRUE,0.577798,FALSE,"""Iam doing a project with nodejs and aws.I am using WebRTC and taking photo.After i am taking photos base64 data and posting nodejs and i am putting it my aws console and i am using it for detectfaces but its giving error.But i am adding photo from my aws console manually detect faces not giving a error.My codes here : MY WEBCAM JS : this is giving a base64 for me.and i am trying post with POSTMAN CHROME EXTENSION to my nodejs i can put it well but i can't  using a detect faces.My nodejs :MY ERROR :How can i do this please help me i could not fint anything.Thanks for help.""","i am trying post with POSTMAN CHROME EXTENSION to my nodejs i can put it well but i can't  using a detect faces.My nodejs :MY ERROR :How can i do this please help me i could not fint anything.Thanks for help."""
1263,47427567,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to testwhich uses Google's Vision API. In, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?""","""I am trying to testwhich uses Google's Vision API."
1264,47427567,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am trying to testwhich uses Google's Vision API. In, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?""","In, I tried to view what fields the argument has."
1265,47427567,,2,,"[{'score': 0.674065, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.674065,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,FALSE,"""I am trying to testwhich uses Google's Vision API. In, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?""",The problem is that field names are cryptic.
1266,47427567,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to testwhich uses Google's Vision API. In, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?""","Why are the field names cryptic?"""
1267,55376350,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am facing an issue with Azure Computer Vision API. If I send a request with contentType = application/json and image URL in JSON request body things work fine but on sending a binary image(base 64 encoded) with contentType = application/octet-stream it gives me ImageFormatInvalid in the respContent-Type: multipart/form-data and asking input as binary image data""","""I am facing an issue with Azure Computer Vision API."
1268,55376350,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am facing an issue with Azure Computer Vision API. If I send a request with contentType = application/json and image URL in JSON request body things work fine but on sending a binary image(base 64 encoded) with contentType = application/octet-stream it gives me ImageFormatInvalid in the respContent-Type: multipart/form-data and asking input as binary image data""","If I send a request with contentType = application/json and image URL in JSON request body things work fine but on sending a binary image(base 64 encoded) with contentType = application/octet-stream it gives me ImageFormatInvalid in the respContent-Type: multipart/form-data and asking input as binary image data"""
1269,43660043,,0,,"[{'score': 0.506191, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.561417, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.506191,TRUE,0.561417,FALSE,0,TRUE,0.647986,FALSE,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""","""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working."
1270,43660043,,1,,"[{'score': 0.582483, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.574817, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.582483,FALSE,0,FALSE,0,TRUE,0.574817,FALSE,0,FALSE,0,FALSE,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""","The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:"""
1271,54672488,,0,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only.""","""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket."
1272,54672488,,1,,"[{'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,FALSE,0,TRUE,"""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only.""",This is the only source code I found but it uses an S3.
1273,54672488,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only.""","I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only."""
1274,49631726,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I getas I am trying to pass an image to. I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?Here is the trace for this:""","""I getas I am trying to pass an image to."
1275,49631726,,1,,"[{'score': 0.664529, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.548904, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.664529,FALSE,0,FALSE,0,TRUE,0.548904,FALSE,0,FALSE,0,FALSE,"""I getas I am trying to pass an image to. I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?Here is the trace for this:""","I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error."
1276,49631726,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I getas I am trying to pass an image to. I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?Here is the trace for this:""",It works for other images but throws an error for some of the images.
1277,49631726,,3,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.67368, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67368,FALSE,0,TRUE,0.716301,TRUE,"""I getas I am trying to pass an image to. I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?Here is the trace for this:""","What could be the reason for this?Here is the trace for this:"""
1278,54132185,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?""","""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature"
1279,54132185,,1,,"[{'score': 0.826956, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.826956,FALSE,0,FALSE,0,TRUE,"""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?""","= ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?"""
1280,53239821,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""","""I want to use Google Cloud Vision API on a family photo."
1281,53239821,,1,,"[{'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,TRUE,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""","I activated the API on my GCP account, received an API Key but I don't know where I should insert it."
1282,53239821,,2,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""","Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:"""
1283,55043291,,0,,"[{'score': 0.689543, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.689543,FALSE,0,TRUE,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""","""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values."
1284,55043291,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""","Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue."
1285,55043291,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""","(I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)"""
1286,52383178,,0,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""","""I am usingonto detect text values in hoarding boards that are usually found above a shop/store."
1287,52383178,,1,,"[{'score': 0.87867, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87867,FALSE,0,FALSE,0,TRUE,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",So far I have been able to detect individual words and their bounding polygons' coordinates.
1288,52383178,,2,,"[{'score': 0.76423, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.76423,FALSE,0,FALSE,0,TRUE,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""","Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned."
1289,52383178,,3,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""","Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them?"
1290,52383178,,4,,"[{'score': 0.906379, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.906379,FALSE,0,FALSE,0,TRUE,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""","Following is an example of an image what I have done so far:Vision API output excerpt:"""
1291,56351778,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.87867, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87867,FALSE,0,TRUE,0.681699,TRUE,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""","""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback."
1292,56351778,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""","When I try to use a fluture, looks like I am getting back a regurgitation of the request."
1293,56351778,,2,,"[{'score': 0.776763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.540203, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.776763,TRUE,0.540203,FALSE,0,TRUE,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""","Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv..."""
1294,45790003,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""What is the daily limit on the number of images that could be processed using Watson Visual Recognition. Free plan on the doc shows 250. Can we upload more on a Standard plan ??""","""What is the daily limit on the number of images that could be processed using Watson Visual Recognition."
1295,45790003,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""What is the daily limit on the number of images that could be processed using Watson Visual Recognition. Free plan on the doc shows 250. Can we upload more on a Standard plan ??""",Free plan on the doc shows 250.
1296,45790003,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""What is the daily limit on the number of images that could be processed using Watson Visual Recognition. Free plan on the doc shows 250. Can we upload more on a Standard plan ??""","Can we upload more on a Standard plan ??"""
1297,47744054,,0,,"[{'score': 0.838593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.838593,FALSE,0,TRUE,0.681699,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","""i'm trying to recognizefrom the card game."
1298,47744054,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","i've been trying to use a variety of image recognition APIs(google vision api, vize.ai,"
1299,47744054,,2,,"[{'score': 0.658902, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.711313, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.658902,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711313,FALSE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've"
1300,47744054,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've"
1301,47744054,,4,,"[{'score': 0.503455, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.787769, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.915488, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.503455,FALSE,0,FALSE,0,TRUE,0.787769,FALSE,0,TRUE,0.915488,FALSE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards."
1302,47744054,,5,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""",Does anyone know of a way i can teach one of these APIs(or another) to read these cards better?
1303,47744054,,6,,"[{'score': 0.663824, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.663824,FALSE,0,TRUE,0.511119,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""",or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank
1304,47744054,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""","you."""
1305,56122848,,0,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.960226, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.960226,TRUE,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""","""Not sure if this is more google-cloud-related or pytest-related."
1306,56122848,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""","See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt"
1307,56122848,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""",file and/or runandwith and withoutflags.
1308,56122848,,3,,"[{'score': 0.569118, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.524685, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.569118,TRUE,0.524685,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,FALSE,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""",What steps can I take to overcome this error?conftest.py:
1309,56122848,,4,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""","(empty)requirements.txt:app/my_script.py:test/test_my_script.py:"""
1310,55466156,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""","""I want to run OCR using the DetectText method of the Google Vision API."
1311,55466156,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running.
1312,55466156,,2,,"[{'score': 0.602833, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.602833,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,FALSE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",So I want to generate an error if there is no response within 2 seconds after calling the DetectText method.
1313,55466156,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""","(Default is 10 minutes, set to 600000 milisecond)."
1314,55466156,,4,,"[{'score': 0.525511, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.525511,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",Thank you for your help.
1315,55466156,,5,,"[{'score': 0.81894, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.81894,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""","In the sample source will be even more helpful.Thank you."""
1316,47570243,,0,,"[{'score': 0.913819, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.913819,TRUE,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""","""I have used Google vision API to read text from any object like newspaper or text in wall."
1317,47570243,,1,,"[{'score': 0.677069, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677069,FALSE,0,FALSE,0,TRUE,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""",I have tried same sample from Google developer website but my Text Recognizer always return false onfunction.
1318,47570243,,2,,"[{'score': 0.51184, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.51184,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""",am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this.
1319,47570243,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""","Thanks in Advance"""
1320,55170430,,0,,"[{'score': 0.722704, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.722704,FALSE,0,FALSE,0,TRUE,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""","""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API)."
1321,55170430,,1,,"[{'score': 0.85813, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85813,TRUE,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""",But the system is not secure!.
1322,55170430,,2,,"[{'score': 0.947344, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.947344,FALSE,0,FALSE,0,TRUE,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""",Because someone using a Photo of other people can validate to another person !.
1323,55170430,,3,,"[{'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.94715, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.94715,TRUE,0.92125,FALSE,0,TRUE,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""",The system is for validate a person!
1324,55170430,,4,,"[{'score': 0.785537, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.702543, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.702543,FALSE,0,TRUE,0.785537,TRUE,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""","If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?"""
1325,53790848,,0,,"[{'score': 0.908148, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.908148,FALSE,0,FALSE,0,TRUE,"""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?Thanks every one""","""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational()"
1326,53790848,,1,,"[{'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,TRUE,0.97759,FALSE,0,TRUE,"""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?Thanks every one""",method returns always false.
1327,53790848,,2,,"[{'score': 0.827997, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827997,FALSE,0,FALSE,0,TRUE,"""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?Thanks every one""",I think it is because google play services cannot download the required OCR file but it works in some devices.
1328,53790848,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?Thanks every one""","Is it possible to manually download and add the file?Thanks every one"""
1329,54685737,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""We are using Microsoft's Azure Face API for over 12 months now. But our low level API Android users (19) starting to get this error:javax.net.ssl.SSLException: hostname in certificate didn't match:  != <.cognitiveservices.azure.com> OR <.cognitiveservices.azure.com>It is working on level api 21+""","""We are using Microsoft's Azure Face API for over 12 months now."
1330,54685737,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are using Microsoft's Azure Face API for over 12 months now. But our low level API Android users (19) starting to get this error:javax.net.ssl.SSLException: hostname in certificate didn't match:  != <.cognitiveservices.azure.com> OR <.cognitiveservices.azure.com>It is working on level api 21+""",But our low level API Android users (19) starting to get this error:javax.net.ssl.SSLException: hostname in certificate didn't match:  != <.cognitiveservices.azure.com>
1331,54685737,,2,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""We are using Microsoft's Azure Face API for over 12 months now. But our low level API Android users (19) starting to get this error:javax.net.ssl.SSLException: hostname in certificate didn't match:  != <.cognitiveservices.azure.com> OR <.cognitiveservices.azure.com>It is working on level api 21+""","OR <.cognitiveservices.azure.com>It is working on level api 21+"""
1332,56175138,,0,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""Wrong image recognition on Azure Custom Vision Service.I have a doubt. I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks.""","""Wrong image recognition on Azure Custom Vision Service.I have a doubt."
1333,56175138,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Wrong image recognition on Azure Custom Vision Service.I have a doubt. I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks.""","I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks."""
1334,41998868,,0,,"[{'score': 0.669417, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.660937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.669417,FALSE,0,FALSE,0,TRUE,0.660937,FALSE,0,FALSE,0,FALSE,"""I am trying to compare faces using AWS Rekognitionthrough Python boto3, as instructed in the AWS documentation.My API call is:But everytime I run this program,I get the following error:I have specified the secret, key, region, and threshold properly. How can I clear off this error and make the request call work?""","""I am trying to compare faces using AWS Rekognitionthrough Python boto3, as instructed in the AWS documentation.My API call is:But everytime I run this program,I get the following error:I have specified the secret, key, region, and threshold properly."
1335,41998868,,1,,"[{'score': 0.526718, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.775702, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.526718,FALSE,0,FALSE,0,TRUE,0.589295,TRUE,0.775702,FALSE,0,FALSE,"""I am trying to compare faces using AWS Rekognitionthrough Python boto3, as instructed in the AWS documentation.My API call is:But everytime I run this program,I get the following error:I have specified the secret, key, region, and threshold properly. How can I clear off this error and make the request call work?""","How can I clear off this error and make the request call work?"""
1336,42222036,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm currently using node.js to create a post-upload API, to upload an image, which is processed by the Watson Visual Recognition Service. This returns a JSON, which is currently logged to the console.Is there a way to send this JSON back to the user, after the process is done?I'm a total newbie to Node.js, so I really appreciate your help.This is my code:""","""I'm currently using node.js to create a post-upload API, to upload an image, which is processed by the Watson Visual Recognition Service."
1337,42222036,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm currently using node.js to create a post-upload API, to upload an image, which is processed by the Watson Visual Recognition Service. This returns a JSON, which is currently logged to the console.Is there a way to send this JSON back to the user, after the process is done?I'm a total newbie to Node.js, so I really appreciate your help.This is my code:""","This returns a JSON, which is currently logged to the console.Is there a way to send this JSON back to the user, after the process is done?I'm a total newbie to Node.js, so I really appreciate your help.This is my code:"""
1338,53798619,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string."
1339,53798619,,1,,"[{'score': 0.712335, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.546148, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.712335,FALSE,0,FALSE,0,TRUE,0.546148,FALSE,0,FALSE,0,FALSE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."""
1340,53798619,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works."
1341,53798619,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""",Here are the relevant sections of my camera.js
1342,53798619,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."""
1343,53798619,,5,,"[{'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with."
1344,53798619,,6,,"[{'score': 0.575475, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.575475,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,FALSE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""",Does anyone know where I'm going wrong?
1345,53798619,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""","Do I need to switch in a different camera/webcam node package?"""
1346,51186135,,0,,"[{'score': 0.840583, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840583,FALSE,0,FALSE,0,TRUE,"""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it. My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name.""","""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it."
1347,51186135,,1,,"[{'score': 0.536757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.536757,FALSE,0,FALSE,0,TRUE,"""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it. My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name.""","My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person."
1348,51186135,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it. My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name.""","(eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name."""
1349,50808403,,0,,"[{'score': 0.818876, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.818876,FALSE,0,FALSE,0,TRUE,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""","""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits."
1350,50808403,,1,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,TRUE,0.91961,TRUE,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""",I can't analyze anything from the web.
1351,50808403,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""","Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error."
1352,50808403,,3,,"[{'score': 0.72016, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.72016,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""",Where am I going wrong?
1353,50808403,,4,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,TRUE,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""","Is it a problem with the service or the URL? TIA"""
1354,54722619,,0,,"[{'score': 0.590455, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.590455,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,FALSE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""","""I am trying to implementusing Angular."
1355,54722619,,1,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,FALSE,0,TRUE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",I need to find a particular header from the response of the first Http call and then call the second one.
1356,54722619,,2,,"[{'score': 0.735502, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.735502,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",But I am unable to find the header.
1357,54722619,,3,,"[{'score': 0.534372, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.504088, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.534372,TRUE,0.504088,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,FALSE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",Can you please help me find what I am missing here?
1358,54722619,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""","You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later."
1359,54722619,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",It will return Accepted immediately and include an  Operation-Location  header.
1360,54722619,,6,,"[{'score': 0.802757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802757,FALSE,0,FALSE,0,TRUE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",Client side should further query the operation status using the URL specified in this header.
1361,54722619,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""","The operation ID will expire in 48 hours."""""
1362,52003791,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml file""","""Currently I am developing a bar code reader android app with Google vision API."
1363,52003791,,1,,"[{'score': 0.635132, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.635132,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml file""",I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen.
1364,52003791,,2,,"[{'score': 0.512098, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.773514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.512098,FALSE,0,FALSE,0,TRUE,0.773514,FALSE,0,FALSE,0,FALSE,"""Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml file""",When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml
1365,52003791,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml file""","file"""
1366,39161387,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""Using Google Visioni want to take photo to detect face, but photos are rotated. I had tried to rotate it using my legacy codebutalways return 0 so image keep wrong rotation""","""Using Google Visioni want to take photo to detect face, but photos are rotated."
1367,39161387,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using Google Visioni want to take photo to detect face, but photos are rotated. I had tried to rotate it using my legacy codebutalways return 0 so image keep wrong rotation""","I had tried to rotate it using my legacy codebutalways return 0 so image keep wrong rotation"""
1368,49741658,,0,,"[{'score': 0.61732, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61732,FALSE,0,FALSE,0,TRUE,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""","""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js"
1369,49741658,,1,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""","and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API."
1370,49741658,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""","When I post an image to the API, it will return an array of labels."
1371,49741658,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js
1372,49741658,,4,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.734478, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.734478,FALSE,0,TRUE,0.75152,TRUE,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""","helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?"""
1373,40843164,,0,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""","""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces."
1374,40843164,,1,,"[{'score': 0.96597, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.96597,FALSE,0,FALSE,0,TRUE,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""",So that System should identify the each person that we have trained.
1375,40843164,,2,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""","Individuals may come with different clothes, and other possible variations."
1376,40843164,,3,,"[{'score': 0.652411, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.652411,FALSE,0,FALSE,0,TRUE,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""","But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?"""
1377,53003814,,0,,"[{'score': 0.602437, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.712559, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.602437,FALSE,0,FALSE,0,TRUE,0.712559,FALSE,0,FALSE,0,FALSE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them."
1378,53003814,,1,,"[{'score': 0.73362, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73362,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",It's working great when I index a face from an Android device and then try to search it with an Android device.
1379,53003814,,2,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","But if I try to search it later on iOS app, it doesn't find it."
1380,53003814,,3,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",Same is the result if I go other way round.
1381,53003814,,4,,"[{'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices."
1382,53003814,,5,,"[{'score': 0.781264, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,TRUE,0.781264,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do."
1383,53003814,,6,,"[{'score': 0.635424, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.635424,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps."
1384,53003814,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",Both of them are showing different list of collections.
1385,53003814,,8,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",This is the issue.
1386,53003814,,9,,"[{'score': 0.511745, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.511745,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",But I can't figure our why it is happening.
1387,53003814,,10,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""","The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:"""
1388,42406824,,0,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I am using PHP library for using Google Cloud Vision. The docs tells about 2 ways of authentication - 1) API and 2) Service Account.How do I use API based auth with my VisionClient?There is no document on using it. Please let me know if I am wrong.I get the below error message when running the above code.""","""I am using PHP library for using Google Cloud Vision."
1389,42406824,,1,,"[{'score': 0.54839, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.54839,FALSE,0,FALSE,0,TRUE,"""I am using PHP library for using Google Cloud Vision. The docs tells about 2 ways of authentication - 1) API and 2) Service Account.How do I use API based auth with my VisionClient?There is no document on using it. Please let me know if I am wrong.I get the below error message when running the above code.""",The docs tells about 2 ways of authentication - 1) API and 2) Service Account.How do I use API based auth with my VisionClient?There is no document on using it.
1390,42406824,,2,,"[{'score': 0.795049, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.795049,FALSE,0,FALSE,0,TRUE,"""I am using PHP library for using Google Cloud Vision. The docs tells about 2 ways of authentication - 1) API and 2) Service Account.How do I use API based auth with my VisionClient?There is no document on using it. Please let me know if I am wrong.I get the below error message when running the above code.""","Please let me know if I am wrong.I get the below error message when running the above code."""
1391,44607269,,0,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""","""I'm trying to use Google Cloud Vision API, but facing some issues."
1392,44607269,,1,,"[{'score': 0.946087, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946087,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""",Let me explain the steps I took and then the issue I'm facing.
1393,44607269,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""","I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd"
1394,44607269,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""",into the folder where image existsThen run the following code:python detect_face.py
1395,44607269,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""","""image.jpg""But"
1396,44607269,,5,,"[{'score': 0.603652, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.603652,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,TRUE,0.681699,FALSE,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""","I'm getting this error:Can anyone please tell me why I'm getting this issue?"""
1397,50546373,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up.""","""I'm trying to use IBM Watson visual recognition in a web application."
1398,50546373,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up.""","I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up."""
1399,43130920,,0,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""","""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID."
1400,43130920,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""","My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync)."
1401,43130920,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""",But the function requires that I pass in a GUID?What GUID do I use here?
1402,43130920,,3,,"[{'score': 0.997314, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.970179, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.970179,FALSE,0,TRUE,0.997314,TRUE,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""",Do I just generate anything?
1403,43130920,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""","How is that GUID is going to be associated with the person's name?"""
1404,38860919,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm trying to use Google Vision service with NodeJs. However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?For example: the real text ""  renci"" but the service returns ""ogrenci""""","""I'm trying to use Google Vision service with NodeJs."
1405,38860919,,1,,"[{'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision service with NodeJs. However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?For example: the real text ""  renci"" but the service returns ""ogrenci""""","However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me."
1406,38860919,,2,,"[{'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision service with NodeJs. However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?For example: the real text ""  renci"" but the service returns ""ogrenci""""","How can I get the UTF-32 characters?For example: the real text ""  renci"" but the service returns ""ogrenci"""""
1407,53530035,,0,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,TRUE,0.825035,FALSE,0,TRUE,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""","""I made a project in order to get all features of an image."
1408,53530035,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""","It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please."
1409,53530035,,2,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""",Which library should I use and if its the second one.
1410,53530035,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""","What should I do?Thank you"""
1411,55450247,,0,,"[{'score': 0.542941, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.623325, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.542941,FALSE,0,FALSE,0,FALSE,0,TRUE,0.623325,FALSE,0,FALSE,0,FALSE,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""","""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is"
1412,55450247,,1,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,TRUE,0.822231,TRUE,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""","there a way to find this out or decrease the possible cases so that i can use it in my model."""
1413,53585778,,0,,"[{'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,TRUE,"""According to the, the maximum number of image files per request is 16., however, I'm finding that the maximum number of requests per minute is as high as 1800. Is there any way to submit that many requests in such a short period of time from a single machine? I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible).""","""According to the, the maximum number of image files per request is 16., however, I'm finding that the maximum number of requests per minute is as high as 1800."
1414,53585778,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""According to the, the maximum number of image files per request is 16., however, I'm finding that the maximum number of requests per minute is as high as 1800. Is there any way to submit that many requests in such a short period of time from a single machine? I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible).""",Is there any way to submit that many requests in such a short period of time from a single machine?
1415,53585778,,2,,"[{'score': 0.57569, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.774345, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.57569,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.774345,FALSE,"""According to the, the maximum number of image files per request is 16., however, I'm finding that the maximum number of requests per minute is as high as 1800. Is there any way to submit that many requests in such a short period of time from a single machine? I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible).""","I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible)."""
1416,39641111,,0,,"[{'score': 0.635197, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635197,FALSE,0,FALSE,0,TRUE,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""","""I am building an app that has a qr scanner using the google vision api."
1417,39641111,,1,,"[{'score': 0.621432, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.621432,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""",I am having trouble stopping the camera after the qr code is read.
1418,39641111,,2,,"[{'score': 0.703734, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.703734,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,FALSE,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""",the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain.
1419,39641111,,3,,"[{'score': 0.563837, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.563837,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,FALSE,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""",however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive?
1420,39641111,,4,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""","and where is the correct place to release the camera source?QrActivityQrReader Class"""
1421,43657393,,0,,"[{'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,FALSE,0,TRUE,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""","""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing."
1422,43657393,,1,,"[{'score': 0.838797, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.838797,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",But I am unable to figure out how should I can add the Gradle dependency to the .aar
1423,43657393,,2,,"[{'score': 0.767076, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767076,FALSE,0,FALSE,0,TRUE,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",File.I don't want to add Google dependency separately while using my .aar
1424,43657393,,3,,"[{'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,FALSE,0,TRUE,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",because my library project already contains the same.I have tried one solution by pushing the .aar
1425,43657393,,4,,"[{'score': 0.538454, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.897946, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.538454,FALSE,0,FALSE,0,TRUE,0.897946,FALSE,0,FALSE,0,FALSE,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""","file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks."""
1426,55897523,,0,,"[{'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,TRUE,0.822231,TRUE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""","""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos."
1427,55897523,,1,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.777256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.777256,TRUE,0.543112,FALSE,0,TRUE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""","These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky."
1428,55897523,,2,,"[{'score': 0.524978, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.524978,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""","The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos."
1429,55897523,,3,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",I now need to write a CSV file which leads to each image.
1430,55897523,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""","Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them."
1431,55897523,,5,,"[{'score': 0.616699, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.719994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.616699,FALSE,0,FALSE,0,TRUE,0.719994,FALSE,0,FALSE,0,FALSE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices.
1432,55897523,,6,,"[{'score': 0.500346, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.703942, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.500346,FALSE,0,FALSE,0,TRUE,0.703942,FALSE,0,FALSE,0,FALSE,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""","I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task."""
1433,54094156,,0,,"[{'score': 0.528596, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.528596,FALSE,0,FALSE,0,TRUE,"""I am using the Google vision api to extract the text from an image and I also want to store this text in a .txt file.Whenever I useI get:Withit gives me:gives me:""","""I am using the Google vision api to extract the text from an image and I also want to store this text in a .txt"
1434,54094156,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the Google vision api to extract the text from an image and I also want to store this text in a .txt file.Whenever I useI get:Withit gives me:gives me:""","file.Whenever I useI get:Withit gives me:gives me:"""
1435,39378862,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""","""I have a script that is iterating through images of different forms."
1436,39378862,,1,,"[{'score': 0.799123, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.799123,FALSE,0,FALSE,0,TRUE,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""","When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate."
1437,39378862,,2,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,TRUE,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""","Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:."
1438,39378862,,3,,"[{'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,FALSE,0,TRUE,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""",I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before?
1439,39378862,,4,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.698634, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.698634,FALSE,0,TRUE,0.615352,TRUE,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""","Is there a fix other than attempting to re-submit the request if it reaches this error?"""
1440,55465835,,0,,"[{'score': 0.700145, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700145,FALSE,0,FALSE,0,TRUE,"""I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You""","""I want to implement the Google Cloud Vision with ImageAnnotator using a service key."
1441,55465835,,1,,"[{'score': 0.883038, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.883038,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You""","What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You"""
1442,56002802,,0,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?""","""I'm working on a project to extract information for receipt images."
1443,56002802,,1,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?""",I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt.
1444,56002802,,2,,"[{'score': 0.879891, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.879891,FALSE,0,FALSE,0,TRUE,"""I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?""","I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box."
1445,56002802,,3,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,TRUE,0.825035,FALSE,0,TRUE,"""I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?""","How do we extract the necessary information from the raw texts?"""
1446,55268232,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am followingarticle trying to make my Python script read labels related to an image using. The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script.""","""I am followingarticle trying to make my Python script read labels related to an image using."
1447,55268232,,1,,"[{'score': 0.631133, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.810144, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.631133,FALSE,0,FALSE,0,TRUE,0.810144,FALSE,0,FALSE,0,FALSE,"""I am followingarticle trying to make my Python script read labels related to an image using. The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script.""",The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud
1448,55268232,,2,,"[{'score': 0.608802, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.608802,FALSE,0,FALSE,0,TRUE,"""I am followingarticle trying to make my Python script read labels related to an image using. The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script.""","module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script."""
1449,46152610,,0,,"[{'score': 0.565693, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.565693,TRUE,"""I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.Does anyone have any experience in this? ThanksMy node-red flow is here""","""I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.Does anyone have any experience in this?"
1450,46152610,,1,,"[{'score': 0.664847, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.664847,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.Does anyone have any experience in this? ThanksMy node-red flow is here""","ThanksMy node-red flow is here"""
1451,51776654,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""","""Python 3.6.6,"
1452,51776654,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",Pillow 5.2.0The
1453,51776654,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""","Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be."
1454,51776654,,3,,"[{'score': 0.65529, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.516664, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.570071, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.65529,FALSE,0,FALSE,0,TRUE,0.516664,FALSE,0,TRUE,0.570071,FALSE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""","Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead."
1455,51776654,,4,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.
1456,51776654,,5,,"[{'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.594263,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.
1457,51776654,,6,,"[{'score': 0.631014, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.631014,FALSE,0,FALSE,0,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?
1458,51776654,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""","What is the best way to manage that size in bytes before I convert it?"""
1459,42421317,,0,,"[{'score': 0.633482, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.633482,FALSE,0,FALSE,0,TRUE,"""In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.In their documentation page we have a pricing tablein which we can see there is a free plan in which you have some limits. In order to start using this I needed to join the free trial and set a billing account.My questions are the following:When the limits are reached, am I going to be billed by Google? Or the service will be unavailable until I accept to be billed for that?As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?""","""In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.In their documentation page we have a pricing tablein which we can see there is a free plan in which you have some limits."
1460,42421317,,1,,"[{'score': 0.804277, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804277,FALSE,0,FALSE,0,TRUE,"""In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.In their documentation page we have a pricing tablein which we can see there is a free plan in which you have some limits. In order to start using this I needed to join the free trial and set a billing account.My questions are the following:When the limits are reached, am I going to be billed by Google? Or the service will be unavailable until I accept to be billed for that?As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?""","In order to start using this I needed to join the free trial and set a billing account.My questions are the following:When the limits are reached, am I going to be billed by Google?"
1461,42421317,,2,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.In their documentation page we have a pricing tablein which we can see there is a free plan in which you have some limits. In order to start using this I needed to join the free trial and set a billing account.My questions are the following:When the limits are reached, am I going to be billed by Google? Or the service will be unavailable until I accept to be billed for that?As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?""","Or the service will be unavailable until I accept to be billed for that?As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?"""
1462,40668684,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text. Here is the code:This is the error I recieve:Does anyone know what the issue is here?Thanks in advance.""","""I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text."
1463,40668684,,1,,"[{'score': 0.746925, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.746925,FALSE,0,FALSE,0,TRUE,"""I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text. Here is the code:This is the error I recieve:Does anyone know what the issue is here?Thanks in advance.""","Here is the code:This is the error I recieve:Does anyone know what the issue is here?Thanks in advance."""
1464,56012355,,0,,"[{'score': 0.601231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.601231,TRUE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""","""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py"
1465,56012355,,1,,"[{'score': 0.600634, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.600634,TRUE,0.982476,FALSE,0,FALSE,0,FALSE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""","URL argument""."
1466,56012355,,2,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""","How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api?"
1467,56012355,,3,,"[{'score': 0.579133, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.579133,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,FALSE,0,FALSE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is.
1468,56012355,,4,,"[{'score': 0.607379, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.913819, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.607379,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,TRUE,0.913819,FALSE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""","Any time I try to run the could without a URL argument it throws this error my way."""
1469,56012355,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",usage: visiontest.py
1470,56012355,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",[-h] image_url visiontest.py:
1471,56012355,,7,,"[{'score': 0.702538, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.965509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.702538,FALSE,0,FALSE,0,TRUE,0.965509,FALSE,0,FALSE,0,FALSE,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""","error: too few arguments"""""
1472,48999636,,0,,"[{'score': 0.957828, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.957828,TRUE,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""","""I just started playing around with Google Cloud Vision a bit."
1473,48999636,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",I wanted to detect text in an image.
1474,48999636,,2,,"[{'score': 0.50939, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.50939,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""","Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything)."
1475,48999636,,3,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""","The same happens, if I use other methods (e.g.or)."
1476,48999636,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""","When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!"""
1477,56050457,,0,,"[{'score': 0.839659, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839659,FALSE,0,FALSE,0,TRUE,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""","""Im following this tutorial to set up the google vision ocr:."
1478,56050457,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""",In the tutorial it says that translated text from images is saved in your google cloud storage.
1479,56050457,,2,,"[{'score': 0.504083, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.504083,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""",Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME.
1480,56050457,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""","It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation."""
1481,35785224,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video."
1482,35785224,,1,,"[{'score': 0.500565, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.500565,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,FALSE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e."
1483,35785224,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I
1484,35785224,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",am running a Samsung Galaxy S4 Mini on KitKat 4.4.3
1485,35785224,,4,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then."
1486,35785224,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",That makes grabbing and checking every frame seem like the only viable solution.)So
1487,35785224,,6,,"[{'score': 0.563284, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.868027, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.563284,FALSE,0,FALSE,0,TRUE,0.868027,TRUE,0.618451,FALSE,0,FALSE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","to at least prove the concept, I wanted to simply modify the Google Vision Demo."
1488,35785224,,7,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,TRUE,0.525007,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",(Found)It seems the easiest thing to do is simply jump in the code and add a media recorder.
1489,35785224,,8,,"[{'score': 0.629655, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.629655,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code."
1490,35785224,,9,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I"
1491,35785224,,10,,"[{'score': 0.606211, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.782808, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.606211,FALSE,0,FALSE,0,TRUE,0.782808,FALSE,0,FALSE,0,FALSE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up."
1492,35785224,,11,,"[{'score': 0.685138, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.685138,FALSE,0,FALSE,0,TRUE,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""","The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly."""
1493,42146912,,0,,"[{'score': 0.611918, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.611918,FALSE,0,FALSE,0,TRUE,"""I am having an issue getting my image to upload to the microsoft face api.I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.It posts to the server, and returns the following in the command line:What do I need to manipulate so that it works with the base64 encoding? It was posting with an image url off the internet prior to the modifications.""","""I am having an issue getting my image to upload to the microsoft face api.I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.It posts to the server, and returns the following in the command line:What do I need to manipulate so that it works with the base64 encoding?"
1494,42146912,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having an issue getting my image to upload to the microsoft face api.I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.It posts to the server, and returns the following in the command line:What do I need to manipulate so that it works with the base64 encoding? It was posting with an image url off the internet prior to the modifications.""","It was posting with an image url off the internet prior to the modifications."""
1495,44446544,,0,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""","""I'm trying to use SVG path element to define an area with ""holes""."
1496,44446544,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR ().
1497,44446544,,2,,"[{'score': 0.634637, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.634637,FALSE,0,FALSE,0,TRUE,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine.
1498,44446544,,3,,"[{'score': 0.543194, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.543194,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,FALSE,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",I have problem only with overlapping bounding boxes.
1499,44446544,,4,,"[{'score': 0.546407, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.881306, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.546407,FALSE,0,FALSE,0,FALSE,0,TRUE,0.881306,FALSE,0,FALSE,0,FALSE,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""","I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want."""
1500,38620455,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""","""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials."
1501,38620455,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""","I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file."""
1502,49801592,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to convert andto java. In order to do this I need the byte array from the AWS Image. However, when I call themethod it returns null instead of returning. My code is as below:The input image is only 80KB in size, not sure if size matters.""","""I am trying to convert andto java."
1503,49801592,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am trying to convert andto java. In order to do this I need the byte array from the AWS Image. However, when I call themethod it returns null instead of returning. My code is as below:The input image is only 80KB in size, not sure if size matters.""",In order to do this I need the byte array from the AWS Image.
1504,49801592,,2,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""I am trying to convert andto java. In order to do this I need the byte array from the AWS Image. However, when I call themethod it returns null instead of returning. My code is as below:The input image is only 80KB in size, not sure if size matters.""","However, when I call themethod it returns null instead of returning."
1505,49801592,,3,,"[{'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,FALSE,0,TRUE,"""I am trying to convert andto java. In order to do this I need the byte array from the AWS Image. However, when I call themethod it returns null instead of returning. My code is as below:The input image is only 80KB in size, not sure if size matters.""","My code is as below:The input image is only 80KB in size, not sure if size matters."""
1506,52448751,,0,,"[{'score': 0.600082, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.600082,FALSE,0,FALSE,0,TRUE,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""","""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two."
1507,52448751,,1,,"[{'score': 0.827997, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827997,FALSE,0,FALSE,0,TRUE,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""","Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap."
1508,52448751,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""","But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on?"
1509,52448751,,3,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""","I just don't know where to put my Google Vision codes.Help me, please."""
1510,52446033,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.730335, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.730335,FALSE,0,TRUE,0.525007,TRUE,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""","""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well."
1511,52446033,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""",Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions.
1512,52446033,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""","For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo."
1513,52446033,,3,,"[{'score': 0.606284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.606284,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""","For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?"""
1514,47415374,,0,,"[{'score': 0.832957, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832957,FALSE,0,FALSE,0,TRUE,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""","""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API."
1515,47415374,,1,,"[{'score': 0.654158, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.739655, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.654158,FALSE,0,FALSE,0,TRUE,0.739655,FALSE,0,TRUE,0.615352,FALSE,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""","The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?**"
1516,47415374,,2,,"[{'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,FALSE,0,TRUE,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""","Also using android play services, gives incorrect results - It would be my preference to use play services"""
1517,54292200,,0,,"[{'score': 0.549911, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.549911,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""","""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue."
1518,54292200,,1,,"[{'score': 0.782619, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.782619,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.615352,FALSE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",How does one catch an error like this?
1519,54292200,,2,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""","I have tried surrounding it with try-catch, I have tried appending a .catch()"
1520,54292200,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",call to the end of the function call.
1521,54292200,,4,,"[{'score': 0.709665, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.709665,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,FALSE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Neither work.
1522,54292200,,5,,"[{'score': 0.54185, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.932977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.54185,FALSE,0,FALSE,0,TRUE,0.932977,FALSE,0,TRUE,0.786991,FALSE,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""","Any advice?An example image that is causing the crash is:"""
1523,47109354,,0,,"[{'score': 0.613236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.613236,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.""","""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error."
1524,47109354,,1,,"[{'score': 0.63003, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.63003,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.""","I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end."
1525,47109354,,2,,"[{'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775166,TRUE,"""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.""","Have tried everything I could come up with but it keeps telling me the function doesn't exist."""
1526,54212819,,0,,"[{'score': 0.63633, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.63633,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,FALSE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","""On my website I'm making login system by user's face."
1527,54212819,,1,,"[{'score': 0.575112, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.575112,FALSE,0,FALSE,0,TRUE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId)."
1528,54212819,,2,,"[{'score': 0.542422, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.654189, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.542422,FALSE,0,FALSE,0,TRUE,0.654189,FALSE,0,FALSE,0,FALSE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem."
1529,54212819,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",He can't login.
1530,54212819,,4,,"[{'score': 0.552027, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.979246, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.552027,FALSE,0,FALSE,0,TRUE,0.979246,FALSE,0,FALSE,0,FALSE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos."
1531,54212819,,5,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",Where I'm using differentdevices with different cameras.
1532,54212819,,6,,"[{'score': 0.782808, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.677668, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.782808,TRUE,0.677668,FALSE,0,TRUE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him."
1533,54212819,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""","But from different devices - can't."""
1534,55714798,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""","""I am trying to work with Google Vision and Python."
1535,55714798,,1,,"[{'score': 0.70601, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.698904, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.659886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.70601,FALSE,0,FALSE,0,TRUE,0.659886,FALSE,0,TRUE,0.698904,FALSE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""",I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.
1536,55714798,,2,,"[{'score': 0.839221, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839221,FALSE,0,FALSE,0,TRUE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""",It does produce a JSON file but i guess it should print it out to the command line.
1537,55714798,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""",Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?
1538,55714798,,4,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""",I am using Python 3.7.2My
1539,55714798,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""","code is below:"""
1540,53952217,,0,,"[{'score': 0.768779, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.650854, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.768779,FALSE,0,FALSE,0,TRUE,0.650854,FALSE,0,FALSE,0,FALSE,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""","""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android?"
1541,53952217,,1,,"[{'score': 0.700246, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.700246,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,FALSE,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""","I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio."""
1542,31741189,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""","""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st"
1543,31741189,,1,,"[{'score': 0.565996, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.565996,FALSE,0,FALSE,0,TRUE,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""","I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g."
1544,31741189,,2,,"[{'score': 0.500843, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.798753, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.500843,FALSE,0,FALSE,0,TRUE,0.798753,FALSE,0,FALSE,0,FALSE,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""","""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)"""
1545,43746016,,0,,"[{'score': 0.635961, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635961,FALSE,0,FALSE,0,TRUE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""","""I am new to the Google Vision API and I would like to conduct a label detection of approx."
1546,43746016,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",10 images and I would like to run the vision quickstart.py
1547,43746016,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",file.
1548,43746016,,3,,"[{'score': 0.517707, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.517707,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,FALSE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",However when I do this with only 3 images then it is successful.
1549,43746016,,4,,"[{'score': 0.530623, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.530623,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",With more than 3 images I am getting the error message below.
1550,43746016,,5,,"[{'score': 0.875083, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.673299, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.673299,FALSE,0,TRUE,0.875083,TRUE,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""","I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi"""
1551,51116095,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""","""I am trying to call the function ""detect web"" from Google Cloud Vision API using python."
1552,51116095,,1,,"[{'score': 0.67368, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67368,FALSE,0,FALSE,0,TRUE,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""","However I am not able to call one of its method named ""best_guess_labels""."
1553,51116095,,2,,"[{'score': 0.779903, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.779903,FALSE,0,FALSE,0,TRUE,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""","When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file."
1554,51116095,,3,,"[{'score': 0.749445, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.687701, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.749445,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.687701,FALSE,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""","I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?"""
1555,54677464,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once""","""I have implemented a QR scanner(QRScanner class) using Google Vision API."
1556,54677464,,1,,"[{'score': 0.910117, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.910117,FALSE,0,FALSE,0,TRUE,"""I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once""",Once a value is detected it is passed to another activity(Info class) using Intents.
1557,54677464,,2,,"[{'score': 0.638512, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.666746, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.638512,FALSE,0,FALSE,0,TRUE,0.666746,FALSE,0,FALSE,0,FALSE,"""I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once""",The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times.
1558,54677464,,3,,"[{'score': 0.507628, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.507628,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,FALSE,0,FALSE,"""I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once""","I want the QRScanner to get only one value and Info class to get called only once"""
1559,51605428,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem? I try to change color and resize the image but it still not work.for example please look : [The API can detect only 'AMATA' but not 'S']""","""I have used Google Cloud Vision API for my small project to detect text from an image."
1560,51605428,,1,,"[{'score': 0.674387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.674387,FALSE,0,FALSE,0,TRUE,"""I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem? I try to change color and resize the image but it still not work.for example please look : [The API can detect only 'AMATA' but not 'S']""","The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem?"
1561,51605428,,2,,"[{'score': 0.538908, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.538908,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem? I try to change color and resize the image but it still not work.for example please look : [The API can detect only 'AMATA' but not 'S']""",I try to change color and resize the image but it still not work.for
1562,51605428,,3,,"[{'score': 0.907142, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.907142,FALSE,0,FALSE,0,TRUE,"""I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem? I try to change color and resize the image but it still not work.for example please look : [The API can detect only 'AMATA' but not 'S']""","example please look : [The API can detect only 'AMATA' but not 'S']"""
1563,37306516,,0,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.67368, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67368,FALSE,0,TRUE,0.716301,TRUE,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""","""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service."
1564,37306516,,1,,"[{'score': 0.666295, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.666295,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",You can see the JAVA examples are missing.
1565,37306516,,2,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,TRUE,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",Please help me to find few suitable examples or any reference to these methods.
1566,37306516,,3,,"[{'score': 0.598602, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,TRUE,0.598602,FALSE,0,TRUE,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition?
1567,37306516,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""","Thanks in Advance!"""
1568,48428894,,0,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint? The purpose is to find the best match between a specific photo and the existing photos in the repository.""","""Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint?"
1569,48428894,,1,,"[{'score': 0.721172, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.721172,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,FALSE,"""Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint? The purpose is to find the best match between a specific photo and the existing photos in the repository.""","The purpose is to find the best match between a specific photo and the existing photos in the repository."""
1570,38363182,,0,,"[{'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.694645, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.694645,FALSE,0,TRUE,0.775166,TRUE,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""","""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way?"
1571,38363182,,1,,"[{'score': 0.833824, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.802309, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802309,FALSE,0,TRUE,0.833824,TRUE,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""","- e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not."
1572,38363182,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""","Thanks!"""
1573,48400312,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""","""I am using google vision in my application to read barcodes and qr-codes."
1574,48400312,,1,,"[{'score': 0.604714, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.548904, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.604714,FALSE,0,FALSE,0,TRUE,0.548904,FALSE,0,FALSE,0,FALSE,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""","This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately."
1575,48400312,,2,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,TRUE,0.525007,TRUE,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""","Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:"""
1576,55994493,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.677069, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677069,FALSE,0,TRUE,0.58393,TRUE,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""","""I am creating an application using google vision api to extract handwritten texts from some images documents."
1577,55994493,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""","I already have a google cloud ready, with the json file saved on my local folder with the code."
1578,55994493,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""","I used google cloud storage to create a bucket, and the name of the bucket is passed to the code."
1579,55994493,,3,,"[{'score': 0.550178, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.519075, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.650694, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.550178,FALSE,0,TRUE,0.519075,TRUE,0.650694,FALSE,0,FALSE,0,FALSE,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""","I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean?"
1580,55994493,,4,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""","How can I solve it?"""
1581,46814207,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""","""I have scanned image and extracted text from image using Google Vision Api."
1582,46814207,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""",Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code.
1583,46814207,,2,,"[{'score': 0.571405, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.571405,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""",How to avoid unncessary value like Turiff EXLA 105.
1584,46814207,,3,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.615352,TRUE,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""",and address and not able to get name also.Can anyone help me to solve this.
1585,46814207,,4,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""","Thank you"""
1586,18142659,,0,,"[{'score': 0.811077, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.863333, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.811077,FALSE,0,TRUE,0.863333,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""","""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side)."
1587,18142659,,1,,"[{'score': 0.76423, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.709321, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.76423,FALSE,0,TRUE,0.709321,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""","I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g."
1588,18142659,,2,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",bounding box around the face) and recognise it (e.g.
1589,18142659,,3,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",telling me with some probability who is the person in the picture).
1590,18142659,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""","For what concerns the database of faces, I will take care of it.The"
1591,18142659,,5,,"[{'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",only resource I know isand looks promising.
1592,18142659,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""","I would like to have more options though.Thanks a bunch!"""
1593,43926563,,0,,"[{'score': 0.679542, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.679542,FALSE,0,FALSE,0,TRUE,"""I am using the following code:based on:Now I need to base64 encode it for posting to Google Cloud Vision API. How to do that?Or does the Google Cloud vision API work with image urls?""","""I am using the following code:based on:Now I need to base64 encode it for posting to Google Cloud Vision API."
1594,43926563,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I am using the following code:based on:Now I need to base64 encode it for posting to Google Cloud Vision API. How to do that?Or does the Google Cloud vision API work with image urls?""","How to do that?Or does the Google Cloud vision API work with image urls?"""
1595,54904596,,0,,"[{'score': 0.693959, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.668095, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.693959,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.668095,FALSE,"""At the moment I am trying to use Image Recognition using Google Cloud Vision API in R. It works until the authorization, but this is the error I got:Does someone knows what possibly goes wrong? In the cloud console I activated Google Vision API.""","""At the moment I am trying to use Image Recognition using Google Cloud Vision API in R. It works until the authorization, but this is the error I got:Does someone knows what possibly goes wrong?"
1596,54904596,,1,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""At the moment I am trying to use Image Recognition using Google Cloud Vision API in R. It works until the authorization, but this is the error I got:Does someone knows what possibly goes wrong? In the cloud console I activated Google Vision API.""","In the cloud console I activated Google Vision API."""
1597,48384087,,0,,"[{'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","""I am working on an Android application with an embedded QR code scanner using the Google Vision API."
1598,48384087,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically."
1599,48384087,,2,,"[{'score': 0.554648, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.554648,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI"
1600,48384087,,3,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","have set asand, respectively."
1601,48384087,,4,,"[{'score': 0.899515, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.899515,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion."
1602,48384087,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""",For one particular emulated Android device I can statically set the right width and height.
1603,48384087,,6,,"[{'score': 0.719583, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719583,FALSE,0,FALSE,0,TRUE,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""","For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version."""
1604,49340214,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""","""I am designing an app where i scan the text using the camera and use that text to fetch more details."
1605,49340214,,1,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""",To do that i am using Google's vision API.
1606,49340214,,2,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.700747, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700747,TRUE,0.618451,FALSE,0,TRUE,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""",But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera.
1607,49340214,,3,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""","But i would like to just scan""Hello World""from the camera."
1608,49340214,,4,,"[{'score': 0.6065, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.786514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.6065,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786514,FALSE,0,TRUE,0.946222,FALSE,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""","Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition"""
1609,49463736,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""","""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image."
1610,49463736,,1,,"[{'score': 0.709665, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.709665,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""",But it is not working.i
1611,49463736,,2,,"[{'score': 0.740506, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.879318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.740506,FALSE,0,FALSE,0,FALSE,0,TRUE,0.879318,FALSE,0,FALSE,0,FALSE,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""","think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set."""
1612,41863595,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I have some code that calls into AWS's Rekognition service. Sometimes it throws this exception:I can't find theanywhere in the documentation or code, though, so I can't write a specific handler for when that occurs. Does anyone know what library module that exception lives in?""","""I have some code that calls into AWS's Rekognition service."
1613,41863595,,1,,"[{'score': 0.686267, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.686267,TRUE,"""I have some code that calls into AWS's Rekognition service. Sometimes it throws this exception:I can't find theanywhere in the documentation or code, though, so I can't write a specific handler for when that occurs. Does anyone know what library module that exception lives in?""","Sometimes it throws this exception:I can't find theanywhere in the documentation or code, though, so I can't write a specific handler for when that occurs."
1614,41863595,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.822231,TRUE,"""I have some code that calls into AWS's Rekognition service. Sometimes it throws this exception:I can't find theanywhere in the documentation or code, though, so I can't write a specific handler for when that occurs. Does anyone know what library module that exception lives in?""","Does anyone know what library module that exception lives in?"""
1615,51580768,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""","""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries."
1616,51580768,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""",Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js
1617,51580768,,2,,"[{'score': 0.774186, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.774186,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""",code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.
1618,51580768,,3,,"[{'score': 0.674208, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.674208,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""","Have I missed a step in Authentication?"""
1619,55868948,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""","""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image."
1620,55868948,,1,,"[{'score': 0.607256, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.607256,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""","It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work."
1621,55868948,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""",How can i get this to work?
1622,55868948,,3,,"[{'score': 0.819711, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.819711,FALSE,0,FALSE,0,TRUE,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""","I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system?"
1623,55868948,,4,,"[{'score': 0.576338, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.576338,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""","I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :"""
1624,51008892,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work.""","""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String)."
1625,51008892,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work.""",It works perfekt for one image.
1626,51008892,,2,,"[{'score': 0.56185, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.56185,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work.""","But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work."""
1627,54315250,,0,,"[{'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,TRUE,0.75152,TRUE,"""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--""","""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF."
1628,54315250,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--""","The API is added to my project, the account service is created and has granted access to the relatories storage interval."
1629,54315250,,2,,"[{'score': 0.651326, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.651326,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--""","When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication?"
1630,54315250,,3,,"[{'score': 0.809841, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.809841,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--""","I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--"""
1631,51443537,,0,,"[{'score': 0.747419, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747419,FALSE,0,FALSE,0,TRUE,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""","""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision."
1632,51443537,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""",What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory.
1633,51443537,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""","But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime."
1634,51443537,,3,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""","Please refer."""
1635,44304400,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.My codeThe response""","""I tried hitting the api with a sample image and got a response of an array with four coordinates."
1636,44304400,,1,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.My codeThe response""",what does this coordinates signify? is the aspect ratio fixed?
1637,44304400,,2,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.My codeThe response""",can i specify a specific aspect ratio? .
1638,44304400,,3,,"[{'score': 0.857227, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.857227,TRUE,"""I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.My codeThe response""","The documentation is not clear or i am not able to understand.My codeThe response"""
1639,54546886,,0,,"[{'score': 0.898858, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.898858,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""","""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i"
1640,54546886,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""","wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the"
1641,54546886,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",sample code is as follows:import iofrom google.cloud
1642,54546886,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with
1643,54546886,,4,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""","io.open(file_name,'rb') as image_file:    content = image_file.read()image"
1644,54546886,,5,,"[{'score': 0.775384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775384,FALSE,0,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",= vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for
1645,54546886,,6,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.543112,FALSE,0,TRUE,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""","label in labels:print(label.description)"""
1646,51223852,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in.""","""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words."
1647,51223852,,1,,"[{'score': 0.722126, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.722126,FALSE,0,FALSE,0,TRUE,"""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in.""","So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in."""
1648,41254591,,0,,"[{'score': 0.592217, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.787769, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.592217,FALSE,0,FALSE,0,TRUE,0.787769,FALSE,0,TRUE,0.594263,FALSE,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""","""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""?"
1649,41254591,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""","Also, how do I read this syntax with the colon "":""?"
1650,41254591,,2,,"[{'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839577,TRUE,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""","Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question)."
1651,41254591,,3,,"[{'score': 0.876752, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.73362, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73362,FALSE,0,TRUE,0.876752,TRUE,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""","If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better?"
1652,41254591,,4,,"[{'score': 0.513505, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.513505,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.984352,FALSE,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""","Any help is appreciated!"""
1653,43584965,,0,,"[{'score': 0.623489, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.623489,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Whenever I request to GoogleVision api's, this error pops up. Even cannot install/uninstall any of the packageSample Output:[sudo] password for engineer: Traceback (most recent call last):ImportError: cannot import name JSONClient""","""Whenever I request to GoogleVision api's, this error pops up."
1654,43584965,,1,,"[{'score': 0.512582, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.849846, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.512582,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849846,FALSE,"""Whenever I request to GoogleVision api's, this error pops up. Even cannot install/uninstall any of the packageSample Output:[sudo] password for engineer: Traceback (most recent call last):ImportError: cannot import name JSONClient""","Even cannot install/uninstall any of the packageSample Output:[sudo] password for engineer: Traceback (most recent call last):ImportError: cannot import name JSONClient"""
1655,39009764,,0,,"[{'score': 0.827172, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.827172,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to use the Microsoft Emotion API.I can use the image version without any issues but when I try to use the video version I get an empty response.It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.Here is my code. Would appreciate any help!""","""I'm trying to use the Microsoft Emotion API.I can use the image version without any issues but when I try to use the video version I get an empty response.It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.Here is my code."
1656,39009764,,1,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.984352,TRUE,"""I'm trying to use the Microsoft Emotion API.I can use the image version without any issues but when I try to use the video version I get an empty response.It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.Here is my code. Would appreciate any help!""","Would appreciate any help!"""
1657,40503587,,0,,"[{'score': 0.767076, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767076,FALSE,0,FALSE,0,TRUE,"""I wish to use Google Cloud Vision API to generate features from images, that I will further use to train my SVM for emotion recognition problem. Please provide a detailed procedure for how to write a script in python that can use Google Cloud Vision API to generate features that I can directly feed into SVM.""","""I wish to use Google Cloud Vision API to generate features from images, that I will further use to train my SVM for emotion recognition problem."
1658,40503587,,1,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I wish to use Google Cloud Vision API to generate features from images, that I will further use to train my SVM for emotion recognition problem. Please provide a detailed procedure for how to write a script in python that can use Google Cloud Vision API to generate features that I can directly feed into SVM.""","Please provide a detailed procedure for how to write a script in python that can use Google Cloud Vision API to generate features that I can directly feed into SVM."""
1659,54898476,,0,,"[{'score': 0.502899, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.78686, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.502899,FALSE,0,FALSE,0,TRUE,0.78686,FALSE,0,FALSE,0,FALSE,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""","""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451)"
1660,54898476,,1,,"[{'score': 0.695433, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.695433,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""",I have set frames rate but not working2) I have used Dispatch but not working.>
1661,54898476,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""","Please share your code to be appreciated Thanks."""
1662,54863800,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have this code for Google Vision API. I have Google credentials as a path and also as enviromental variable, butreturn nothing and error appears atHere is code:And here is console with emptyand error:""","""I have this code for Google Vision API."
1663,54863800,,1,,"[{'score': 0.517137, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.517137,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""I have this code for Google Vision API. I have Google credentials as a path and also as enviromental variable, butreturn nothing and error appears atHere is code:And here is console with emptyand error:""","I have Google credentials as a path and also as enviromental variable, butreturn nothing and error appears atHere is code:And here is console with emptyand error:"""
1664,48456300,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""","""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd."
1665,48456300,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection.
1666,48456300,,2,,"[{'score': 0.798791, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.784247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784247,FALSE,0,TRUE,0.798791,TRUE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object().
1667,48456300,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",I have provided the code below.
1668,48456300,,4,,"[{'score': 0.738937, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.793846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.738937,FALSE,0,FALSE,0,FALSE,0,TRUE,0.793846,FALSE,0,TRUE,0.976993,FALSE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",Does anyone have a solution or a better method?
1669,48456300,,5,,"[{'score': 0.84155, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.84155,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""","Thank you for your time!"""
1670,54816799,,0,,"[{'score': 0.904038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904038,FALSE,0,FALSE,0,TRUE,"""I am usingto link Rekognition results to a DynamoDB table. It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}""","""I am usingto link Rekognition results to a DynamoDB table."
1671,54816799,,1,,"[{'score': 0.641294, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.641294,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am usingto link Rekognition results to a DynamoDB table. It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}""","It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}"""
1672,44195115,,0,,"[{'score': 0.587391, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.587391,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have one project that integrates with Google vision APIs.I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all. For my case I need to proof that every photos uploaded must not contain any glasses.This image, it seems that the Google Vision API can not detect wearing glasses at all.""","""I have one project that integrates with Google vision APIs.I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all."
1673,44195115,,1,,"[{'score': 0.5188, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.5188,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have one project that integrates with Google vision APIs.I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all. For my case I need to proof that every photos uploaded must not contain any glasses.This image, it seems that the Google Vision API can not detect wearing glasses at all.""","For my case I need to proof that every photos uploaded must not contain any glasses.This image, it seems that the Google Vision API can not detect wearing glasses at all."""
1674,48514721,,0,,"[{'score': 0.540225, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.540225,FALSE,0,FALSE,0,TRUE,"""Edited:I installedGoogle Cloud Speechby using the followning command:when I runPython Programto convertAudiointoTextusingPython3, I get an error:But If I run the same program usingPython(Python2),I get the correct output.I want to run the program usingPython3. I am usingOpenSuse Tumblebeed.So, I tried the following commands to installgoogle-cloud:But I am getting the following error by all these commands:I thinkGCCis trying to findpython.hso the process is exiting with error code 1.""","""Edited:I installedGoogle Cloud Speechby using the followning command:when I runPython Programto convertAudiointoTextusingPython3, I get an error:But If I run the same program usingPython(Python2),I get the correct output.I want to run the program usingPython3."
1675,48514721,,1,,"[{'score': 0.595827, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.595827,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Edited:I installedGoogle Cloud Speechby using the followning command:when I runPython Programto convertAudiointoTextusingPython3, I get an error:But If I run the same program usingPython(Python2),I get the correct output.I want to run the program usingPython3. I am usingOpenSuse Tumblebeed.So, I tried the following commands to installgoogle-cloud:But I am getting the following error by all these commands:I thinkGCCis trying to findpython.hso the process is exiting with error code 1.""","I am usingOpenSuse Tumblebeed.So, I tried the following commands to installgoogle-cloud:But I am getting the following error by all these commands:I thinkGCCis trying to findpython.hso the process is exiting with error code 1."""
1676,49711924,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use the google vision api in a java project in eclipse. I've looked for tutorials and looked over the steps that google provide but I am still lost. Can someone explain the steps i need to take in order to import the api?""","""I am trying to use the google vision api in a java project in eclipse."
1677,49711924,,1,,"[{'score': 0.624855, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.624855,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use the google vision api in a java project in eclipse. I've looked for tutorials and looked over the steps that google provide but I am still lost. Can someone explain the steps i need to take in order to import the api?""",I've looked for tutorials and looked over the steps that google provide but I am still lost.
1678,49711924,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to use the google vision api in a java project in eclipse. I've looked for tutorials and looked over the steps that google provide but I am still lost. Can someone explain the steps i need to take in order to import the api?""","Can someone explain the steps i need to take in order to import the api?"""
1679,44994201,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the"
1680,44994201,,1,,"[{'score': 0.667349, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.667349,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the"
1681,44994201,,2,,"[{'score': 0.852143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.852143,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials?
1682,44994201,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",":""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When"
1683,44994201,,4,,"[{'score': 0.730517, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.730517,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727988,FALSE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3
1684,44994201,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",1.0.0'.
1685,44994201,,6,,"[{'score': 0.511454, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.695447, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.511454,FALSE,0,TRUE,0.695447,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework."
1686,44994201,,7,,"[{'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+"
1687,44994201,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+"
1688,44994201,,9,,"[{'score': 0.802309, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802309,FALSE,0,FALSE,0,TRUE,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here."""
1689,52818392,,0,,"[{'score': 0.784773, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784773,FALSE,0,FALSE,0,TRUE,"""I'm using the Python SDK snippet provided byI want to return face attributes, The docssuggest that addingTo the Base URl will return age and gender attributes. It's throwing me an error, am I missing something?This is my first time using Azure Face API.""","""I'm using the Python SDK snippet provided byI want to return face attributes, The docssuggest that addingTo the Base URl will return age and gender attributes."
1690,52818392,,1,,"[{'score': 0.744829, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.744829,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.5538,FALSE,"""I'm using the Python SDK snippet provided byI want to return face attributes, The docssuggest that addingTo the Base URl will return age and gender attributes. It's throwing me an error, am I missing something?This is my first time using Azure Face API.""","It's throwing me an error, am I missing something?This is my first time using Azure Face API."""
1691,54120224,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""","""I installed AWS SDK along with Facebook and Google SDKs."
1692,54120224,,1,,"[{'score': 0.631785, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.775702, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.631785,FALSE,0,FALSE,0,TRUE,0.589295,TRUE,0.775702,FALSE,0,FALSE,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""",All of them are working with no problem on my local MacOs environment.
1693,54120224,,2,,"[{'score': 0.789882, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.775702, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.789882,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775702,FALSE,0,FALSE,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""",But once I pushed to our server all AWS clients are not working.
1694,54120224,,3,,"[{'score': 0.797782, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.797782,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""","FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here."""
1695,44893985,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or soIf a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead. This makes it impossible for me to know whether there is a new person in front of the camera, or a different personThis was not the case a couple of weeks ago, it's just something which has started happening recentlyThe parameters that I'm sending are...... the gender and age detection are working fine, it's just the faceId that I'm having problems withIs there a limit to how many faceIds it'll assign per month or something? I can't find any reference to a limit in the documentation""","""I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or soIf a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead."
1696,44893985,,1,,"[{'score': 0.701265, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.701265,TRUE,"""I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or soIf a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead. This makes it impossible for me to know whether there is a new person in front of the camera, or a different personThis was not the case a couple of weeks ago, it's just something which has started happening recentlyThe parameters that I'm sending are...... the gender and age detection are working fine, it's just the faceId that I'm having problems withIs there a limit to how many faceIds it'll assign per month or something? I can't find any reference to a limit in the documentation""","This makes it impossible for me to know whether there is a new person in front of the camera, or a different personThis was not the case a couple of weeks ago, it's just something which has started happening recentlyThe parameters that I'm sending are...... the gender and age detection are working fine, it's just the faceId that I'm having problems withIs there a limit to how many faceIds it'll assign per month or something?"
1697,44893985,,2,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,TRUE,0.786991,TRUE,"""I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or soIf a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead. This makes it impossible for me to know whether there is a new person in front of the camera, or a different personThis was not the case a couple of weeks ago, it's just something which has started happening recentlyThe parameters that I'm sending are...... the gender and age detection are working fine, it's just the faceId that I'm having problems withIs there a limit to how many faceIds it'll assign per month or something? I can't find any reference to a limit in the documentation""","I can't find any reference to a limit in the documentation"""
1698,56160026,,0,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I want to know the font size of the text used in an image. Do you have that way?I used Google vision, but it only return""","""I want to know the font size of the text used in an image."
1699,56160026,,1,,"[{'score': 0.500545, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.500545,FALSE,0,FALSE,0,TRUE,"""I want to know the font size of the text used in an image. Do you have that way?I used Google vision, but it only return""","Do you have that way?I used Google vision, but it only return"""
1700,53737023,,0,,"[{'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,FALSE,0,TRUE,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""","""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date."
1701,53737023,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""",Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3.
1702,53737023,,2,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,TRUE,0.825035,FALSE,0,TRUE,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""",The google vision link shows my code exactly.My requirements.txt
1703,53737023,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""","(the relevant parts):"""
1704,45917756,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""","""I would like to use the Google Vision API for label detection."
1705,45917756,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",For this I am using a .NET library.
1706,45917756,,2,,"[{'score': 0.898327, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.898327,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",This is my code:It works very well.
1707,45917756,,3,,"[{'score': 0.961633, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961633,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",It displays all the labels.
1708,45917756,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",But on the Googleit also displays the percentages of the labels.
1709,45917756,,5,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""","See the image for an example.How can I achieve this by using the .NET library?"""
1710,43639575,,0,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""","""i'm using Microsoft Emotion Api and it return a result as a json."
1711,43639575,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",so i want to access emotion values and assign that values to php variables.
1712,43639575,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",i used json_decode function but it can't do it.
1713,43639575,,3,,"[{'score': 0.597809, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.597809,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.946222,FALSE,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""","result like below"""
1714,49386572,,0,,"[{'score': 0.589122, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.624557, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.734369, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.589122,FALSE,0,FALSE,0,TRUE,0.734369,FALSE,0,TRUE,0.624557,FALSE,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""","""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise."
1715,49386572,,1,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""",Is there a way to improve Vision response in these situations?
1716,49386572,,2,,"[{'score': 0.717281, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.789111, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.632, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.717281,FALSE,0,FALSE,0,TRUE,0.632,FALSE,0,TRUE,0.789111,FALSE,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""",(for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr.
1717,49386572,,3,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,TRUE,0.716301,TRUE,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""","Is there a way to force the recognition of digits or latin characters?"""
1718,51907473,,0,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""","""I am building a face recogition app using AWS Rekognition."
1719,51907473,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""",I also want to show the image to user that matches with my input image.
1720,51907473,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""",Does AWS also saves the image when we index them?
1721,51907473,,3,,"[{'score': 0.765977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765977,FALSE,0,FALSE,0,TRUE,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""","If yes, how can I get that image?If no I am thinking to do it this way."
1722,51907473,,4,,"[{'score': 0.72739, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.72739,FALSE,0,FALSE,0,TRUE,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""","I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:"""
1723,51192145,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""","""I am running into an issue with AWS Rekognition python API."
1724,51192145,,1,,"[{'score': 0.709868, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.780088, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.709868,FALSE,0,FALSE,0,TRUE,0.780088,FALSE,0,FALSE,0,FALSE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with.
1725,51192145,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",What I'm trying to do is to filter out only the information that I want to use in the next parts of my program.
1726,51192145,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment.
1727,51192145,,4,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""","However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting."
1728,51192145,,5,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""","According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after."
1729,51192145,,6,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""","However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again."
1730,51192145,,7,,"[{'score': 0.724786, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.799476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.724786,FALSE,0,FALSE,0,TRUE,0.799476,FALSE,0,FALSE,0,FALSE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked.
1731,51192145,,8,,"[{'score': 0.675827, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.925458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.675827,FALSE,0,FALSE,0,FALSE,0,TRUE,0.925458,FALSE,0,FALSE,0,FALSE,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""","If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this."""
1732,55886635,,0,,"[{'score': 0.841001, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841001,FALSE,0,FALSE,0,TRUE,"""I'm using the Google Cloud Vision API with Python 3, but i'm getting the error""Cannot find reference 'Image' in types.py"" when i use:I made the correct imports and the documentation tells me to use this function to get an image. Anyone can help me?Code:Error Message:Google Cloud Vision API version: 0.36.0""","""I'm using the Google Cloud Vision API with Python 3, but i'm getting the error""Cannot find reference 'Image' in types.py"""
1733,55886635,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Google Cloud Vision API with Python 3, but i'm getting the error""Cannot find reference 'Image' in types.py"" when i use:I made the correct imports and the documentation tells me to use this function to get an image. Anyone can help me?Code:Error Message:Google Cloud Vision API version: 0.36.0""",when i use:I made the correct imports and the documentation tells me to use this function to get an image.
1734,55886635,,2,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I'm using the Google Cloud Vision API with Python 3, but i'm getting the error""Cannot find reference 'Image' in types.py"" when i use:I made the correct imports and the documentation tells me to use this function to get an image. Anyone can help me?Code:Error Message:Google Cloud Vision API version: 0.36.0""","Anyone can help me?Code:Error Message:Google Cloud Vision API version: 0.36.0"""
1735,49672973,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""","""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available."
1736,49672973,,1,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",They are available if running in Google Compute Engine.
1737,49672973,,2,,"[{'score': 0.944551, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.944551,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""","Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials."
1738,49672973,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""","Seefor more information.""I"
1739,49672973,,4,,"[{'score': 0.727798, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727798,FALSE,0,TRUE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong.
1740,49672973,,5,,"[{'score': 0.681313, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.681313,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,FALSE,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""","Am i missing some important steps?"""
1741,35951874,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""","""I'm trying to run an app with node.js"
1742,35951874,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",functions my browser.
1743,35951874,,2,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",I checked in the terminal and my javascript file which includes node.js
1744,35951874,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",functions runs well.
1745,35951874,,4,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined.
1746,35951874,,5,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",I understand that this is because the browser can't run node.js
1747,35951874,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",aloneThe node.js
1748,35951874,,7,,"[{'score': 0.699729, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.699729,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal.
1749,35951874,,8,,"[{'score': 0.913895, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.913895,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",Therefore I proceeded to include:in my index.html
1750,35951874,,9,,"[{'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined.
1751,35951874,,10,,"[{'score': 0.512262, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.512262,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,FALSE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",Am I doing something wrong?
1752,35951874,,11,,"[{'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873263,TRUE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",Is there any way I could run this javascript file in the browser that has node.js
1753,35951874,,12,,"[{'score': 0.688562, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.688562,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,TRUE,0.681699,FALSE,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""","but specifically without using browserify (which caused me some directory problems in the past)?"""
1754,49944780,,0,,"[{'score': 0.516526, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.635961, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.516526,FALSE,0,FALSE,0,TRUE,0.635961,FALSE,0,FALSE,0,FALSE,"""I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error. I am not sure where the issue is. I make the request as suggested in the documentation .The error I get is:""","""I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error."
1755,49944780,,1,,"[{'score': 0.753635, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.753635,TRUE,"""I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error. I am not sure where the issue is. I make the request as suggested in the documentation .The error I get is:""",I am not sure where the issue is.
1756,49944780,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error. I am not sure where the issue is. I make the request as suggested in the documentation .The error I get is:""","I make the request as suggested in the documentation .The error I get is:"""
1757,49425490,,0,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this? I tried to do with an ajax request and via browser too.Thanks in advance!""","""I have a problem with the Google Cloud Vision Api."
1758,49425490,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this? I tried to do with an ajax request and via browser too.Thanks in advance!""",When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this?
1759,49425490,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this? I tried to do with an ajax request and via browser too.Thanks in advance!""","I tried to do with an ajax request and via browser too.Thanks in advance!"""
1760,55292198,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""","""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud."
1761,55292198,,1,,"[{'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,FALSE,0,TRUE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use.
1762,55292198,,2,,"[{'score': 0.500468, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.708196, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.816541, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.500468,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708196,FALSE,0,TRUE,0.816541,FALSE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions.
1763,55292198,,3,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""","Also, would something like socket.io"
1764,55292198,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",work for this?
1765,55292198,,5,,"[{'score': 0.527739, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.527739,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,TRUE,0.615352,FALSE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration.
1766,55292198,,6,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.994446,TRUE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",Appreciate any pointers.
1767,55292198,,7,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""","Thanks!"""
1768,41127382,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a call to the Amazon Rekognition service with NodeJS. The call is going through but I receive anerror in which it says:I'm basing my code off an S3 example:The documentation states that the service only accepts PNG or JPEG images but I can't figure out what is going on.""","""I'm trying to make a call to the Amazon Rekognition service with NodeJS."
1769,41127382,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a call to the Amazon Rekognition service with NodeJS. The call is going through but I receive anerror in which it says:I'm basing my code off an S3 example:The documentation states that the service only accepts PNG or JPEG images but I can't figure out what is going on.""","The call is going through but I receive anerror in which it says:I'm basing my code off an S3 example:The documentation states that the service only accepts PNG or JPEG images but I can't figure out what is going on."""
1770,53281993,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""","""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am."
1771,53281993,,1,,"[{'score': 0.694414, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.694414,FALSE,0,FALSE,0,TRUE,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""","If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there."""
1772,40984635,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google vision api to perform OCR on my images. The Json Output to the API call returns recognized words with bounding box information.Could someone please tell me how to use this bounding box information to do layout analysis of my image?If there is a library which takes this as input and returns sentences instead of words?For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line. Is there a library which can give me this information out of the box?Image used for OCR""","""I am trying to use google vision api to perform OCR on my images."
1773,40984635,,1,,"[{'score': 0.868755, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868755,FALSE,0,FALSE,0,TRUE,"""I am trying to use google vision api to perform OCR on my images. The Json Output to the API call returns recognized words with bounding box information.Could someone please tell me how to use this bounding box information to do layout analysis of my image?If there is a library which takes this as input and returns sentences instead of words?For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line. Is there a library which can give me this information out of the box?Image used for OCR""","The Json Output to the API call returns recognized words with bounding box information.Could someone please tell me how to use this bounding box information to do layout analysis of my image?If there is a library which takes this as input and returns sentences instead of words?For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line."
1774,40984635,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google vision api to perform OCR on my images. The Json Output to the API call returns recognized words with bounding box information.Could someone please tell me how to use this bounding box information to do layout analysis of my image?If there is a library which takes this as input and returns sentences instead of words?For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line. Is there a library which can give me this information out of the box?Image used for OCR""","Is there a library which can give me this information out of the box?Image used for OCR"""
1775,56256801,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!""","""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done)."
1776,56256801,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!""",Now i want to send video to AWS Rekognition and perform face detection on the live video.
1777,56256801,,2,,"[{'score': 0.974578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974578,FALSE,0,FALSE,0,TRUE,"""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!""",Kindly answer in detail and with links.
1778,56256801,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!""","Thankyou!"""
1779,47634218,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","""I try text_detection to photo."
1780,47634218,,1,,"[{'score': 0.618053, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.618053,FALSE,0,FALSE,0,TRUE,0.890188,TRUE,0.80026,FALSE,0,FALSE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","However, strange problem happens now.I post very simply debug program.the"
1781,47634218,,2,,"[{'score': 0.545272, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.87913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.545272,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87913,FALSE,0,TRUE,0.615352,FALSE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token."
1782,47634218,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",Add it to ServiceToken.(ref:)3:
1783,47634218,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",rbody is Google Cloud Vision's request.4:
1784,47634218,,5,,"[{'score': 0.572402, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.990154, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.572402,FALSE,0,FALSE,0,FALSE,0,TRUE,0.990154,FALSE,0,FALSE,0,FALSE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","Finally, using Request module."
1785,47634218,,6,,"[{'score': 0.905748, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.905748,FALSE,0,FALSE,0,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus,"
1786,47634218,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",I combined between firebase storage handler in cloud functions and this program.
1787,47634218,,8,,"[{'score': 0.502325, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.929993, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.502325,FALSE,0,FALSE,0,TRUE,0.929993,FALSE,0,FALSE,0,FALSE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","However, in Cloud functions, request module outputted this error.I can not understand this error."
1788,47634218,,9,,"[{'score': 0.535661, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.535661,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,FALSE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",I think that this error indicates a header.
1789,47634218,,10,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",But headers's variable is not modified.
1790,47634218,,11,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",Why behavior of request module changes in cloud functions?
1791,47634218,,12,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.968123,TRUE,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""","Does anyone know the details?"""
1792,50805898,,0,,"[{'score': 0.761877, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.761877,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,FALSE,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""","""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem"
1793,50805898,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""","File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:"""
1794,36416503,,0,,"[{'score': 0.735031, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.916265, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.735031,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.916265,FALSE,"""I'm getting this error:And it may have something to do with the encoding of the images, not sure.  I am sending Google a bunch ofs.Note that this is similar to, but that one doesn't help a whole lot - there's no answer.Here's my JSON:This is the first image I send it.Any thoughts on what's causing this?""","""I'm getting this error:And it may have something to do with the encoding of the images, not sure."
1795,36416503,,1,,"[{'score': 0.757749, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.757749,FALSE,0,FALSE,0,TRUE,"""I'm getting this error:And it may have something to do with the encoding of the images, not sure.  I am sending Google a bunch ofs.Note that this is similar to, but that one doesn't help a whole lot - there's no answer.Here's my JSON:This is the first image I send it.Any thoughts on what's causing this?""","I am sending Google a bunch ofs.Note that this is similar to, but that one doesn't help a whole lot - there's no answer.Here's my JSON:This is the first image I send it.Any thoughts on what's causing this?"""
1796,42391009,,0,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""","""I am using Google Vision API, primarily to extract texts."
1797,42391009,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""","I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line."
1798,42391009,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.660937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660937,FALSE,0,TRUE,0.681699,TRUE,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""","However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan."
1799,42391009,,3,,"[{'score': 0.637265, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.899725, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.899725,FALSE,0,TRUE,0.637265,TRUE,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""","I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex"""
1800,45075891,,0,,"[{'score': 0.649149, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.6821, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.649149,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,TRUE,0.6821,FALSE,0,FALSE,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""","""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use."
1801,45075891,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",But I don't know how many teacher labels it has.
1802,45075891,,2,,"[{'score': 0.922765, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.91588, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91588,FALSE,0,TRUE,0.922765,TRUE,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""","Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something."
1803,45075891,,3,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.822231,TRUE,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Anyone knows how many teacher label it has and the rules about the labels?(I
1804,45075891,,4,,"[{'score': 0.537774, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.537774,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,FALSE,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""","felt it has some hierarchical rules.)"""
1805,46731393,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""","""I am trying to Implement Google Vision OCR Request."
1806,46731393,,1,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.512886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512886,TRUE,0.509368,FALSE,0,TRUE,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""","Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true."
1807,46731393,,2,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.802462, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802462,FALSE,0,TRUE,0.786991,TRUE,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""","API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You"""
1808,52471113,,0,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""","""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON)."
1809,52471113,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""",I can get correct results back only when my request sent via CURL is under 16k or so.
1810,52471113,,2,,"[{'score': 0.617598, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.544189, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.617598,FALSE,0,FALSE,0,TRUE,0.544189,FALSE,0,FALSE,0,FALSE,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""",As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have.
1811,52471113,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""","Thanks."""
1812,51417691,,0,,"[{'score': 0.677676, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677676,FALSE,0,TRUE,0.75152,TRUE,"""How to darken the area around the field to be scanned?I would like the view as in the. I have a frame around, but I do not know how to dim the area around. I use google vision.[]""","""How to darken the area around the field to be scanned?I would like the view as in the."
1813,51417691,,1,,"[{'score': 0.987862, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.987862,TRUE,"""How to darken the area around the field to be scanned?I would like the view as in the. I have a frame around, but I do not know how to dim the area around. I use google vision.[]""","I have a frame around, but I do not know how to dim the area around."
1814,51417691,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How to darken the area around the field to be scanned?I would like the view as in the. I have a frame around, but I do not know how to dim the area around. I use google vision.[]""","I use google vision.[]"""
1815,49664844,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?""","""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku."
1816,49664844,,1,,"[{'score': 0.800509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.800509,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?""","Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable."
1817,49664844,,2,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,TRUE,0.75152,TRUE,"""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?""","Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?"""
1818,56043593,,0,,"[{'score': 0.745029, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.746925, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.745029,FALSE,0,FALSE,0,TRUE,0.746925,FALSE,0,FALSE,0,FALSE,"""I am following this tutorial:However, when I run the commandI get the following error:I have no idea what's wrong. I replaced the single quote with double quotes and double quotes with escaped double quotes (\"") so it'd work but I got that error.""","""I am following this tutorial:However, when I run the commandI get the following error:I have no idea what's wrong."
1819,56043593,,1,,"[{'score': 0.650627, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.650627,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am following this tutorial:However, when I run the commandI get the following error:I have no idea what's wrong. I replaced the single quote with double quotes and double quotes with escaped double quotes (\"") so it'd work but I got that error.""","I replaced the single quote with double quotes and double quotes with escaped double quotes (\"") so it'd work but I got that error."""
1820,38748027,,0,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API and upload an image using their API to get analysis. I am using this php code:It always show ""you did not upload image"". And here's my Swift function where I upload the image:When I do, I get:""","""I am trying to use Google Vision API and upload an image using their API to get analysis."
1821,38748027,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,TRUE,0.751512,FALSE,0,TRUE,"""I am trying to use Google Vision API and upload an image using their API to get analysis. I am using this php code:It always show ""you did not upload image"". And here's my Swift function where I upload the image:When I do, I get:""","I am using this php code:It always show ""you did not upload image""."
1822,38748027,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Vision API and upload an image using their API to get analysis. I am using this php code:It always show ""you did not upload image"". And here's my Swift function where I upload the image:When I do, I get:""","And here's my Swift function where I upload the image:When I do, I get:"""
1823,53203939,,0,,"[{'score': 0.593611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.593611,FALSE,0,FALSE,0,TRUE,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""","""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i"
1824,53203939,,1,,"[{'score': 0.778006, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.778006,FALSE,0,FALSE,0,TRUE,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""",found a library on githup that detect  regions of image to find texts and after detection it does ocr.
1825,53203939,,2,,"[{'score': 0.610552, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.610552,FALSE,0,FALSE,0,TRUE,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""","I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that."
1826,53203939,,3,,"[{'score': 0.864115, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.864115,FALSE,0,FALSE,0,TRUE,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""","If it contains text then it should OCR, if not it should discard the image."
1827,53203939,,4,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.998976,TRUE,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""","Any ideas?"""
1828,38288634,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""","""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR."
1829,38288634,,1,,"[{'score': 0.728129, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.728129,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,TRUE,0.80026,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",It works perfect and i get the response in a Json object.
1830,38288634,,2,,"[{'score': 0.600431, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.600431,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",The problem is that when I try to detect swedish text it does not return swedish letters.
1831,38288634,,3,,"[{'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849827,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",Api correctly detects the local.
1832,38288634,,4,,"[{'score': 0.597849, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.597565, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.5687, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.597849,FALSE,0,FALSE,0,TRUE,0.597565,FALSE,0,TRUE,0.5687,FALSE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""","it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response."
1833,38288634,,5,,"[{'score': 0.823464, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.823464,FALSE,0,TRUE,0.804675,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""","I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful."""
1834,40526090,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""Got a problem with project oxford vision API. The example fromworks fine and recognise text on images. But my code throws exception:Class code:Did anyone have same problem and how can i solve it?""","""Got a problem with project oxford vision API."
1835,40526090,,1,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""Got a problem with project oxford vision API. The example fromworks fine and recognise text on images. But my code throws exception:Class code:Did anyone have same problem and how can i solve it?""",The example fromworks fine and recognise text on images.
1836,40526090,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.5538,TRUE,"""Got a problem with project oxford vision API. The example fromworks fine and recognise text on images. But my code throws exception:Class code:Did anyone have same problem and how can i solve it?""","But my code throws exception:Class code:Did anyone have same problem and how can i solve it?"""
1837,39478404,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","""I'm trying to make a batch request to Google Vision Text Detection API."
1838,39478404,,1,,"[{'score': 0.745225, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.745225,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","So far, I put the paths of the images into a list, make the batch request and get the responses."
1839,39478404,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","However, I cannot determine which result belongs to which image."
1840,39478404,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs."
1841,39478404,,4,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.822231,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","However, I cannot put any custom field into the request."
1842,39478404,,5,,"[{'score': 0.675067, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.675067,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,FALSE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""",Is there something wrong with my approach?
1843,39478404,,6,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""","How can I know which response belongs to which image?Here is the code I use for these requests:"""
1844,46790435,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I m calling Microsoft Face API for detecting the face in an image. While loading an image from local , I always get an error as belowBut using the same image via URL , its working fine.""","""I m calling Microsoft Face API for detecting the face in an image."
1845,46790435,,1,,"[{'score': 0.676663, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.579436, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.676663,FALSE,0,FALSE,0,TRUE,0.579367,TRUE,0.579436,FALSE,0,FALSE,"""I m calling Microsoft Face API for detecting the face in an image. While loading an image from local , I always get an error as belowBut using the same image via URL , its working fine.""","While loading an image from local , I always get an error as belowBut using the same image via URL , its working fine."""
1846,51095417,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.785611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.785611,FALSE,0,TRUE,0.615352,TRUE,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""","""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database."
1847,51095417,,1,,"[{'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.725909, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.725909,FALSE,0,TRUE,0.704683,TRUE,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""","Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help."""
1848,54961012,,0,,"[{'score': 0.754467, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.754467,FALSE,0,FALSE,0,TRUE,"""I would appreciate some guidance on the following issue.Use Case:Create a collection with known faces.Search a stored video toidentify ""known"" faces & draw a bounding box around them in the videoframeSteps taken:I'm able to create a collection and index faces I'm able to analysethe stored video and get the results of PersonMatch and FaceMatchusing getFaceSearch()I'm able to draw the bounding boxes around Persons found in the video, etc., however...Issue:The response of getFaceSearch() returns an array of FaceMatches.However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video.I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame. Here is the API document that I'm referring to.Thanks for your help on this issue!""","""I would appreciate some guidance on the following issue.Use Case:Create a collection with known faces.Search a stored video toidentify ""known"" faces & draw a bounding box around them in the videoframeSteps taken:I'm able to create a collection and index faces I'm able to analysethe stored video and get the results of PersonMatch and FaceMatchusing getFaceSearch()I'm able to draw the bounding boxes around Persons found in the video, etc., however...Issue:The response of getFaceSearch() returns an array of FaceMatches.However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video.I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame."
1849,54961012,,1,,"[{'score': 0.784247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784247,FALSE,0,FALSE,0,TRUE,"""I would appreciate some guidance on the following issue.Use Case:Create a collection with known faces.Search a stored video toidentify ""known"" faces & draw a bounding box around them in the videoframeSteps taken:I'm able to create a collection and index faces I'm able to analysethe stored video and get the results of PersonMatch and FaceMatchusing getFaceSearch()I'm able to draw the bounding boxes around Persons found in the video, etc., however...Issue:The response of getFaceSearch() returns an array of FaceMatches.However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video.I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame. Here is the API document that I'm referring to.Thanks for your help on this issue!""","Here is the API document that I'm referring to.Thanks for your help on this issue!"""
1850,47833305,,0,,"[{'score': 0.593611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.593611,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect logos of brands or companies, when testing everything works correctly. However, in the application that I am developing I need to upload images or logos of brands that are not so popular (new companies, new logos or new brands e).Example: it is necessary for a new company to upload logos and images and remain in the google database to be able to scan or upload an image and theAPI REST  I get the name of the  newcompany.""","""I'm using Google Cloud Vision API to detect logos of brands or companies, when testing everything works correctly."
1851,47833305,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to detect logos of brands or companies, when testing everything works correctly. However, in the application that I am developing I need to upload images or logos of brands that are not so popular (new companies, new logos or new brands e).Example: it is necessary for a new company to upload logos and images and remain in the google database to be able to scan or upload an image and theAPI REST  I get the name of the  newcompany.""","However, in the application that I am developing I need to upload images or logos of brands that are not so popular (new companies, new logos or new brands e).Example: it is necessary for a new company to upload logos and images and remain in the google database to be able to scan or upload an image and theAPI REST  I get the name of the  newcompany."""
1852,44762298,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API. However, the output I am getting from theis only providing individual characters as an output, when I require words to be grouped together.Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or orfunction in Python?I am not looking for full text extraction as my .jpg files are not visually structured in a way that thefunction satisfies.Google's Sample Code:Sample output of the off the shelf sample provided by Google:""","""I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API."
1853,44762298,,1,,"[{'score': 0.581742, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.581742,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API. However, the output I am getting from theis only providing individual characters as an output, when I require words to be grouped together.Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or orfunction in Python?I am not looking for full text extraction as my .jpg files are not visually structured in a way that thefunction satisfies.Google's Sample Code:Sample output of the off the shelf sample provided by Google:""","However, the output I am getting from theis only providing individual characters as an output, when I require words to be grouped together.Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or orfunction in Python?I am not looking for full text extraction as my .jpg"
1854,44762298,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API. However, the output I am getting from theis only providing individual characters as an output, when I require words to be grouped together.Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or orfunction in Python?I am not looking for full text extraction as my .jpg files are not visually structured in a way that thefunction satisfies.Google's Sample Code:Sample output of the off the shelf sample provided by Google:""","files are not visually structured in a way that thefunction satisfies.Google's Sample Code:Sample output of the off the shelf sample provided by Google:"""
1855,46995404,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should theoption look like?""","""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175."
1856,46995404,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should theoption look like?""","Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded."
1857,46995404,,2,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should theoption look like?""","However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first?"
1858,46995404,,3,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should theoption look like?""","How should theoption look like?"""
1859,39407269,,0,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""So I just want to detect text or labels from an image using the google cloud vision API. But When I run this code I always get:But I don't know why...here is the full json output what I get:My test code is here:So the question is.. what is wrong with this code?""","""So I just want to detect text or labels from an image using the google cloud vision API."
1860,39407269,,1,,"[{'score': 0.503446, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.503446,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I just want to detect text or labels from an image using the google cloud vision API. But When I run this code I always get:But I don't know why...here is the full json output what I get:My test code is here:So the question is.. what is wrong with this code?""","But When I run this code I always get:But I don't know why...here is the full json output what I get:My test code is here:So the question is.. what is wrong with this code?"""
1861,50733961,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""","""I am following instructions from a github page documentation."
1862,50733961,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""","And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID."
1863,50733961,,2,,"[{'score': 0.772053, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.772053,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""","But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before."""
1864,52925404,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""","""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called."
1865,52925404,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""","To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file."
1866,52925404,,2,,"[{'score': 0.700439, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.761647, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700439,FALSE,0,TRUE,0.761647,TRUE,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""","Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?"""
1867,48706864,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""","""I have a simple Python application which uses Google vision API."
1868,48706864,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""",How can I make it to work offline?
1869,48706864,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.946222,TRUE,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""",I searched a lot but couldn't find anything useful.
1870,48706864,,3,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""",Thisis about android app and it seems that for that case the answer is positive.
1871,48706864,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""","Here is my code:"""
1872,54113520,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have built an application with the following architecture :Client : Angular 7Back-end: Spring bootThe aim is to take a screenshot of the user and compare it with a picture in a database, using AWS Rekognition API.But I keep getting a CORS errorThe front make a POST calls to the back (a REST api built with Spring) and send an imageThe Back get the image and send it to AWSRekognition API (using AWS SDK) with the target image stored in a databaseThe AWS api send back the resultThe Back Spring api send back to the front-end the resultWhen I use Postman and keep the code which calls the AWS API: OK it works fineWhen I try on my web browser and KEEP the AWS API call code : I get a CORS errorHere is the Postman screenshot :Here is the Angular code : picture.components.tsHere is the request.service.tsIf someone can help me, that's would be super great,CheersAlexis""","""I have built an application with the following architecture :Client : Angular 7Back-end: Spring bootThe aim is to take a screenshot of the user and compare it with a picture in a database, using AWS Rekognition API.But I keep getting a CORS errorThe front make a POST calls to the back (a REST api built with Spring) and send an imageThe Back get the image and send it to AWSRekognition API (using AWS SDK) with the target image stored in a databaseThe AWS api send back the resultThe Back Spring api send back to the front-end the resultWhen I use Postman and keep the code which calls the AWS API: OK it works fineWhen I try on my web browser and KEEP the AWS API call code : I get a CORS errorHere is the Postman screenshot :Here is the Angular code : picture.components.tsHere is the request.service.tsIf"
1873,54113520,,1,,"[{'score': 0.736685, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.736685,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""I have built an application with the following architecture :Client : Angular 7Back-end: Spring bootThe aim is to take a screenshot of the user and compare it with a picture in a database, using AWS Rekognition API.But I keep getting a CORS errorThe front make a POST calls to the back (a REST api built with Spring) and send an imageThe Back get the image and send it to AWSRekognition API (using AWS SDK) with the target image stored in a databaseThe AWS api send back the resultThe Back Spring api send back to the front-end the resultWhen I use Postman and keep the code which calls the AWS API: OK it works fineWhen I try on my web browser and KEEP the AWS API call code : I get a CORS errorHere is the Postman screenshot :Here is the Angular code : picture.components.tsHere is the request.service.tsIf someone can help me, that's would be super great,CheersAlexis""","someone can help me, that's would be super great,CheersAlexis"""
1874,49518796,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}""","""I am Using Google vision API and developing BARcodes/QRcodes scanning APP."
1875,49518796,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}""",I am also trying to use Overlay to make the scanning square area.
1876,49518796,,2,,"[{'score': 0.515711, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515711,TRUE,"""I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}""","But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight."
1877,49518796,,3,,"[{'score': 0.518191, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.518191,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.845297,FALSE,"""I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}""","Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}"""
1878,54824911,,0,,"[{'score': 0.779757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.779757,FALSE,0,FALSE,0,TRUE,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""","""I am using Lambda to detect faces and would like to send the response to a Dynamotable."
1879,54824911,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1.
1880,54824911,,2,,"[{'score': 0.54506, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.54506,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",What to add as a key and partition key in the table?2.
1881,54824911,,3,,"[{'score': 0.786514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786514,FALSE,0,FALSE,0,TRUE,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result.
1882,54824911,,4,,"[{'score': 0.789435, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.789435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""","Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!"""
1883,40472437,,0,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""","""I am using microsoft face api from my client side code using java script/Jquery.Here is the code."
1884,40472437,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",I am capturing the image using camera and then convert that image to a blob and send that to the api.
1885,40472437,,2,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",I am getting the results.
1886,40472437,,3,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,TRUE,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",But this api takes around 4-6 seconds to get the results.
1887,40472437,,4,,"[{'score': 0.990865, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.990865,TRUE,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""","Is this usual or there could be some performance improvement?Thank you!"""
1888,47906157,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.My source code is taken from the Google Cloud tutorial for this API and it is the following:However I get the following error:This is weird because:1) I created a new service account2) I addedto my .bash_profile (I put the json file at the Pycharm file of this project)Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.How can I fix this bug and make the program running?By the way the problem is solved if I simply addto my source code.""","""I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.My source code is taken from the Google Cloud tutorial for this API and it is the following:However I get the following error:This is weird because:1) I created a new service account2) I addedto my .bash_profile"
1889,47906157,,1,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.My source code is taken from the Google Cloud tutorial for this API and it is the following:However I get the following error:This is weird because:1) I created a new service account2) I addedto my .bash_profile (I put the json file at the Pycharm file of this project)Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.How can I fix this bug and make the program running?By the way the problem is solved if I simply addto my source code.""","(I put the json file at the Pycharm file of this project)Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.How can I fix this bug and make the program running?By the way the problem is solved if I simply addto my source code."""
1890,40037830,,0,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.716301,TRUE,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""","""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not."
1891,40037830,,1,,"[{'score': 0.548513, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.730332, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.548513,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.730332,FALSE,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""","I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated."""
1892,56334124,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks""","""I am streaming video the amazon kinesis from raspberry pi (This is done)."
1893,56334124,,1,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,FALSE,0,TRUE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks""",Now i want to perform face detection/recognition on that video using amazon Rekognition.
1894,56334124,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks""",For that i need to connect aws kinesis with rekognition how to connect them?
1895,56334124,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks""","Thanks"""
1896,54015378,,0,,"[{'score': 0.714106, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.565996, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.714106,FALSE,0,FALSE,0,TRUE,0.565996,FALSE,0,FALSE,0,FALSE,"""I try to compare the two image which is in s3.So I have completed the code by referring to the following:I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).But it makes that error.com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)I thought if I make a mistake of IAM, the app can't upload the file.I try to upload the file with same credentialsProvider, upload success.I don't think that's what happened because of permission.S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2is there any information to get s3 object with Rekognition?""","""I try to compare the two image which is in s3.So I have completed the code by referring to the following:I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).But it makes that error.com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions."
1897,54015378,,1,,"[{'score': 0.582716, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.8067, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.582716,FALSE,0,FALSE,0,TRUE,0.8067,FALSE,0,FALSE,0,FALSE,"""I try to compare the two image which is in s3.So I have completed the code by referring to the following:I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).But it makes that error.com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)I thought if I make a mistake of IAM, the app can't upload the file.I try to upload the file with same credentialsProvider, upload success.I don't think that's what happened because of permission.S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2is there any information to get s3 object with Rekognition?""","(Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)I thought if I make a mistake of IAM, the app can't upload the file.I try to upload the file with same credentialsProvider, upload success.I don't think that's what happened because of permission.S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2is there any information to get s3 object with Rekognition?"""
1898,43814654,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to usein my Google App Engine Python application. Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?""","""I am trying to usein my Google App Engine Python application."
1899,43814654,,1,,"[{'score': 0.523235, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.523235,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to usein my Google App Engine Python application. Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?""","Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?"""
1900,47534024,,0,,"[{'score': 0.612109, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.612109,FALSE,0,FALSE,0,TRUE,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""","""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images."
1901,47534024,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""","I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?"""
1902,53039190,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""","""I am using Goggle vision 'documentTextDetection' for one of my project."
1903,53039190,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""","My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same."
1904,53039190,,2,,"[{'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713028,FALSE,0,FALSE,0,TRUE,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""","Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses?"
1905,53039190,,3,,"[{'score': 0.859009, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.859009,FALSE,0,TRUE,0.75152,TRUE,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""","Also let me know anything I can do to improve the results."""
1906,42970980,,0,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""","""I'm making a little project using the Google Vision API."
1907,42970980,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""",I want to detect the face of a base64 encoded image that a send to the API in a POST request.
1908,42970980,,2,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""",My code is based on this tutorial of Google:.
1909,42970980,,3,,"[{'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,FALSE,0,TRUE,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""","Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string."
1910,42970980,,4,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.75152,TRUE,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""","I just don't know what I am doing wrong.Thanks in advance."""
1911,47428173,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""","""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV?"
1912,47428173,,1,,"[{'score': 0.783887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.783887,FALSE,0,FALSE,0,TRUE,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""","Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?"""
1913,44862532,,0,,"[{'score': 0.564523, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.564523,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""","""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery."
1914,44862532,,1,,"[{'score': 0.61859, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.61859,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,FALSE,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""","Testing in Chrome, I get a 400 error via the console and no further info to help in debugging."
1915,44862532,,2,,"[{'score': 0.629021, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.629021,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""","I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data)."
1916,44862532,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""","The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:"""
1917,52509581,,0,,"[{'score': 0.736294, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.736294,FALSE,0,FALSE,0,TRUE,"""I am trying to create an app which makes use ofAmazon Rekogitionin AWS for identification of a person and retrieving the personal information for an internal storage system.I wanted to know how to connect the Amazon Rekognition part and the information stored in the database. The face detection part will be done by Amazon Rekognition, but how will my app store and retrieve the personal information after detection of face?Can anyone give me a sample code in Java for retrieving the information stored inusing?""","""I am trying to create an app which makes use ofAmazon Rekogitionin AWS for identification of a person and retrieving the personal information for an internal storage system.I wanted to know how to connect the Amazon Rekognition part and the information stored in the database."
1918,52509581,,1,,"[{'score': 0.654488, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.654488,FALSE,0,FALSE,0,TRUE,"""I am trying to create an app which makes use ofAmazon Rekogitionin AWS for identification of a person and retrieving the personal information for an internal storage system.I wanted to know how to connect the Amazon Rekognition part and the information stored in the database. The face detection part will be done by Amazon Rekognition, but how will my app store and retrieve the personal information after detection of face?Can anyone give me a sample code in Java for retrieving the information stored inusing?""","The face detection part will be done by Amazon Rekognition, but how will my app store and retrieve the personal information after detection of face?Can anyone give me a sample code in Java for retrieving the information stored inusing?"""
1919,50598790,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks""","""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now"
1920,50598790,,1,,"[{'score': 0.718921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718921,FALSE,0,FALSE,0,TRUE,"""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks""","here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks"""
1921,36976312,,0,,"[{'score': 0.798791, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.798791,TRUE,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""","""I am trying to extract text from a picture taken in Android Mobile through some API."
1922,36976312,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""",Will Google Vision help me with that?
1923,36976312,,2,,"[{'score': 0.868721, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868721,TRUE,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""",I used OCR too but I felt that the output is not accurate.
1924,36976312,,3,,"[{'score': 0.999857, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999857,TRUE,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""","Any suggestions?"""
1925,53799577,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""","""I've been using microsoft computer vision cognitive services API as trial version."
1926,53799577,,1,,"[{'score': 0.775384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775384,FALSE,0,FALSE,0,TRUE,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""","I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated."""
1927,55947906,,0,,"[{'score': 0.808897, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.808897,FALSE,0,FALSE,0,TRUE,"""I am using the amazon rekognition API to analyse my video to find and Search faces.A one minute video has been on processing for nearly an hour now. But have not received any result .Is this normal ?""","""I am using the amazon rekognition API to analyse my video to find and Search faces.A one minute video has been on processing for nearly an hour now."
1928,55947906,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.856622,TRUE,"""I am using the amazon rekognition API to analyse my video to find and Search faces.A one minute video has been on processing for nearly an hour now. But have not received any result .Is this normal ?""","But have not received any result .Is this normal ?"""
1929,53109098,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was trying to create an ios app for text recognition with Google Vision text recognition.I had integrated all the required pods into my project as mentioned inIn order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class.Whenever I create an instance of that class and with a variable and try to access the properties of that class it there is aRed errorindication at the line where I try to access the properties with the instance variable of the class and aGRAY ERRORindication at the top of theViewController ClassThe ERROR MESSAGEGoogle ML DocumentationIn that documentation, they have used let and I also checked with that but every time same problem.Here is the code also:""","""I was trying to create an ios app for text recognition with Google Vision text recognition.I had integrated all the required pods into my project as mentioned inIn order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class.Whenever"
1930,53109098,,1,,"[{'score': 0.561882, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.561882,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I was trying to create an ios app for text recognition with Google Vision text recognition.I had integrated all the required pods into my project as mentioned inIn order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class.Whenever I create an instance of that class and with a variable and try to access the properties of that class it there is aRed errorindication at the line where I try to access the properties with the instance variable of the class and aGRAY ERRORindication at the top of theViewController ClassThe ERROR MESSAGEGoogle ML DocumentationIn that documentation, they have used let and I also checked with that but every time same problem.Here is the code also:""","I create an instance of that class and with a variable and try to access the properties of that class it there is aRed errorindication at the line where I try to access the properties with the instance variable of the class and aGRAY ERRORindication at the top of theViewController ClassThe ERROR MESSAGEGoogle ML DocumentationIn that documentation, they have used let and I also checked with that but every time same problem.Here is the code also:"""
1931,45126387,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.810415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.810415,FALSE,0,TRUE,0.525007,TRUE,"""Is there any way of using Watsons image classifying abilities to extract information from an image of a document? Rather than simply classifying an image as a, b or c?""","""Is there any way of using Watsons image classifying abilities to extract information from an image of a document?"
1932,45126387,,1,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.904038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904038,FALSE,0,TRUE,0.822231,TRUE,"""Is there any way of using Watsons image classifying abilities to extract information from an image of a document? Rather than simply classifying an image as a, b or c?""","Rather than simply classifying an image as a, b or c?"""
1933,39768788,,0,,"[{'score': 0.725194, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.725194,FALSE,0,FALSE,0,TRUE,"""Is there any API to compare two images and gives me similarity score?I have tried using IBM watson and Google vision services.But they are doing sophisticated stuffs of matching similar images across net. Is there any straight forward API where I can mention URL of two images and it gives me similarity score between them""","""Is there any API to compare two images and gives me similarity score?I have tried using IBM watson and Google vision services.But they are doing sophisticated stuffs of matching similar images across net."
1934,39768788,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there any API to compare two images and gives me similarity score?I have tried using IBM watson and Google vision services.But they are doing sophisticated stuffs of matching similar images across net. Is there any straight forward API where I can mention URL of two images and it gives me similarity score between them""","Is there any straight forward API where I can mention URL of two images and it gives me similarity score between them"""
1935,50343162,,0,,"[{'score': 0.566178, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.566178,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,FALSE,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""","""I've been unable to find the URL to make the API call for AWS Rekognition for text detection."
1936,50343162,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""","I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?"""
1937,53718791,,0,,"[{'score': 0.788351, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.788351,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this...""","""I am new to 'AWS'.."
1938,53718791,,1,,"[{'score': 0.537959, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.963748, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.537959,FALSE,0,FALSE,0,TRUE,0.963748,FALSE,0,FALSE,0,FALSE,"""I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this...""",I trying to compare two faces using command line..
1939,53718791,,2,,"[{'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,FALSE,0,TRUE,"""I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this...""","I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this..."""
1940,50868017,,0,,"[{'score': 0.542112, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.542112,FALSE,0,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,FALSE,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""","""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this."
1941,50868017,,1,,"[{'score': 0.583568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583568,FALSE,0,FALSE,0,TRUE,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""","I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text."
1942,50868017,,2,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""",Shouldn't both python and the browser return the same result?.
1943,50868017,,3,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""","And if not, why not?, Do I need to include additional parameters in the code?."""
1944,51257124,,0,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""","""I am using OpenCV in Python on MacOS and Linux Ubuntu systems."
1945,51257124,,1,,"[{'score': 0.672133, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672133,FALSE,0,FALSE,0,TRUE,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""","My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg""."
1946,51257124,,2,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""","However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that?"
1947,51257124,,3,,"[{'score': 0.628582, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.838593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.628582,FALSE,0,FALSE,0,FALSE,0,TRUE,0.838593,FALSE,0,TRUE,0.91961,FALSE,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""",Is there any way to solve it?
1948,51257124,,4,,"[{'score': 0.503121, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.666663, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.503121,FALSE,0,FALSE,0,TRUE,0.666663,FALSE,0,FALSE,0,FALSE,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""","Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image."""
1949,41972937,,0,,"[{'score': 0.550349, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.550349,FALSE,0,FALSE,0,TRUE,"""I am using trail version of google vision API ,using the rest API i am trying to get the face_detection values from postman tool but i am facing an issue showed below.can anyone help me on this.""","""I am using trail version of google vision API ,using the rest API i am trying to get the face_detection values from postman tool but i am facing an issue showed below.can"
1950,41972937,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I am using trail version of google vision API ,using the rest API i am trying to get the face_detection values from postman tool but i am facing an issue showed below.can anyone help me on this.""","anyone help me on this."""
1951,48527517,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am studying Amazon Rekognition API. I would to like to know if it is possible to call Amazon Rekognition API via curl?""","""I am studying Amazon Rekognition API."
1952,48527517,,1,,"[{'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,TRUE,0.775166,TRUE,"""I am studying Amazon Rekognition API. I would to like to know if it is possible to call Amazon Rekognition API via curl?""","I would to like to know if it is possible to call Amazon Rekognition API via curl?"""
1953,53578143,,0,,"[{'score': 0.879398, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.836435, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.879398,FALSE,0,FALSE,0,TRUE,0.836435,FALSE,0,FALSE,0,FALSE,"""I am doing some thing wrong over here, while comparing two images in different S3 Bucket.Even though, I am comparing images of male and female it would give 99% confidenceor am i missing something in the declaration yetMaybe This line is causing a problemOr my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below. What else do i need to correct so that it will return the confidence within the rang specified""","""I am doing some thing wrong over here, while comparing two images in different S3 Bucket.Even though, I am comparing images of male and female it would give 99% confidenceor am i missing something in the declaration yetMaybe This line is causing a problemOr my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below."
1954,53578143,,1,,"[{'score': 0.618533, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.816764, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.887937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.618533,FALSE,0,FALSE,0,FALSE,0,TRUE,0.887937,TRUE,0.816764,FALSE,0,FALSE,"""I am doing some thing wrong over here, while comparing two images in different S3 Bucket.Even though, I am comparing images of male and female it would give 99% confidenceor am i missing something in the declaration yetMaybe This line is causing a problemOr my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below. What else do i need to correct so that it will return the confidence within the rang specified""","What else do i need to correct so that it will return the confidence within the rang specified"""
1955,51646954,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""","""I am trying to read handwritten text from an image using Google Vision API."
1956,51646954,,1,,"[{'score': 0.669478, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.698634, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.669478,FALSE,0,FALSE,0,TRUE,0.698634,FALSE,0,FALSE,0,FALSE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""","But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block."
1957,51646954,,2,,"[{'score': 0.559057, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.559057,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,FALSE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""",Even though I am scanning the same page.
1958,51646954,,3,,"[{'score': 0.602762, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.602762,FALSE,0,FALSE,0,TRUE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""","For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10."
1959,51646954,,4,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.866434, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.866434,FALSE,0,TRUE,0.58393,TRUE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""",There is no inconsistency.I understand the position of text in blocks depends on the scanned document.
1960,51646954,,5,,"[{'score': 0.50459, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.50459,FALSE,0,FALSE,0,TRUE,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""","But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API."""
1961,47466195,,0,,"[{'score': 0.609946, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.609946,FALSE,0,FALSE,0,TRUE,"""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next. I am completely stuck right now.""","""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next."
1962,47466195,,1,,"[{'score': 0.711683, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.942582, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.711683,FALSE,0,FALSE,0,FALSE,0,TRUE,0.942582,FALSE,0,FALSE,"""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next. I am completely stuck right now.""","I am completely stuck right now."""
1963,42914619,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?Here is the package.json that I am using:and the error throws while loading the vision api module -  at the line mention below""","""Trying to use Google Vision Api from Bot Framework app hosted on Azure."
1964,42914619,,1,,"[{'score': 0.794436, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.794436,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,FALSE,"""Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?Here is the package.json that I am using:and the error throws while loading the vision api module -  at the line mention below""",The code works just fine on local but I get this error when I try it on Azure.
1965,42914619,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?Here is the package.json that I am using:and the error throws while loading the vision api module -  at the line mention below""",Can someone help?Here is the package.json
1966,42914619,,3,,"[{'score': 0.607118, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.810415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.607118,FALSE,0,FALSE,0,TRUE,0.810415,FALSE,0,FALSE,0,FALSE,"""Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?Here is the package.json that I am using:and the error throws while loading the vision api module -  at the line mention below""","that I am using:and the error throws while loading the vision api module -  at the line mention below"""
1967,44373968,,0,,"[{'score': 0.63698, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.63698,TRUE,"""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right.How do I use vision.detectText() when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket() indicated onI have:the console reports:I have tried using:but the error is:""","""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText()"
1968,44373968,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right.How do I use vision.detectText() when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket() indicated onI have:the console reports:I have tried using:but the error is:""",syntax right.How do I use vision.detectText()
1969,44373968,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right.How do I use vision.detectText() when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket() indicated onI have:the console reports:I have tried using:but the error is:""",when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket()
1970,44373968,,3,,"[{'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,TRUE,"""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right.How do I use vision.detectText() when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket() indicated onI have:the console reports:I have tried using:but the error is:""","indicated onI have:the console reports:I have tried using:but the error is:"""
1971,54406629,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""","""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API."
1972,54406629,,1,,"[{'score': 0.784758, 'tone_id': 'anger', 'tone_name': 'Anger'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.784758,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",Is there anyway that I can do that in R ?Expample..
1973,54406629,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",I am doing text detection for one creativeSo my code isthis only gives me text on that creative.
1974,54406629,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",How do I get the color that are used on the creative below.
1975,54406629,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""","I also want the dominant color detection feature which google vision has under there 'properties' tab."""
1976,51691313,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need help with my Ionic typescript code.This uses a Google Cloud Vision API to tag photos that you upload to Firebase Storage.My problem: (returns the following error)My ts file:This index.ts file is located at AppName/functions/src/index.ts.""","""I need help with my Ionic typescript code.This uses a Google Cloud Vision API to tag photos that you upload to Firebase Storage.My problem: (returns the following error)My ts file:This index.ts"
1977,51691313,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need help with my Ionic typescript code.This uses a Google Cloud Vision API to tag photos that you upload to Firebase Storage.My problem: (returns the following error)My ts file:This index.ts file is located at AppName/functions/src/index.ts.""","file is located at AppName/functions/src/index.ts."""
1978,45942150,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches () within a particular website, rather than the entire web, as is the default behavior.I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"" (for example) in the search bar.Is this possible with the Google Cloud Vision API?""","""Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches () within a particular website, rather than the entire web, as is the default behavior.I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"""
1979,45942150,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.916468, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.916468,FALSE,0,TRUE,0.681699,TRUE,"""Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches () within a particular website, rather than the entire web, as is the default behavior.I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"" (for example) in the search bar.Is this possible with the Google Cloud Vision API?""","(for example) in the search bar.Is this possible with the Google Cloud Vision API?"""
1980,51036159,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""","""I need only video codec for videos from the s3 bucket."
1981,51036159,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",I don't want to use label detection operation to get the video metadata for video.
1982,51036159,,2,,"[{'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.663824, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.663824,FALSE,0,TRUE,0.511119,TRUE,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""","Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated."""
1983,44041039,,0,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""I have an issue with detecting image whether it is painting image or real picture taken. I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that.Appreciate if you can share algorithm how to detect it.""","""I have an issue with detecting image whether it is painting image or real picture taken."
1984,44041039,,1,,"[{'score': 0.776154, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.776154,FALSE,0,FALSE,0,TRUE,"""I have an issue with detecting image whether it is painting image or real picture taken. I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that.Appreciate if you can share algorithm how to detect it.""","I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that.Appreciate if you can share algorithm how to detect it."""
1985,47647693,,0,,"[{'score': 0.586987, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586987,FALSE,0,FALSE,0,TRUE,"""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py file.The error i'm getting is as followsThanks in advance!""","""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py"
1986,47647693,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py file.The error i'm getting is as followsThanks in advance!""","file.The error i'm getting is as followsThanks in advance!"""
1987,50545515,,0,,"[{'score': 0.627784, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.595897, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.627784,FALSE,0,FALSE,0,TRUE,0.595897,FALSE,0,FALSE,0,FALSE,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""","""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong?"
1988,50545515,,1,,"[{'score': 0.561023, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.561023,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""",Please help me.My Environment isUbuntu 16.04.3
1989,50545515,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""",LTS (GNU/Linux 4.4.0-112-generic
1990,50545515,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""",x86_64)Python 3.5.2google-cloud-vision
1991,50545515,,4,,"[{'score': 0.689978, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.689978,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""","(0.31.1)The error logs:The script exported above errors is following:"""
1992,55927749,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""","""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described."
1993,55927749,,1,,"[{'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825035,FALSE,0,TRUE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",All configurations and credentials are set and the app is running.
1994,55927749,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",But it's just stuck in the loop printing:And never get's out.
1995,55927749,,3,,"[{'score': 0.641509, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.974792, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.641509,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,TRUE,0.974792,FALSE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",Not sure if anything is wrong with the code or configurations or whatnot.What are the problems?
1996,55927749,,4,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,TRUE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",Why I don't see any results back?
1997,55927749,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""","Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:"""
1998,53117283,,0,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""","""I have been using google vision OCR for a while now."
1999,53117283,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",And I have observed that the OCR result varies with image dimension.
2000,53117283,,2,,"[{'score': 0.552969, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.911475, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.552969,FALSE,0,FALSE,0,FALSE,0,TRUE,0.911475,FALSE,0,FALSE,0,FALSE,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720.
2001,53117283,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result?
2002,53117283,,4,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""","How does the image dimensions affect the OCR result?"""
2003,48473858,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""","""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me."
2004,48473858,,1,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""","Thanks you."""
2005,38914432,,0,,"[{'score': 0.743828, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743828,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""","""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked."
2006,38914432,,1,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.647986,TRUE,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",If this variable is specified it should point to a file that defines the credentials.
2007,38914432,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""","The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create."
2008,38914432,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing.
2009,38914432,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""","This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!"""
2010,48625509,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""","""I am preparing my first batch of requests to google vision/natural language apis."
2011,48625509,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""",I plan on sending enough requests to exceed the free quota.
2012,48625509,,2,,"[{'score': 0.603388, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.603388,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""",I do still have my $300 in free credits in my account.
2013,48625509,,3,,"[{'score': 0.52458, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.663387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.52458,TRUE,0.663387,FALSE,0,FALSE,0,FALSE,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""","So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks"""
2014,42162320,,0,,"[{'score': 0.824794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824794,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision for face detection on Android. Currently my code:The problem is thatandare not correct and even negative sometimes. I know that to get correct coordinates it should be rotated somehow, but how exactly?""","""I'm using Google Vision for face detection on Android."
2015,42162320,,1,,"[{'score': 0.576298, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.956336, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.576298,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.956336,FALSE,"""I'm using Google Vision for face detection on Android. Currently my code:The problem is thatandare not correct and even negative sometimes. I know that to get correct coordinates it should be rotated somehow, but how exactly?""",Currently my code:The problem is thatandare not correct and even negative sometimes.
2016,42162320,,2,,"[{'score': 0.727798, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,TRUE,0.727798,FALSE,0,TRUE,"""I'm using Google Vision for face detection on Android. Currently my code:The problem is thatandare not correct and even negative sometimes. I know that to get correct coordinates it should be rotated somehow, but how exactly?""","I know that to get correct coordinates it should be rotated somehow, but how exactly?"""
2017,48022812,,0,,"[{'score': 0.73958, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73958,FALSE,0,FALSE,0,TRUE,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""","""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word."
2018,48022812,,1,,"[{'score': 0.512128, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.692529, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.512128,FALSE,0,FALSE,0,FALSE,0,TRUE,0.692529,FALSE,0,FALSE,0,FALSE,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""",Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work.
2019,48022812,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""",I try to get the actual 'word' by creating an array of keys in AfinnKeys.
2020,48022812,,3,,"[{'score': 0.961411, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961411,TRUE,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""",But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out.
2021,48022812,,4,,"[{'score': 0.563761, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.563761,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,FALSE,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""","Have been stuck on this for some time now."""
2022,55619304,,0,,"[{'score': 0.913895, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.913895,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","""I've been using Google Cloud Vision to identify faces."
2023,55619304,,1,,"[{'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","So far, so good, but I've noticed that face landmarks are returned as 3-axis positions."
2024,55619304,,2,,"[{'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning."
2025,55619304,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","They are definitely non-zero, so they mean something."
2026,55619304,,4,,"[{'score': 0.562273, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.591808, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.591808,FALSE,0,TRUE,0.562273,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions."
2027,55619304,,5,,"[{'score': 0.904882, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904882,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",The returned Z values seems to scale (approximately) relative to the scale of the image.
2028,55619304,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere."
2029,55619304,,7,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,FALSE,0,TRUE,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""","But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point)."""
2030,46838135,,0,,"[{'score': 0.550349, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.550349,FALSE,0,FALSE,0,TRUE,"""When I load my application I get this:I am trying to follow this:I have run the command:Then on the Client Libary it is saying I have to set up a Client Library? I have done this with all he correct things then it says to-:And the execute this:Where I am stuck is, where do I execute this, how do I set the environment variable?""","""When I load my application I get this:I am trying to follow this:I have run the command:Then on the Client Libary it is saying I have to set up a Client Library?"
2031,46838135,,1,,"[{'score': 0.598779, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.598779,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""When I load my application I get this:I am trying to follow this:I have run the command:Then on the Client Libary it is saying I have to set up a Client Library? I have done this with all he correct things then it says to-:And the execute this:Where I am stuck is, where do I execute this, how do I set the environment variable?""","I have done this with all he correct things then it says to-:And the execute this:Where I am stuck is, where do I execute this, how do I set the environment variable?"""
2032,42850135,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""","""I have a list of external URLs (.jpg or .png"
2033,42850135,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",images) and want to send those  as requests to the Google Cloud Vision API for label detection.
2034,42850135,,2,,"[{'score': 0.653034, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.767592, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.653034,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767592,FALSE,0,FALSE,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",I want the image with the highest confidence for a particular label(s) returned first.
2035,42850135,,3,,"[{'score': 0.796521, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.796521,FALSE,0,FALSE,0,TRUE,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""","Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'."""
2036,55708104,,0,,"[{'score': 0.551377, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.551377,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""","""I am trying to dockerize 4 services and I have a problem with one of the services."
2037,55708104,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""","Particularly, this service is implemented is spring boot service and uses google vision API."
2038,55708104,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""","When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used."
2039,55708104,,3,,"[{'score': 0.593224, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.694268, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.593224,FALSE,0,FALSE,0,TRUE,0.694268,FALSE,0,FALSE,0,FALSE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile
2040,55708104,,4,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter
2041,55708104,,5,,"[{'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.804675,TRUE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc.
2042,55708104,,6,,"[{'score': 0.93742, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.93742,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""","Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)"""
2043,40808590,,0,,"[{'score': 0.716569, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716569,FALSE,0,FALSE,0,TRUE,"""I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:However, I get the error message below. Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here? I was able to get the Google Vision API to work locally using the same Service Account.""","""I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:However, I get the error message below."
2044,40808590,,1,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:However, I get the error message below. Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here? I was able to get the Google Vision API to work locally using the same Service Account.""",Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here?
2045,40808590,,2,,"[{'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,FALSE,0,TRUE,"""I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:However, I get the error message below. Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here? I was able to get the Google Vision API to work locally using the same Service Account.""","I was able to get the Google Vision API to work locally using the same Service Account."""
2046,51500118,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""","""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths"
2047,51500118,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",+= %W(#{config.root}/lib) in application.rbadd
2048,51500118,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",file ocr_google_test.rb in libin file ocr_google_test.rb
2049,51500118,,3,,"[{'score': 0.767683, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767683,FALSE,0,FALSE,0,TRUE,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",": class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day"""
2050,52579907,,0,,"[{'score': 0.519539, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.758932, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.519539,FALSE,0,FALSE,0,TRUE,0.758932,FALSE,0,FALSE,0,FALSE,"""I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in,,andhierarchy.However, when convertingtotext andtotext, I need to understand the DetectedBreak property.I went through.But I did not understand few of the them.Can somebody explain what do the following Breaks mean? I only understoodand.EOL_SURE_SPACEHYPHENLINE_BREAKSPACESURE_SPACEUNKNOWNCan they be replaced by either a newline char or space ?""","""I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in,,andhierarchy.However, when convertingtotext andtotext, I need to understand the DetectedBreak property.I went through.But I did not understand few of the them.Can somebody explain what do the following Breaks mean?"
2051,52579907,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,TRUE,0.681699,TRUE,"""I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in,,andhierarchy.However, when convertingtotext andtotext, I need to understand the DetectedBreak property.I went through.But I did not understand few of the them.Can somebody explain what do the following Breaks mean? I only understoodand.EOL_SURE_SPACEHYPHENLINE_BREAKSPACESURE_SPACEUNKNOWNCan they be replaced by either a newline char or space ?""","I only understoodand.EOL_SURE_SPACEHYPHENLINE_BREAKSPACESURE_SPACEUNKNOWNCan they be replaced by either a newline char or space ?"""
2052,45176829,,0,,"[{'score': 0.594723, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.594723,FALSE,0,FALSE,0,TRUE,"""Since google vision has someon input image size, I want to first resize input image and then use thefunction.Here's theirthey useto open the image file. I wonder in this way, how to resize the imagein memoryand then call?""","""Since google vision has someon input image size, I want to first resize input image and then use thefunction.Here's theirthey useto open the image file."
2053,45176829,,1,,"[{'score': 0.767076, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767076,FALSE,0,TRUE,0.716301,TRUE,"""Since google vision has someon input image size, I want to first resize input image and then use thefunction.Here's theirthey useto open the image file. I wonder in this way, how to resize the imagein memoryand then call?""","I wonder in this way, how to resize the imagein memoryand then call?"""
2054,56301560,,0,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""","""I have been using Google Vision API to read text off several hundred thousand images."
2055,56301560,,1,,"[{'score': 0.972742, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.972742,TRUE,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""","Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents."
2056,56301560,,2,,"[{'score': 0.537848, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.537848,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""","I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable"""
2057,50219443,,0,,"[{'score': 0.635961, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635961,FALSE,0,FALSE,0,TRUE,"""I want to extract emails from a text which I get from the business cards using Google Vision API. How can I do that using Natural Language API?(I'm using Python)""","""I want to extract emails from a text which I get from the business cards using Google Vision API."
2058,50219443,,1,,"[{'score': 0.859009, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.859009,FALSE,0,FALSE,0,TRUE,"""I want to extract emails from a text which I get from the business cards using Google Vision API. How can I do that using Natural Language API?(I'm using Python)""","How can I do that using Natural Language API?(I'm using Python)"""
2059,52549743,,0,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""","""I am using google vision api for face detection in my app."
2060,52549743,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.515917, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,TRUE,0.515917,FALSE,0,TRUE,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""",its working fine but in my case i need to deal with only real human faces.
2061,52549743,,2,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""",but my app is considering there faces in photo as a face.
2062,52549743,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""","but i want to detect which is photo and which is live image.below is the class of face graphicsany help?"""
2063,54989726,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.""","""Is it possible to create your own bucket with images where you can add extra information for each image."
2064,54989726,,1,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.514704, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.514704,FALSE,0,TRUE,0.615352,TRUE,"""Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.""","Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text."
2065,54989726,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.""",For this reason I would add these type of images to a bucket and manually add more information.
2066,54989726,,3,,"[{'score': 0.727113, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727113,FALSE,0,FALSE,0,TRUE,"""Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.""","The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image."""
2067,51863232,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""","""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API."
2068,51863232,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""",I have another indexwith documents containing afield (i.e.
2069,51863232,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""","""John Smith"")."
2070,51863232,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""",I'd like to run a query to return the top 5documents matched onto thefield of a givendocument.
2071,51863232,,4,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""","Can anyone help me out with this or point me in the right direction?"""
2072,53341184,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""","""I need a small help."
2073,53341184,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""","I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic."
2074,53341184,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""","The camera is laid on SurfaceView and text, that is found on the surfaceView is captured."
2075,53341184,,3,,"[{'score': 0.778549, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.778549,FALSE,0,FALSE,0,TRUE,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""","I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me."
2076,53341184,,4,,"[{'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,TRUE,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""","To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only"""""
2077,51880350,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""","""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete."
2078,51880350,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""",The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.
2079,51880350,,2,,"[{'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,TRUE,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""","I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)"""
2080,54008514,,0,,"[{'score': 0.850468, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.554178, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.850468,FALSE,0,TRUE,0.554178,TRUE,"""I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image.I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data. The field separator(|) is being read as ISo,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image havingcomb fields?Or Is there an option to blur or remove the pipes in between the letters before reading the text?""","""I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image.I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data."
2081,54008514,,1,,"[{'score': 0.795844, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.795844,TRUE,"""I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image.I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data. The field separator(|) is being read as ISo,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image havingcomb fields?Or Is there an option to blur or remove the pipes in between the letters before reading the text?""","The field separator(|) is being read as ISo,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image havingcomb fields?Or Is there an option to blur or remove the pipes in between the letters before reading the text?"""
2082,55744464,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects? Like Coca Cola brands or what ever you would like to detect?""","""Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects?"
2083,55744464,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects? Like Coca Cola brands or what ever you would like to detect?""","Like Coca Cola brands or what ever you would like to detect?"""
2084,54557026,,0,,"[{'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""","""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI."
2085,54557026,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""",But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI).
2086,54557026,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""",How i can use ML Kit with NNAPI?
2087,54557026,,3,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.822231,TRUE,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""","I am doing something wrong?Link to documentation of mlkit:"""
2088,56250899,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""","""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff)."
2089,56250899,,1,,"[{'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,FALSE,0,TRUE,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""",They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them.
2090,56250899,,2,,"[{'score': 0.686147, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.686147,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""","It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?"""
2091,41434746,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""","""I have a webapp where users are authenticated anonymously with Firebase Auth."
2092,41434746,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""","I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket."
2093,41434746,,2,,"[{'score': 0.516022, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.516022,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""","When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works."
2094,41434746,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""",But this is user data and can't be public.
2095,41434746,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""","How can I solve this?My code for calling the vision api:My code for uploading images to storage:"""
2096,54855549,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to create an application that read an image containing a simple Javascript code -- ex: a-- and execute this code afterwards.For that, I am trying to use Google Vision API. My main problem so far is that some special characters (,,) are not being recognized correctly.Some examples:is converted tois converted toI saw this, and it seems that I can insert some special chars to be recognized, but I do not know where.Is there a way to improve the recognition for this context?""","""I am trying to create an application that read an image containing a simple Javascript code -- ex: a-- and execute this code afterwards.For that, I am trying to use Google Vision API."
2097,54855549,,1,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.661502, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.661502,FALSE,0,TRUE,0.822231,TRUE,"""I am trying to create an application that read an image containing a simple Javascript code -- ex: a-- and execute this code afterwards.For that, I am trying to use Google Vision API. My main problem so far is that some special characters (,,) are not being recognized correctly.Some examples:is converted tois converted toI saw this, and it seems that I can insert some special chars to be recognized, but I do not know where.Is there a way to improve the recognition for this context?""","My main problem so far is that some special characters (,,) are not being recognized correctly.Some examples:is converted tois converted toI saw this, and it seems that I can insert some special chars to be recognized, but I do not know where.Is there a way to improve the recognition for this context?"""
2098,38967064,,0,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you""","""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?"
2099,38967064,,1,,"[{'score': 0.58289, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58289,FALSE,0,FALSE,0,TRUE,"""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you""","I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you"""
2100,53882629,,0,,"[{'score': 0.609697, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.609697,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""","""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this?"
2101,53882629,,1,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""","Thank you."""
2102,48834479,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,TRUE,0.681699,TRUE,"""Does Google Cloud Video intelligence work with embedded video content from Vimeo or YouTube. Will it be able to create tags, see faces etc... since the content is not directly uploaded?""","""Does Google Cloud Video intelligence work with embedded video content from Vimeo or YouTube."
2103,48834479,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Does Google Cloud Video intelligence work with embedded video content from Vimeo or YouTube. Will it be able to create tags, see faces etc... since the content is not directly uploaded?""","Will it be able to create tags, see faces etc... since the content is not directly uploaded?"""
2104,54418688,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to specify multiple modules to install by version number.If this is my:How can I adjust the contents of the mods list to specify the version number?Edit:The utilities.py file begins:The error I get after applying the advice from @jpeg, is as follows:My pip freeze is:The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear.""","""I would like to specify multiple modules to install by version number.If this is my:How can I adjust the contents of the mods list to specify the version number?Edit:The utilities.py"
2105,54418688,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to specify multiple modules to install by version number.If this is my:How can I adjust the contents of the mods list to specify the version number?Edit:The utilities.py file begins:The error I get after applying the advice from @jpeg, is as follows:My pip freeze is:The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear.""","file begins:The error I get after applying the advice from @jpeg, is as follows:My pip freeze is:The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear."""
2106,37796580,,0,,"[{'score': 0.587649, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.587649,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,FALSE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""","""I am trying to useMicrosoft face APIsoftware in."
2107,37796580,,1,,"[{'score': 0.641091, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.641091,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,FALSE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key.
2108,37796580,,2,,"[{'score': 0.512232, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,TRUE,0.512232,TRUE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""","This key needs to be either passed through a query string parameter, or specified in the request header."
2109,37796580,,3,,"[{'score': 0.82831, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.82831,FALSE,0,FALSE,0,TRUE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""","To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass."
2110,37796580,,4,,"[{'score': 0.861593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.861593,FALSE,0,FALSE,0,TRUE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",For example:The subscription key can be obtained from the Marketplace page of your Azure management portal.
2111,37796580,,5,,"[{'score': 0.673733, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.867767, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.673733,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867767,FALSE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",See Subscriptions.Now I am confused at how I would go about this.
2112,37796580,,6,,"[{'score': 0.694842, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.694842,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,FALSE,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""","So first I tried using the query string way of doing it .But after i do this i get an error saying :"""
2113,56346851,,0,,"[{'score': 0.615362, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.615362,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm getting an error while sending Image for recognition to AWS Rekognition. This is the code which I use:And this is an error:Exception looks likeis empty, I have debugged and checked that ByteBuffer is valid and is not empty""","""I'm getting an error while sending Image for recognition to AWS Rekognition."
2114,56346851,,1,,"[{'score': 0.666368, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.666368,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm getting an error while sending Image for recognition to AWS Rekognition. This is the code which I use:And this is an error:Exception looks likeis empty, I have debugged and checked that ByteBuffer is valid and is not empty""","This is the code which I use:And this is an error:Exception looks likeis empty, I have debugged and checked that ByteBuffer is valid and is not empty"""
2115,52512050,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to detect text in PDF and TIFF files with Google Cloud Vision, but from the looks of it that can only be done if you first store the file to the Google Cloud Storage. Can this also be done without storing it in the cloud?""","""I want to detect text in PDF and TIFF files with Google Cloud Vision, but from the looks of it that can only be done if you first store the file to the Google Cloud Storage."
2116,52512050,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to detect text in PDF and TIFF files with Google Cloud Vision, but from the looks of it that can only be done if you first store the file to the Google Cloud Storage. Can this also be done without storing it in the cloud?""","Can this also be done without storing it in the cloud?"""
2117,54065638,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I have a lot of documents in offline books. It's in table format and I don't want to manually input these tables into a dynamoDB table. Can I use AWS rekognition to help me here OR I should look at some other service ?""","""I have a lot of documents in offline books."
2118,54065638,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a lot of documents in offline books. It's in table format and I don't want to manually input these tables into a dynamoDB table. Can I use AWS rekognition to help me here OR I should look at some other service ?""",It's in table format and I don't want to manually input these tables into a dynamoDB table.
2119,54065638,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,TRUE,0.88939,TRUE,"""I have a lot of documents in offline books. It's in table format and I don't want to manually input these tables into a dynamoDB table. Can I use AWS rekognition to help me here OR I should look at some other service ?""","Can I use AWS rekognition to help me here OR I should look at some other service ?"""
2120,45134020,,0,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.72085, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.72085,FALSE,0,TRUE,0.946222,TRUE,"""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However, I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient.""","""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However,"
2121,45134020,,1,,"[{'score': 0.633859, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.786013, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786013,FALSE,0,TRUE,0.633859,TRUE,"""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However, I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient.""","I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient."""
2122,51894464,,0,,"[{'score': 0.607238, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.607238,FALSE,0,FALSE,0,TRUE,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""","""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision."
2123,51894464,,1,,"[{'score': 0.715667, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.715667,FALSE,0,FALSE,0,TRUE,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""","I can import google cloud, but it throws an error below.What module should I import with anaconda?"
2124,51894464,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.990161, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.990161,TRUE,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""","(I've already imported,,,,,,,,)Could anyone solve this?"
2125,51894464,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""","Thanks in advance."""
2126,49014560,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""","""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time."
2127,49014560,,1,,"[{'score': 0.565986, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.677676, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.565986,FALSE,0,FALSE,0,TRUE,0.677676,FALSE,0,FALSE,0,FALSE,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""","If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S."
2128,49014560,,2,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""",I tried using different images and the behaviors is the same.
2129,49014560,,3,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.91961,TRUE,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""","I would appreciate any help.The application:"""
2130,51142340,,0,,"[{'score': 0.626922, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.626922,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using the Java Google Vision API to run through a batch of images to find the Web properties and that's been working fine the last couple months.I'm now getting a channel closed and ClosedChannelException error on the request. I created a new project with just theto test and I run into the same issue.I updated my Maven dependency version to the most current and still get the issue on the original application and the new test project.It errors on the request here:This is the error message I'm getting:""","""I'm using the Java Google Vision API to run through a batch of images to find the Web properties and that's been working fine the last couple months.I'm now getting a channel closed and ClosedChannelException error on the request."
2131,51142340,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Java Google Vision API to run through a batch of images to find the Web properties and that's been working fine the last couple months.I'm now getting a channel closed and ClosedChannelException error on the request. I created a new project with just theto test and I run into the same issue.I updated my Maven dependency version to the most current and still get the issue on the original application and the new test project.It errors on the request here:This is the error message I'm getting:""","I created a new project with just theto test and I run into the same issue.I updated my Maven dependency version to the most current and still get the issue on the original application and the new test project.It errors on the request here:This is the error message I'm getting:"""
2132,56379854,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am querying the Azure Custom Vision V3.0 Training API (see) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:u'precision': 0.9859485, u'precisionStdDeviation': 0.0, u'recall': 0.3752228, u'recallStdDeviation': 0.0}The precision and recall uncertainties,andrespectively, always seem to be 0.0. Is this user error and if not are there any plans to activate these stats?""","""I am querying the Azure Custom Vision V3.0 Training API (see) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:u'precision': 0.9859485, u'precisionStdDeviation': 0.0, u'recall': 0.3752228, u'recallStdDeviation': 0.0}The precision and recall uncertainties,andrespectively, always seem to be 0.0."
2133,56379854,,1,,"[{'score': 0.534158, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.534158,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.647986,FALSE,"""I am querying the Azure Custom Vision V3.0 Training API (see) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:u'precision': 0.9859485, u'precisionStdDeviation': 0.0, u'recall': 0.3752228, u'recallStdDeviation': 0.0}The precision and recall uncertainties,andrespectively, always seem to be 0.0. Is this user error and if not are there any plans to activate these stats?""","Is this user error and if not are there any plans to activate these stats?"""
2134,35883234,,0,,"[{'score': 0.870269, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.870269,FALSE,0,FALSE,0,TRUE,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""","""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results."
2135,35883234,,1,,"[{'score': 0.548193, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.754738, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.829951, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.548193,FALSE,0,FALSE,0,TRUE,0.754738,FALSE,0,TRUE,0.829951,FALSE,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""","The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):"""
2136,48075707,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""","""I used google vision online into get the detected text for an image file."
2137,48075707,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""",I received 5 results.Then I used the api to get the detected text for the same image file.
2138,48075707,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""","But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?"""
2139,50765893,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""","""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision."
2140,50765893,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""",The image is not accepted.
2141,50765893,,2,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.856622,TRUE,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""",Does someone knows if there is a new incompatibilty.
2142,50765893,,3,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,TRUE,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""",I'm pretty sure it was working 6 months ago.Any ideas?
2143,50765893,,4,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,TRUE,0.91961,TRUE,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""","or people that experiences similar problems ?Regards"""
2144,52322574,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""","""I have a whole bunch of aerial photos."
2145,52322574,,1,,"[{'score': 0.580791, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.580791,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.804675,FALSE,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""","For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other."
2146,52322574,,2,,"[{'score': 0.539, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.539,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""","Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit."
2147,52322574,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""",I think this is a bug on Rekognition.
2148,52322574,,4,,"[{'score': 0.584195, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.584195,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.946222,FALSE,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""","Has anyone else run into this?"""
2149,55323321,,0,,"[{'score': 0.651975, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.651975,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model""","""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'."
2150,55323321,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model""","this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model"""
2151,46332861,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js),"
2152,46332861,,1,,"[{'score': 0.637878, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.637878,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",but I keep running into the same error:I tried to dive into that client.js
2153,46332861,,2,,"[{'score': 0.719195, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.602068, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.719195,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.602068,FALSE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","file listed in the error, but it was not very illuminating."
2154,46332861,,3,,"[{'score': 0.884922, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.884922,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,FALSE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at).
2155,46332861,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file."
2156,46332861,,5,,"[{'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,TRUE,0.825035,FALSE,0,TRUE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue."
2157,46332861,,6,,"[{'score': 0.761487, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.761487,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.511119,FALSE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up."
2158,46332861,,7,,"[{'score': 0.544055, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.779077, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.544055,TRUE,0.779077,FALSE,0,TRUE,0.511119,FALSE,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""","Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?"""
2159,41228649,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""","""I'm pulling jpg frames out of a mjpg stream."
2160,41228649,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""","These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool)."
2161,41228649,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""","When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting."
2162,41228649,,3,,"[{'score': 0.952181, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.952181,TRUE,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""","Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes."""
2163,54231123,,0,,"[{'score': 0.568567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568567,FALSE,0,FALSE,0,TRUE,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""","""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR."
2164,54231123,,1,,"[{'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,FALSE,0,TRUE,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""",This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters.
2165,54231123,,2,,"[{'score': 0.840583, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840583,FALSE,0,TRUE,0.88939,TRUE,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""",It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand.
2166,54231123,,3,,"[{'score': 0.951204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.96417, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.951204,FALSE,0,TRUE,0.96417,TRUE,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""","Does anybody know about some way of establishing a whitelist of desired characters?"""
2167,48056251,,0,,"[{'score': 0.703641, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703641,FALSE,0,FALSE,0,TRUE,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""","""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only."
2168,48056251,,1,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""","So I want to use it.Any help would be highly appreciated."""
2169,54118524,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm making a request with the google vision api that appears to have worked, I get an operation number back. The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.This is the script I ranThis returns backand when I do a lookup on the operation I get thisHowever the output folder is completely empty and I am not sure what to make of the state created.""","""I'm making a request with the google vision api that appears to have worked, I get an operation number back."
2170,54118524,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm making a request with the google vision api that appears to have worked, I get an operation number back. The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.This is the script I ranThis returns backand when I do a lookup on the operation I get thisHowever the output folder is completely empty and I am not sure what to make of the state created.""","The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.This is the script I ranThis returns backand when I do a lookup on the operation I get thisHowever the output folder is completely empty and I am not sure what to make of the state created."""
2171,49120897,,0,,"[{'score': 0.598448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.598448,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I"
2172,49120897,,1,,"[{'score': 0.742315, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.742315,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e"
2173,49120897,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","clothes,animals)?"
2174,49120897,,3,,"[{'score': 0.885271, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.885271,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output."
2175,49120897,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images?"
2176,49120897,,5,,"[{'score': 0.575269, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.849626, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849626,TRUE,0.575269,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image.
2177,49120897,,6,,"[{'score': 0.512589, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512589,FALSE,0,FALSE,0,TRUE,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""","But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)"""
2178,53955753,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""","""I've installed both react-native-firebase and react-native-camera."
2179,53955753,,1,,"[{'score': 0.687054, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.542121, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.687054,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.542121,FALSE,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""","The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror."
2180,53955753,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Bumping up to 17.0.2
2181,53955753,,3,,"[{'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,TRUE,0.933436,TRUE,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",would cause a version conflict from.Anyone using both react-native-firebase and react-native camera?
2182,53955753,,4,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""","Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:"""
2183,49380672,,0,,"[{'score': 0.770155, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.770155,FALSE,0,FALSE,0,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""","""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application."
2184,49380672,,1,,"[{'score': 0.746119, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.746119,FALSE,0,FALSE,0,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes.
2185,49380672,,2,,"[{'score': 0.523822, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.523822,FALSE,0,FALSE,0,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""","I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code)."
2186,49380672,,3,,"[{'score': 0.611956, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.611956,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional.
2187,49380672,,4,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods.
2188,49380672,,5,,"[{'score': 0.631447, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.627413, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.631447,FALSE,0,FALSE,0,TRUE,0.627413,FALSE,0,FALSE,0,FALSE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""","I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects."
2189,49380672,,6,,"[{'score': 0.599051, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599051,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",But I am not sure this is the case.
2190,49380672,,7,,"[{'score': 0.691568, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.691568,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,TRUE,0.525007,FALSE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""","I can't for the life of me figure out why this is happening, it seems to work everywhere else."
2191,49380672,,8,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.93534, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93534,FALSE,0,TRUE,0.856622,TRUE,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""","I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv"""
2192,47415721,,0,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But, when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?""","""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But,"
2193,47415721,,1,,"[{'score': 0.816951, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.816951,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But, when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?""","when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?"""
2194,53029413,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm reading this article :and reproduced the sample application. It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx by mine.We get the exception.Here all my project's configuration:""","""I'm reading this article :and reproduced the sample application."
2195,53029413,,1,,"[{'score': 0.545755, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.545755,FALSE,0,FALSE,0,TRUE,"""I'm reading this article :and reproduced the sample application. It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx by mine.We get the exception.Here all my project's configuration:""","It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx"
2196,53029413,,2,,"[{'score': 0.825035, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825035,FALSE,0,TRUE,"""I'm reading this article :and reproduced the sample application. It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx by mine.We get the exception.Here all my project's configuration:""","by mine.We get the exception.Here all my project's configuration:"""
2197,44635222,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""","""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices."
2198,44635222,,1,,"[{'score': 0.64763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.64763,FALSE,0,FALSE,0,TRUE,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""","Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same?"
2199,44635222,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""","and not as in documentation?"""
2200,36452038,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags.  But I can't get the hashtag symbol recognized.  Any ideas?""","""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags."
2201,36452038,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags.  But I can't get the hashtag symbol recognized.  Any ideas?""",But I can't get the hashtag symbol recognized.
2202,36452038,,2,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.998976,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags.  But I can't get the hashtag symbol recognized.  Any ideas?""","Any ideas?"""
2203,43383886,,0,,"[{'score': 0.837617, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.837617,FALSE,0,FALSE,0,TRUE,"""in google vision api label detection, can't know where object located ? any options or idea ??I have tried in sample, and then response json is does not include object position!""","""in google vision api label detection, can't know where object located ?"
2204,43383886,,1,,"[{'score': 0.884998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.913819, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.884998,FALSE,0,TRUE,0.913819,TRUE,"""in google vision api label detection, can't know where object located ? any options or idea ??I have tried in sample, and then response json is does not include object position!""","any options or idea ??I have tried in sample, and then response json is does not include object position!"""
2205,55644244,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Please Note that I am a NEWBIE. Under Learning ProcessRight now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name. Now i want to replace the image with the image i uploaded on ImageView.Any help would work.""","""Please Note that I am a NEWBIE."
2206,55644244,,1,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""Please Note that I am a NEWBIE. Under Learning ProcessRight now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name. Now i want to replace the image with the image i uploaded on ImageView.Any help would work.""",Under Learning ProcessRight now what this code is doing?
2207,55644244,,2,,"[{'score': 0.700591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700591,FALSE,0,FALSE,0,TRUE,"""Please Note that I am a NEWBIE. Under Learning ProcessRight now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name. Now i want to replace the image with the image i uploaded on ImageView.Any help would work.""","It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name."
2208,55644244,,3,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""Please Note that I am a NEWBIE. Under Learning ProcessRight now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name. Now i want to replace the image with the image i uploaded on ImageView.Any help would work.""","Now i want to replace the image with the image i uploaded on ImageView.Any help would work."""
2209,46236523,,0,,"[{'score': 0.66863, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.66863,FALSE,0,FALSE,0,TRUE,"""i am using Amazon Rekognition when i upload an image to my s3 bucket , the api resonse i get isis there any api to search for similar product (eg. search: 'striped blue t shirts' when i upload a blue striped tshirt) among images in my bucket.""","""i am using Amazon Rekognition when i upload an image to my s3 bucket , the api resonse i get isis there any api to search for similar product (eg."
2210,46236523,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am using Amazon Rekognition when i upload an image to my s3 bucket , the api resonse i get isis there any api to search for similar product (eg. search: 'striped blue t shirts' when i upload a blue striped tshirt) among images in my bucket.""","search: 'striped blue t shirts' when i upload a blue striped tshirt) among images in my bucket."""
2211,51934306,,0,,"[{'score': 0.883056, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.660391, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.883056,FALSE,0,TRUE,0.660391,TRUE,"""We are just thinking whether to validate some input for google vision (OCR).We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.I can't find it the documentation. Does anyone know?""","""We are just thinking whether to validate some input for google vision (OCR).We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.I can't find it the documentation."
2212,51934306,,1,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.994446,TRUE,"""We are just thinking whether to validate some input for google vision (OCR).We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.I can't find it the documentation. Does anyone know?""","Does anyone know?"""
2213,50767594,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""","""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly."
2214,50767594,,1,,"[{'score': 0.870269, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.870269,FALSE,0,FALSE,0,TRUE,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""",However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this.
2215,50767594,,2,,"[{'score': 0.999857, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999857,TRUE,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""",Any suggestions?
2216,50767594,,3,,"[{'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839577,TRUE,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""",I was hoping to have the API response give me a json dict or json dict of dicts etc...
2217,50767594,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""","All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance."""
2218,42702426,,0,,"[{'score': 0.724608, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.524847, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.724608,FALSE,0,FALSE,0,TRUE,0.524847,FALSE,0,FALSE,0,FALSE,"""I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.I've been experimenting with the API and using PHP with no success.I've been looking through all the tutorials and always get stuck on some point.This is the closest I've came so far in phpBut then I get this error.I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.Does anyone have any experience with the google vision API that can help me out? Preferably with the android part, help me get on my way or help me get started?Thanks ahead.""","""I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.I've been experimenting with the API and using PHP with no success.I've been looking through all the tutorials and always get stuck on some point.This is the closest I've came so far in phpBut then I get this error.I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.Does anyone have any experience with the google vision API that can help me out?"
2219,42702426,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.564476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.564476,FALSE,0,TRUE,0.5538,TRUE,"""I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.I've been experimenting with the API and using PHP with no success.I've been looking through all the tutorials and always get stuck on some point.This is the closest I've came so far in phpBut then I get this error.I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.Does anyone have any experience with the google vision API that can help me out? Preferably with the android part, help me get on my way or help me get started?Thanks ahead.""","Preferably with the android part, help me get on my way or help me get started?Thanks ahead."""
2220,49335721,,0,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""","""I am making an app using Google Vision API for face detection."
2221,49335721,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""","As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers)"
2222,49335721,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""",and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior.
2223,49335721,,3,,"[{'score': 0.774376, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.774376,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""",Simply havingin the build.gradle
2224,49335721,,4,,"[{'score': 0.910117, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.910117,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""","file causes this behavior, even if the library is not referenced anywhere in the actual code."
2225,49335721,,5,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""",I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database.
2226,49335721,,6,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""","If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly."
2227,49335721,,7,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""",The problem is that this app is for a school project and I am required to use SQLite.
2228,49335721,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""","This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter."""
2229,44629662,,0,,"[{'score': 0.71364, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.71364,FALSE,0,FALSE,0,TRUE,"""I'm trying to do face detection in a video using Google Vision API. I'm using the following code:But I'm getting the error:The ""frames"" are getting read as numpy array. But don't know how to bypass them.Can anyone please help me?""","""I'm trying to do face detection in a video using Google Vision API."
2230,44629662,,1,,"[{'score': 0.636495, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.636495,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,FALSE,"""I'm trying to do face detection in a video using Google Vision API. I'm using the following code:But I'm getting the error:The ""frames"" are getting read as numpy array. But don't know how to bypass them.Can anyone please help me?""","I'm using the following code:But I'm getting the error:The ""frames"" are getting read as numpy array."
2231,44629662,,2,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.75152,TRUE,"""I'm trying to do face detection in a video using Google Vision API. I'm using the following code:But I'm getting the error:The ""frames"" are getting read as numpy array. But don't know how to bypass them.Can anyone please help me?""","But don't know how to bypass them.Can anyone please help me?"""
2232,55334563,,0,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""","""I'm trying to recognize vertical text using google cloud vision."
2233,55334563,,1,,"[{'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.865159, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.865159,FALSE,0,TRUE,0.727988,TRUE,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""",Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something?
2234,55334563,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""","Thank you."""
2235,41713056,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","""I want to make live filter in camera like Snapchat app."
2236,41713056,,1,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""",This app based on.I have these following code in FaceGraphic.java:I
2237,41713056,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them."
2238,41713056,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","I create button ""Take a picture"" with idcapture.This is my main.xml:How"
2239,41713056,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""",can I get bitmap that I draw in FaceGraphic?
2240,41713056,,5,,"[{'score': 0.767076, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767076,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","If I take a photo, I only get the default photo without bitmap."
2241,41713056,,6,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""",I want to take photo with user faces and bitmap and save into gallery.
2242,41713056,,7,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.974578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974578,FALSE,0,TRUE,0.91961,TRUE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","Sorry, I hope you understand my question."
2243,41713056,,8,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""","Thank you."""
2244,43041575,,0,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this?Thanks to all""","""I was wondering how the google cloud vision works behind the scenes."
2245,43041575,,1,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this?Thanks to all""",What kind of algorithms are used for processing the images?
2246,43041575,,2,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this?Thanks to all""","Is there some texts explaining this?Thanks to all"""
2247,37264402,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""","""I have added a credit card and associated the billing account with my project."
2248,37264402,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""","However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled."
2249,37264402,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""","Please enable it."""
2250,37264402,,3,,"[{'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,TRUE,0.839577,TRUE,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""","Does anyone know if there are any tricks to get the project to recognize that billing has been added?"""
2251,45695542,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""","""I'm having a problem with base64 encoded images sent to Google Cloud Vision."
2252,45695542,,1,,"[{'score': 0.769391, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769391,FALSE,0,FALSE,0,TRUE,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""","Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?"""
2253,54586779,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am creating an API which will take a text based image(Business card) as an input and return image with improved quality by remove noise and applying some image filter. For this I want to use AWS Rekognition or any othre php library.I have read AWS Rekognition documentation but I have not found any api to just improve image quality.There are various php Libraries are available but I can not find a proper solution for this problem.Please let me know a solution where a text based image quality can be improved either using php library or AWS Rekognition .Following is sample image of Business Card of which we want to improve.""","""I am creating an API which will take a text based image(Business card) as an input and return image with improved quality by remove noise and applying some image filter."
2254,54586779,,1,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.563171, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,TRUE,0.563171,TRUE,"""I am creating an API which will take a text based image(Business card) as an input and return image with improved quality by remove noise and applying some image filter. For this I want to use AWS Rekognition or any othre php library.I have read AWS Rekognition documentation but I have not found any api to just improve image quality.There are various php Libraries are available but I can not find a proper solution for this problem.Please let me know a solution where a text based image quality can be improved either using php library or AWS Rekognition .Following is sample image of Business Card of which we want to improve.""","For this I want to use AWS Rekognition or any othre php library.I have read AWS Rekognition documentation but I have not found any api to just improve image quality.There are various php Libraries are available but I can not find a proper solution for this problem.Please let me know a solution where a text based image quality can be improved either using php library or AWS Rekognition .Following is sample image of Business Card of which we want to improve."""
2255,55283215,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""","""I have a binary text image like this oneI want to perform OCR on images like these."
2256,55283215,,1,,"[{'score': 0.57933, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.57933,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,FALSE,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code.
2257,55283215,,2,,"[{'score': 0.605329, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.605329,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",Please help me out!EDIT:Thanks toF10for pointing me in the right direction.
2258,55283215,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""","This is how I got it to work with a local image."""
2259,54981232,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciatedBest regards,Daniel""","""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description."
2260,54981232,,1,,"[{'score': 0.692569, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.665015, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.692569,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.665015,FALSE,0,FALSE,"""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciatedBest regards,Daniel""","I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags."
2261,54981232,,2,,"[{'score': 0.654467, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.654467,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,FALSE,"""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciatedBest regards,Daniel""",Anyone had this problem before or do you have any tips on how to add the confidence scores?
2262,54981232,,3,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciatedBest regards,Daniel""","Help is appreciatedBest regards,Daniel"""
2263,49842534,,0,,"[{'score': 0.978086, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.978086,FALSE,0,TRUE,0.615352,TRUE,"""I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that? Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys""","""I am using the google vision library (although this might not matter for the purpose of this question)."
2264,49842534,,1,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that? Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys""","Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that?"
2265,49842534,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that? Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys""","Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys"""
2266,56031856,,0,,"[{'score': 0.855825, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855825,FALSE,0,FALSE,0,TRUE,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""","""I'm looking to build an app that detects certain objects and then overlays something using ARCore."
2267,56031856,,1,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""",Is it possible to use Google's Vision API for real-time detection of objects?
2268,56031856,,2,,"[{'score': 0.635961, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.635961,FALSE,0,TRUE,0.839577,TRUE,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""","If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?"""
2269,49187806,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""","""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service."
2270,49187806,,1,,"[{'score': 0.514715, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.514715,TRUE,0.506763,FALSE,0,FALSE,0,FALSE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result.
2271,49187806,,2,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""","I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached."
2272,49187806,,3,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously.
2273,49187806,,4,,"[{'score': 0.832004, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.832004,FALSE,0,FALSE,0,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""","The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided."
2274,49187806,,5,,"[{'score': 0.838593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.838593,FALSE,0,TRUE,0.91961,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",Any help would appreciate in this matter.
2275,49187806,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""","Thanks in advance"""
2276,52968434,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""","""I'm planning to useservice forforgot passwordin my iOS and android apps."
2277,52968434,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""","Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number."
2278,52968434,,2,,"[{'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,TRUE,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""",For this I will be asking user to take one live pic of himself/herself and check this against reference image.
2279,52968434,,3,,"[{'score': 0.510831, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.504802, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.510831,FALSE,0,FALSE,0,TRUE,0.504802,FALSE,0,FALSE,0,FALSE,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""","But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario?"
2280,52968434,,4,,"[{'score': 0.925256, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.677069, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677069,FALSE,0,TRUE,0.925256,TRUE,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""","and I just want to know whether this is suggested way to proceed or not.Please advice."""
2281,50513107,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.""","""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above."
2282,50513107,,1,,"[{'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,FALSE,0,TRUE,"""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.""","Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example."
2283,50513107,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.""","I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency."""
2284,54581027,,0,,"[{'score': 0.879891, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.879891,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.""","""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue?"
2285,54581027,,1,,"[{'score': 0.633504, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.633504,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,FALSE,"""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.""","I tried using very common images like country flags but still it gives error."""
2286,51350903,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""","""In Aws lambda function I am storing image And My image is my primary key."
2287,51350903,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""",But No Case is I can store same image in different function as well like.
2288,51350903,,2,,"[{'score': 0.864115, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.864115,FALSE,0,FALSE,0,TRUE,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""",John can be part of function1 and function2 as well.
2289,51350903,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""",So when I store in both 2nd one got remove.
2290,51350903,,4,,"[{'score': 0.583864, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.583864,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,FALSE,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""","My table structure is which I got by doingResult:And I made this by querying like this"""
2291,50958350,,0,,"[{'score': 0.698312, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.698312,FALSE,0,FALSE,0,TRUE,"""For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.How can I get the raw json? The .NET SDK does not offer any methods. I tried to serialize the face details without success.""","""For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.How can I get the raw json?"
2292,50958350,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.How can I get the raw json? The .NET SDK does not offer any methods. I tried to serialize the face details without success.""",The .NET SDK does not offer any methods.
2293,50958350,,2,,"[{'score': 0.589187, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.589187,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.How can I get the raw json? The .NET SDK does not offer any methods. I tried to serialize the face details without success.""","I tried to serialize the face details without success."""
2294,46594701,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""","""I would like  to use the Google Vision API for label detection."
2295,46594701,,1,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",But I want to decrease the labels percentages and I do not know how I can do this.
2296,46594701,,2,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.998976,TRUE,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",Could someone help me?
2297,46594701,,3,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",I am using a sample.
2298,46594701,,4,,"[{'score': 0.750688, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.750688,FALSE,0,FALSE,0,TRUE,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""","I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:"""
2299,48085989,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""","""I'm writing an android plugin for Unity that uses android face detection."
2300,48085989,,1,,"[{'score': 0.501246, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.619042, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.501246,FALSE,0,FALSE,0,TRUE,0.619042,FALSE,0,FALSE,0,FALSE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""","I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable."
2301,48085989,,2,,"[{'score': 0.695447, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.695447,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity.
2302,48085989,,3,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""","Without this variable, my plugin works."
2303,48085989,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""","With it, it crashes.I did also make a change to my build.gradle"
2304,48085989,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",file.
2305,48085989,,6,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659112,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.
2306,48085989,,7,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",I then tried creating the variable inside a function and returning it.
2307,48085989,,8,,"[{'score': 0.803247, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.803247,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""","Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie"""
2308,45024118,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""I am creating an android aar in which I am using google's vision API. To check if Play Services are available or not I have added check using.For excluding this from obfuscation I have addedI am getting this error when obfuscated:Update:When I am using playservices dependency in project where this aar is also imported, then my code is working perfectly.Is there any way, to avoid adding playservices dependency in project and just use it from aar?""","""I am creating an android aar in which I am using google's vision API."
2309,45024118,,1,,"[{'score': 0.66222, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.66222,FALSE,0,FALSE,0,TRUE,"""I am creating an android aar in which I am using google's vision API. To check if Play Services are available or not I have added check using.For excluding this from obfuscation I have addedI am getting this error when obfuscated:Update:When I am using playservices dependency in project where this aar is also imported, then my code is working perfectly.Is there any way, to avoid adding playservices dependency in project and just use it from aar?""","To check if Play Services are available or not I have added check using.For excluding this from obfuscation I have addedI am getting this error when obfuscated:Update:When I am using playservices dependency in project where this aar is also imported, then my code is working perfectly.Is there any way, to avoid adding playservices dependency in project and just use it from aar?"""
2310,51676317,,0,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,TRUE,"""Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:""","""Is it possible to send image byte through Lambda using Boto3?"
2311,51676317,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:""",The byte will be sent to Lambda function which will then forward the image to Rekognition.
2312,51676317,,2,,"[{'score': 0.749999, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.749999,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:""","I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:"""
2313,36728347,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""","""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server."
2314,36728347,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""",I just follow the instructions in this page.Until now I was able to tested with .jpg
2315,36728347,,2,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""","files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files."
2316,36728347,,3,,"[{'score': 0.72499, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.72499,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""","Is there a way to see the whole document transformed?Thanks!"""
2317,53910973,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""","""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10"
2318,53910973,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""","environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc."
2319,53910973,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""",My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When
2320,53910973,,3,,"[{'score': 0.525926, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525926,FALSE,0,TRUE,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""",I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow
2321,53910973,,4,,"[{'score': 0.839095, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.839095,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.75152,FALSE,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""","to solve this timeout error as it doesn't show any error on?"""
2322,47250652,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""","""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs."
2323,47250652,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues.
2324,47250652,,2,,"[{'score': 0.765373, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.55632, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.765373,FALSE,0,FALSE,0,FALSE,0,TRUE,0.55632,FALSE,0,FALSE,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""","But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong"""
2325,55391510,,0,,"[{'score': 0.548733, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.548733,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage.I want to make app which take photo only face but i get whole image instead of face when i save into my storage file . so how to crop  face and save into storage with the help google vision face detection API.**so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage""","""so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage.I want to make app which take photo only face but i get whole image instead of face when i save into my storage file ."
2326,55391510,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage.I want to make app which take photo only face but i get whole image instead of face when i save into my storage file . so how to crop  face and save into storage with the help google vision face detection API.**so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage""","so how to crop  face and save into storage with the help google vision face detection API.**so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage"""
2327,53515301,,0,,"[{'score': 0.87867, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87867,FALSE,0,FALSE,0,TRUE,"""I'm using android studio and cloud vision in order to detect faces features in a picture. When compiling, I get this error (About cloud vision V 1.53):My Gradles dependencies are:""","""I'm using android studio and cloud vision in order to detect faces features in a picture."
2328,53515301,,1,,"[{'score': 0.673576, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.673576,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using android studio and cloud vision in order to detect faces features in a picture. When compiling, I get this error (About cloud vision V 1.53):My Gradles dependencies are:""","When compiling, I get this error (About cloud vision V 1.53):My Gradles dependencies are:"""
2329,44594617,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""","""The IBM Visual Recognition classifier is simple to use and works well."
2330,44594617,,1,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""","However, custom classifier creation is expensive ($0.10/image) and time-consuming."
2331,44594617,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,TRUE,0.681699,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",Accidental deleting of a custom classifier puts any workflow using that classifier at risk.
2332,44594617,,3,,"[{'score': 0.926877, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926877,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""","There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier."
2333,44594617,,4,,"[{'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition?
2334,44594617,,5,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""","This questionand I am hoping someone from IBM can provide guidance here.Thank you!"""
2335,51273104,,0,,"[{'score': 0.580808, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.665015, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.580808,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.665015,FALSE,0,FALSE,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""","""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same."
2336,51273104,,1,,"[{'score': 0.988176, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.988176,FALSE,0,FALSE,0,TRUE,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""",This is also for example the case.
2337,51273104,,2,,"[{'score': 0.717081, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.961593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.645985, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.717081,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961593,TRUE,0.645985,FALSE,0,FALSE,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""","According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence""."
2338,51273104,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""","Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same?"
2339,51273104,,4,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""","What does that mean?"""
2340,40714481,,0,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""","""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg"
2341,40714481,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""","file in the local system (say, stored in a directoryD:\Image\abc.jpg)The"
2342,40714481,,2,,"[{'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""","example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address."
2343,40714481,,3,,"[{'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.845297,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",I have tried to do the following:But it does not seem to work.
2344,40714481,,4,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.822231,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",It seems that there is a method for Java (using).
2345,40714481,,5,,"[{'score': 0.793846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.793846,FALSE,0,TRUE,0.856622,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",I'm wondering if there is a method for Python.
2346,40714481,,6,,"[{'score': 0.600566, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.600566,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",I'm new to coding.
2347,40714481,,7,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.976993,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",I really hope someone can help me with this.
2348,40714481,,8,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""","I'm using Python3."""
2349,51961697,,0,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""","""I am searching for the answer on this question on the internet, but can't find it."
2350,51961697,,1,,"[{'score': 0.946327, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.946327,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""","I mean something like auto-correction, or no correction but suggestions for more obvious words."
2351,51961697,,2,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""","Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc)."
2352,51961697,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""",Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document?
2353,51961697,,4,,"[{'score': 0.932977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.932977,FALSE,0,FALSE,0,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""","For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'?"
2354,51961697,,5,,"[{'score': 0.738662, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.738662,TRUE,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""","So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?"""
2355,52928909,,0,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance""","""I am using google vision api for ocr with Java 8."
2356,52928909,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance""","It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance"""
2357,46921518,,0,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""","""I am attempting to use the google vision library in java."
2358,46921518,,1,,"[{'score': 0.891685, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.891685,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""",The steps specify that I need to setup my auth credentials in order to start using thelibrary .
2359,46921518,,2,,"[{'score': 0.682315, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682315,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""",I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties
2360,46921518,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""","file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:"""
2361,51991109,,0,,"[{'score': 0.567034, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.567034,FALSE,0,TRUE,"""In the above activity, I have to scan two different barcode and assigned their values to the each. But I am not able to do it. I have tried many logical approaches but none of them worked.activity_scan_qr.xmlScanQR.java""","""In the above activity, I have to scan two different barcode and assigned their values to the each."
2362,51991109,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In the above activity, I have to scan two different barcode and assigned their values to the each. But I am not able to do it. I have tried many logical approaches but none of them worked.activity_scan_qr.xmlScanQR.java""",But I am not able to do it.
2363,51991109,,2,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""In the above activity, I have to scan two different barcode and assigned their values to the each. But I am not able to do it. I have tried many logical approaches but none of them worked.activity_scan_qr.xmlScanQR.java""","I have tried many logical approaches but none of them worked.activity_scan_qr.xmlScanQR.java"""
2364,48806569,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK""","""We are facing one issue when we try to scan bag."
2365,48806569,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK""",The main reason behind that the text color on the bag which are embroidered are almost same as bag's color.
2366,48806569,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK""",So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491).
2367,48806569,,3,,"[{'score': 0.719382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719382,FALSE,0,FALSE,0,TRUE,"""We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK""","Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK"""
2368,48140339,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""","""I am using guava 23-5 in my application and1.2.0."
2369,48140339,,1,,"[{'score': 0.526595, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.526595,FALSE,0,FALSE,0,TRUE,0.727988,FALSE,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""",This is causing a conflict in my application and throwing the below exception whenever I am trying to use.
2370,48140339,,2,,"[{'score': 0.668095, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.69266, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69266,FALSE,0,TRUE,0.668095,TRUE,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""",Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency.
2371,48140339,,3,,"[{'score': 0.800515, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.800515,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""","The dependency tree of maven looks like belowBut I am still getting the same error."""
2372,51959287,,0,,"[{'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.594263,TRUE,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""","""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API."
2373,51959287,,1,,"[{'score': 0.7922, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.7922,FALSE,0,FALSE,0,TRUE,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""","The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help"""
2374,48521816,,0,,"[{'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,TRUE,"""I want to integrateAmazon Rekognitionfor the Face Recognition.I have created bucket and IAM user. I am trying to hit""RekognitionService.ListCollections""for the testing in POSTMAN but getting error as follows;My request header is as follows;Can anyone please guide me how to test AWS apis in POSTMAN ?""","""I want to integrateAmazon Rekognitionfor the Face Recognition.I have created bucket and IAM user."
2375,48521816,,1,,"[{'score': 0.667817, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.667817,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to integrateAmazon Rekognitionfor the Face Recognition.I have created bucket and IAM user. I am trying to hit""RekognitionService.ListCollections""for the testing in POSTMAN but getting error as follows;My request header is as follows;Can anyone please guide me how to test AWS apis in POSTMAN ?""","I am trying to hit""RekognitionService.ListCollections""for the testing in POSTMAN but getting error as follows;My request header is as follows;Can anyone please guide me how to test AWS apis in POSTMAN ?"""
2376,56155219,,0,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.""","""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity."
2377,56155219,,1,,"[{'score': 0.746925, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.746925,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.""","My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity."
2378,56155219,,2,,"[{'score': 0.736107, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.736107,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,FALSE,"""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.""","I am new to java and android, any help is welcomed."""
2379,42761695,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""","""I am trying to implement VideoIntelligence API in my 'personal-project'."
2380,42761695,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",but I am not able to do so.
2381,42761695,,2,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",[I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project.
2382,42761695,,3,,"[{'score': 0.914626, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.914626,FALSE,0,FALSE,0,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",Whereas It should search/use permission for my 'personal-project'.[since
2383,42761695,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ?
2384,42761695,,5,,"[{'score': 0.996505, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.996505,TRUE,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""","Any Suggestions please ?Thanks"""
2385,51803569,,0,,"[{'score': 0.752173, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.784773, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784773,FALSE,0,TRUE,0.752173,TRUE,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""","""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every"
2386,51803569,,1,,"[{'score': 0.646597, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.81855, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.741834, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.646597,FALSE,0,FALSE,0,TRUE,0.741834,FALSE,0,TRUE,0.81855,FALSE,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""","time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding."""
2387,40893623,,0,,"[{'score': 0.670352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.670352,TRUE,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""","""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine."
2388,40893623,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,TRUE,0.5538,TRUE,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""","It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!"""
2389,40949801,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""","""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When"
2390,40949801,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""","I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial""."
2391,40949801,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""","Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere..."""
2392,49590288,,0,,"[{'score': 0.545436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.526316, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.526316,FALSE,0,TRUE,0.545436,TRUE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""","""I have been playing around with the google cloud vision API, namely the logo detection feature."
2393,49590288,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""","Basically I want to determine if an image is a logo, so I run it through the API."
2394,49590288,,2,,"[{'score': 0.952441, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,TRUE,0.952441,FALSE,0,TRUE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""","However, I always get different results every time I run it."
2395,49590288,,3,,"[{'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.933436,TRUE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""","Sometimes the API classifies it as a logo, and sometimes it does not."
2396,49590288,,4,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.895668, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895668,FALSE,0,TRUE,0.88939,TRUE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""",Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is.
2397,49590288,,5,,"[{'score': 0.512269, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.729689, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.565996, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.512269,FALSE,0,FALSE,0,TRUE,0.565996,FALSE,0,TRUE,0.729689,FALSE,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""","I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?"""
2398,46718939,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""","""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api."
2399,46718939,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""",I get the image from canvas.toDataUrl(
2400,46718939,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""",image/png ) which contains the Data Uri.
2401,46718939,,3,,"[{'score': 0.671196, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.671196,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""","I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image."
2402,46718939,,4,,"[{'score': 0.719199, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.776154, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.719199,FALSE,0,FALSE,0,TRUE,0.776154,FALSE,0,FALSE,0,FALSE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""","If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object."
2403,46718939,,5,,"[{'score': 0.66317, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.821444, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.66317,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821444,FALSE,0,TRUE,0.681699,FALSE,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""","I would really appreciate any help for pointing me in the right direction.Thanks!"""
2404,35121089,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am integrating Google vision API into my existing android application.  the app does recognises the QR codes but i need to implement the UI feature where the user is shown a graphic outline over the bar code .""","""I am integrating Google vision API into my existing android application."
2405,35121089,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am integrating Google vision API into my existing android application.  the app does recognises the QR codes but i need to implement the UI feature where the user is shown a graphic outline over the bar code .""","the app does recognises the QR codes but i need to implement the UI feature where the user is shown a graphic outline over the bar code ."""
2406,53844994,,0,,"[{'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,FALSE,0,TRUE,"""I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).I had check below option.1.The palette library attempts to extract the following six color profiles.2. Get color of a particular pixelif I break bitmap in small size then find out color and save in list.Then there is time taken and OutOfMemoryError.Please suggest any library  to find color of images.EditPick from gallaryGet Color code from Pixel""","""I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).I had check below option.1.The palette library attempts to extract the following six color profiles.2."
2407,53844994,,1,,"[{'score': 0.637553, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.637553,FALSE,0,FALSE,0,TRUE,"""I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).I had check below option.1.The palette library attempts to extract the following six color profiles.2. Get color of a particular pixelif I break bitmap in small size then find out color and save in list.Then there is time taken and OutOfMemoryError.Please suggest any library  to find color of images.EditPick from gallaryGet Color code from Pixel""","Get color of a particular pixelif I break bitmap in small size then find out color and save in list.Then there is time taken and OutOfMemoryError.Please suggest any library  to find color of images.EditPick from gallaryGet Color code from Pixel"""
2408,55298114,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""","""I have a set of a hundred or so images (eventually this will be a few thousand)."
2409,55298114,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""",From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one.
2410,55298114,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""",Does ML Kit provide a suitable way to do this?
2411,55298114,,3,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""",I also saw that there is now a Google Cloud Vision API but this might be overkill?
2412,55298114,,4,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""","Is there already some open source projects on something similar?"""
2413,47368685,,0,,"[{'score': 0.693021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.521005, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.693021,FALSE,0,TRUE,0.521005,TRUE,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""","""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON."
2414,47368685,,1,,"[{'score': 0.650963, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.649406, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.650963,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649406,FALSE,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""","Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that."
2415,47368685,,2,,"[{'score': 0.879227, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.872734, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.879227,FALSE,0,TRUE,0.872734,TRUE,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""","I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this."""
2416,49114316,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that?""","""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API."
2417,49114316,,1,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,TRUE,0.80026,FALSE,0,TRUE,"""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that?""","Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API."
2418,49114316,,2,,"[{'score': 0.525112, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.525112,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that?""",What I'm struggling with currently is where to put this API function.
2419,49114316,,3,,"[{'score': 0.679143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.679143,FALSE,0,TRUE,0.845297,TRUE,"""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that?""","Do I have to create a model for this (even though there is no db table) or is there some way around that?"""
2420,45680183,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log (), how do I get this""id""received and use it in""Get"" to return the results?The API name is:""","""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log"
2421,45680183,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log (), how do I get this""id""received and use it in""Get"" to return the results?The API name is:""","(), how do I get this""id""received and use it in""Get"" to return the results?The API name is:"""
2422,45372938,,0,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""","""I'm trying to authenticate to Google Vision API using a JSON file."
2423,45372938,,1,,"[{'score': 0.643254, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.643254,FALSE,0,FALSE,0,TRUE,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""","Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod."
2424,45372938,,2,,"[{'score': 0.547508, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.55307, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.547508,FALSE,0,FALSE,0,FALSE,0,TRUE,0.55307,FALSE,0,FALSE,0,FALSE,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""","Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?"""
2425,55929639,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've written node.js application that identifies and pulls a face out of an image using AWS Rekognition which gives me a rectangular bounding box:I want to tightly crop out the face itself so that I can merge it with a snazzy background like the might do in a theme park.Can anyone suggest a node package or web-service?thanks!""","""I've written node.js"
2426,55929639,,1,,"[{'score': 0.798287, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.798287,FALSE,0,TRUE,0.681699,TRUE,"""I've written node.js application that identifies and pulls a face out of an image using AWS Rekognition which gives me a rectangular bounding box:I want to tightly crop out the face itself so that I can merge it with a snazzy background like the might do in a theme park.Can anyone suggest a node package or web-service?thanks!""","application that identifies and pulls a face out of an image using AWS Rekognition which gives me a rectangular bounding box:I want to tightly crop out the face itself so that I can merge it with a snazzy background like the might do in a theme park.Can anyone suggest a node package or web-service?thanks!"""
2427,43598191,,0,,"[{'score': 0.635714, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.635714,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,FALSE,0,FALSE,"""I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""Anyone knows how I could fix it?""","""I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""Anyone"
2428,43598191,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.873624, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873624,FALSE,0,TRUE,0.946222,TRUE,"""I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""Anyone knows how I could fix it?""","knows how I could fix it?"""
2429,42663690,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","""I am currently experimenting with levaraging Google Vision API for OCR."
2430,42663690,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","When I upload a image, I see the resulting JSON payload returned to me is rather large."
2431,42663690,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket."
2432,42663690,,3,,"[{'score': 0.639386, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.639386,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,FALSE,0,FALSE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data."
2433,42663690,,4,,"[{'score': 0.72739, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.842095, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.72739,FALSE,0,TRUE,0.842095,TRUE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request?"
2434,42663690,,5,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.9651, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.9651,FALSE,0,TRUE,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""","Surely there must be because this JSON being returned is very large."""
2435,39982559,,0,,"[{'score': 0.520311, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.542239, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.520311,FALSE,0,FALSE,0,TRUE,0.542239,FALSE,0,FALSE,0,FALSE,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""","""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time."
2436,39982559,,1,,"[{'score': 0.690772, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.775384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.560944, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.690772,FALSE,0,FALSE,0,TRUE,0.775384,TRUE,0.560944,FALSE,0,FALSE,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""","Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases."
2437,39982559,,2,,"[{'score': 0.573328, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.573328,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.856622,FALSE,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""","However, after some time, the error message keeps coming."
2438,39982559,,3,,"[{'score': 0.749858, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.749858,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""","Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process?"
2439,39982559,,4,,"[{'score': 0.517052, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.517052,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,FALSE,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""","Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it)."""
2440,50815200,,0,,"[{'score': 0.840424, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840424,FALSE,0,FALSE,0,TRUE,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""","""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out."
2441,50815200,,1,,"[{'score': 0.815867, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.6501, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.6501,FALSE,0,TRUE,0.815867,TRUE,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""","Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image."
2442,50815200,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""","ThanksEdit:For example - For the image shown below, the count returned should be 10 cars."
2443,50815200,,3,,"[{'score': 0.817883, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.817883,FALSE,0,FALSE,0,TRUE,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""","As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18."
2444,50815200,,4,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""","The returned object is somewhat like this."""
2445,52705012,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.""","""I've provided an image to the Google Cloud Vision OCR API to be annotated."
2446,52705012,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.""",The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'.
2447,52705012,,2,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.88939,TRUE,"""I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.""",Does this mean undefined?
2448,52705012,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.88939,TRUE,"""I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.""","I'm not finding any information in the documentation."""
2449,43740356,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though)."
2450,43740356,,1,,"[{'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",What I would like to do is determine how specific one MID value is compared to the others.
2451,43740356,,2,,"[{'score': 0.592932, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.592932,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Essentially how broad vs. refined a term is.
2452,43740356,,3,,"[{'score': 0.802215, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802215,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information."
2453,43740356,,4,,"[{'score': 0.785904, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.785904,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase."
2454,43740356,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy."
2455,43740356,,6,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Assuming broader terms have more connections to other terms than more refined terms.
2456,43740356,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections."
2457,43740356,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python."
2458,43740356,,9,,"[{'score': 0.591832, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.68121, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.591832,FALSE,0,FALSE,0,TRUE,0.68121,FALSE,0,FALSE,0,FALSE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated."
2459,43740356,,10,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""","Thank you!"""
2460,42735068,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using the google vision API and it can return the color scheme of a picture like this:The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.I am trying to create something like in the first picture. But I have no clue how to do that, so far I just have a listview that gives an overview shown like here.Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself? Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.Thank you!""","""I am using the google vision API and it can return the color scheme of a picture like this:The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.I am trying to create something like in the first picture."
2461,42735068,,1,,"[{'score': 0.560998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560998,FALSE,0,FALSE,0,TRUE,"""I am using the google vision API and it can return the color scheme of a picture like this:The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.I am trying to create something like in the first picture. But I have no clue how to do that, so far I just have a listview that gives an overview shown like here.Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself? Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.Thank you!""","But I have no clue how to do that, so far I just have a listview that gives an overview shown like here.Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself?"
2462,42735068,,2,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,TRUE,0.856622,TRUE,"""I am using the google vision API and it can return the color scheme of a picture like this:The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.I am trying to create something like in the first picture. But I have no clue how to do that, so far I just have a listview that gives an overview shown like here.Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself? Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.Thank you!""","Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.Thank you!"""
2463,40013910,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","""I am interested in TEXT_DETECTION of Google Vision API, it works impressively."
2464,40013910,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",But it seems that TEXT_DETECTION only gives exactly result when the text is in English.
2465,40013910,,2,,"[{'score': 0.827846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827846,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case)."
2466,40013910,,3,,"[{'score': 0.767592, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,TRUE,0.767592,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Can i train the machine on my own data collection to get more exactly result?
2467,40013910,,4,,"[{'score': 0.568262, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568262,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract."
2468,40013910,,5,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","As i known, they have different algorithms to detect text."
2469,40013910,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture.
2470,40013910,,7,,"[{'score': 0.683603, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.683603,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,FALSE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Google Docs gave a good result but Vision API didn't.
2471,40013910,,8,,"[{'score': 0.851788, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.851788,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this."
2472,40013910,,9,,"[{'score': 0.614562, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.512886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.614562,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512886,FALSE,0,FALSE,0,FALSE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly."
2473,40013910,,10,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","It even picks up the word ""beacon"" on the presenter's t-shirt""."
2474,40013910,,11,,"[{'score': 0.689344, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.689344,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,FALSE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","But for some of my pics, what happened was really funny."
2475,40013910,,12,,"[{'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized."
2476,40013910,,13,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too."
2477,40013910,,14,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""","There must be something wrong with the text detection algorithm."""
2478,47478044,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""","""I'm using Google Cloud Vision API to perform OCR on my documents."
2479,47478044,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,TRUE,0.5538,TRUE,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""","Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter."
2480,47478044,,2,,"[{'score': 0.873229, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873229,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""","So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2-"
2481,47478044,,3,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""","If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3-"
2482,47478044,,4,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""","If not, why isn't it documented in the optimization page?"""
2483,55263570,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run a test on theto see how it fares to the client side.I am hoping toJSON with a base64 encoded image and get image text and barcodes returned.I have created aproject and API key per the tutorial at (), but am getting an 401 error when trying to make requests.The request is written in Polymer 2.x as follows:......How do I resolve this authentication error?It is an account admin issue? Improperly formatted code?""","""I am trying to run a test on theto see how it fares to the client side.I am hoping toJSON with a base64 encoded image and get image text and barcodes returned.I have created aproject and API key per the tutorial at (), but am getting an 401 error when trying to make requests.The request is written in Polymer 2.x as follows:......How do I resolve this authentication error?It is an account admin issue?"
2484,55263570,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run a test on theto see how it fares to the client side.I am hoping toJSON with a base64 encoded image and get image text and barcodes returned.I have created aproject and API key per the tutorial at (), but am getting an 401 error when trying to make requests.The request is written in Polymer 2.x as follows:......How do I resolve this authentication error?It is an account admin issue? Improperly formatted code?""","Improperly formatted code?"""
2485,55408126,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:""","""I am trying to integrate the Google Vision API."
2486,55408126,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:""","I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:"""
2487,55113529,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong? Hoping to figure this out as it is a very interesting concept. I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.Thanks for the help!Im getting this error:Here is my code which I tried""","""Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong?"
2488,55113529,,1,,"[{'score': 0.633068, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.633068,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,FALSE,"""Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong? Hoping to figure this out as it is a very interesting concept. I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.Thanks for the help!Im getting this error:Here is my code which I tried""",Hoping to figure this out as it is a very interesting concept.
2489,55113529,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong? Hoping to figure this out as it is a very interesting concept. I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.Thanks for the help!Im getting this error:Here is my code which I tried""","I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.Thanks for the help!Im getting this error:Here is my code which I tried"""
2490,50685445,,0,,"[{'score': 0.657939, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.657939,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""","""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint."
2491,50685445,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",The CloudFormation serverless.template
2492,50685445,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""","file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events."
2493,50685445,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""","I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix."
2494,50685445,,4,,"[{'score': 0.736294, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.736294,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.
2495,50685445,,5,,"[{'score': 0.891509, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.891509,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""","I am receiving an error stating ""Rules key is invalid for this object"" on line 48."
2496,50685445,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",I have read the documentation and googled this and this seems to be the correct syntax.
2497,50685445,,7,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.946222,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",Did I specify something wrong here?
2498,50685445,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""","Thanks in advance."""
2499,48857882,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""","""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python."
2500,48857882,,1,,"[{'score': 0.617261, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.617261,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.97759,FALSE,0,FALSE,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",It all goes well.
2501,48857882,,2,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,TRUE,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get.
2502,48857882,,3,,"[{'score': 0.552743, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.552743,FALSE,0,FALSE,0,TRUE,0.527318,TRUE,0.638987,FALSE,0,FALSE,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""","I can't iterate through the response, there is also no method the interface that would be an obvious candidate."
2503,48857882,,4,,"[{'score': 0.782703, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.782703,FALSE,0,FALSE,0,TRUE,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""","Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?"""
2504,51317429,,0,,"[{'score': 0.582548, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.582548,FALSE,0,FALSE,0,TRUE,"""i want to set CCL option in google vision api..but api document is not support this infomationI found that Google Image Search provides the following URL.tbs=sur:fcI wonder if this is also available in the GOOGLE VISION API. If possible, I want to know the URL that contains the method.""","""i want to set CCL option in google vision api..but api document is not support this infomationI found that Google Image Search provides the following URL.tbs=sur:fcI wonder if this is also available in the GOOGLE VISION API."
2505,51317429,,1,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,TRUE,0.75152,TRUE,"""i want to set CCL option in google vision api..but api document is not support this infomationI found that Google Image Search provides the following URL.tbs=sur:fcI wonder if this is also available in the GOOGLE VISION API. If possible, I want to know the URL that contains the method.""","If possible, I want to know the URL that contains the method."""
2506,54769503,,0,,"[{'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,FALSE,0,TRUE,"""I'm trying to analyse my images using(Azure Cognitive Service)But the issue is my Image is stored in Blob container withPrivate accesswhich means without a SAS token it will not able to access. So when I tried to call the Computer Vision API with myimage URL + SAS.It's givingbad requestYou can easily repro this issue intoo""","""I'm trying to analyse my images using(Azure Cognitive Service)But the issue is my Image is stored in Blob container withPrivate accesswhich means without a SAS token it will not able to access."
2507,54769503,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to analyse my images using(Azure Cognitive Service)But the issue is my Image is stored in Blob container withPrivate accesswhich means without a SAS token it will not able to access. So when I tried to call the Computer Vision API with myimage URL + SAS.It's givingbad requestYou can easily repro this issue intoo""","So when I tried to call the Computer Vision API with myimage URL + SAS.It's givingbad requestYou can easily repro this issue intoo"""
2508,45836819,,0,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I want to integrate the Amazon rekognition, compare face api in my ionic project. Can anyone suggest any sample or way to that.I am trying on simple JS but getting an authorization error.""","""I want to integrate the Amazon rekognition, compare face api in my ionic project."
2509,45836819,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I want to integrate the Amazon rekognition, compare face api in my ionic project. Can anyone suggest any sample or way to that.I am trying on simple JS but getting an authorization error.""","Can anyone suggest any sample or way to that.I am trying on simple JS but getting an authorization error."""
2510,48262231,,0,,"[{'score': 0.581659, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.581659,FALSE,0,FALSE,0,TRUE,"""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After inclusion of these lines in the root-context its gives page not found.""","""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After"
2511,48262231,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After inclusion of these lines in the root-context its gives page not found.""","inclusion of these lines in the root-context its gives page not found."""
2512,51563897,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""","""I'm trying to use Google Cloud Vision in a NodeJS app."
2513,51563897,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""",Following the:Only the 'started' is printed in the console.
2514,51563897,,2,,"[{'score': 0.562986, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.562986,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.822231,FALSE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""",It does not enter in either success or failure functions.
2515,51563897,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""","Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout."
2516,51563897,,4,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.946222,TRUE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""","But I can't find anything in docs, Stack Overflow or GitHub issues."
2517,51563897,,5,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.998976,TRUE,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""","Any clue?"""
2518,46483447,,0,,"[{'score': 0.950868, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.950868,FALSE,0,FALSE,0,TRUE,"""I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!""","""I've been trying to use the AWSRekognition SDK in order to compare face."
2519,46483447,,1,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!""","However, Amazon has no Documentation on how to integrate their SDK with iOS."
2520,46483447,,2,,"[{'score': 0.61383, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61383,FALSE,0,FALSE,0,TRUE,"""I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!""","They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!"""
2521,54133439,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""","""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api."
2522,54133439,,1,,"[{'score': 0.569534, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.755313, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.569534,FALSE,0,FALSE,0,TRUE,0.755313,FALSE,0,FALSE,0,FALSE,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""",The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640.
2523,54133439,,2,,"[{'score': 0.597185, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.597185,FALSE,0,FALSE,0,TRUE,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""","So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how."
2524,54133439,,3,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""","Can anyone help or point me in the right direction?"""
2525,55063440,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""In my application, I am uploading image to get emotion response from Google Cloud Vision API. But while sending the captured image to cloud, I am getting error asErrorThis is the code i have written to send image to URL,i am not getting response from Google Cloud Vision API ,when i paste that url in web browser i am getting error as 404 file not found""","""In my application, I am uploading image to get emotion response from Google Cloud Vision API."
2526,55063440,,1,,"[{'score': 0.540126, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.520668, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.540126,FALSE,0,FALSE,0,TRUE,0.520668,FALSE,0,FALSE,0,FALSE,"""In my application, I am uploading image to get emotion response from Google Cloud Vision API. But while sending the captured image to cloud, I am getting error asErrorThis is the code i have written to send image to URL,i am not getting response from Google Cloud Vision API ,when i paste that url in web browser i am getting error as 404 file not found""","But while sending the captured image to cloud, I am getting error asErrorThis is the code i have written to send image to URL,i am not getting response from Google Cloud Vision API ,when i paste that url in web browser i am getting error as 404 file not found"""
2527,56040881,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""","""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp."
2528,56040881,,1,,"[{'score': 0.682143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682143,FALSE,0,FALSE,0,TRUE,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""","Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted."
2529,56040881,,2,,"[{'score': 0.61203, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.932907, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.932907,FALSE,0,TRUE,0.61203,TRUE,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description.
2530,56040881,,3,,"[{'score': 0.511296, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.605764, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.884172, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.511296,FALSE,0,FALSE,0,TRUE,0.884172,FALSE,0,TRUE,0.605764,FALSE,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API.
2531,56040881,,4,,"[{'score': 0.845958, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.845958,FALSE,0,FALSE,0,TRUE,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""","Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated."""
2532,49890108,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:""","""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature."
2533,49890108,,1,,"[{'score': 0.676523, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.644135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.676523,FALSE,0,FALSE,0,TRUE,0.644135,FALSE,0,FALSE,0,FALSE,"""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:""","There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:"""
2534,38534147,,0,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.841239, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841239,FALSE,0,TRUE,0.91961,TRUE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""","""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project."
2535,38534147,,1,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,TRUE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""","I tried this by puttinginin the Android module, like in this."
2536,38534147,,2,,"[{'score': 0.643235, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.643235,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.909883,FALSE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",This did not work (maybe I should write it somewhere else?
2537,38534147,,3,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",I can't figure it out).
2538,38534147,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",Now there are many inspections shown in this.
2539,38534147,,5,,"[{'score': 0.624925, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.624925,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",There is said that there components cannot be applied to.I have installed the Google Repository.
2540,38534147,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""","And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?"""
2541,50498421,,0,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""","""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API."
2542,50498421,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""","It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp."
2543,50498421,,2,,"[{'score': 0.685577, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.671221, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.685577,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.671221,FALSE,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""","I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?"""
2544,49067894,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.""","""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java."
2545,49067894,,1,,"[{'score': 0.742848, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.742848,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.946222,FALSE,"""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.""","Any help will be highly appreciated."""
2546,54574681,,0,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code""","""I am using azure face api using node js, below is the code."
2547,54574681,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.5538,TRUE,"""I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code""",However instead of the image hosted some where i want to use my local image and post it.
2548,54574681,,2,,"[{'score': 0.750456, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.750456,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.594263,FALSE,"""I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code""","i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code"""
2549,47754119,,0,,"[{'score': 0.743682, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743682,FALSE,0,FALSE,0,TRUE,"""Code :I'm trying to extract text from an image using the google vision api. I need to send a batch of images - things were working fine but now there is this error:""","""Code :I'm trying to extract text from an image using the google vision api."
2550,47754119,,1,,"[{'score': 0.538946, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.538946,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Code :I'm trying to extract text from an image using the google vision api. I need to send a batch of images - things were working fine but now there is this error:""","I need to send a batch of images - things were working fine but now there is this error:"""
2551,50181484,,0,,"[{'score': 0.817586, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.706183, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.817586,FALSE,0,FALSE,0,TRUE,0.706183,FALSE,0,FALSE,0,FALSE,"""I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.My bucket's region is us-east-1 and I have configured the same in my code.what am I doing wrong?""","""I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.My bucket's region is us-east-1 and I have configured the same in my code.what"
2552,50181484,,1,,"[{'score': 0.931034, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.916667, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.916667,FALSE,0,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.My bucket's region is us-east-1 and I have configured the same in my code.what am I doing wrong?""","am I doing wrong?"""
2553,49915680,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""","""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.."
2554,49915680,,1,,"[{'score': 0.827997, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827997,FALSE,0,FALSE,0,TRUE,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""","I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks."""
2555,47920981,,0,,"[{'score': 0.564476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.564476,FALSE,0,FALSE,0,TRUE,"""I'm trying to make an API call to Google Cloud Vision API using an API key from Golang. But I'm getting a.The apiKey/apiKeyOption part of the code below is mine.What is the right way to make this call?  Is it possible at all?...""","""I'm trying to make an API call to Google Cloud Vision API using an API key from Golang."
2556,47920981,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make an API call to Google Cloud Vision API using an API key from Golang. But I'm getting a.The apiKey/apiKeyOption part of the code below is mine.What is the right way to make this call?  Is it possible at all?...""",But I'm getting a.The apiKey/apiKeyOption part of the code below is mine.What is the right way to make this call?
2557,47920981,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to make an API call to Google Cloud Vision API using an API key from Golang. But I'm getting a.The apiKey/apiKeyOption part of the code below is mine.What is the right way to make this call?  Is it possible at all?...""","Is it possible at all?..."""
2558,54821969,,0,,"[{'score': 0.588341, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.588341,TRUE,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""","""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy."
2559,54821969,,1,,"[{'score': 0.698059, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.708196, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.698059,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708196,FALSE,0,TRUE,0.594263,FALSE,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""","I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)"""
2560,55464541,,0,,"[{'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,FALSE,0,TRUE,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""","""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model."
2561,55464541,,1,,"[{'score': 0.692318, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.676737, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.692318,FALSE,0,FALSE,0,TRUE,0.676737,FALSE,0,FALSE,0,FALSE,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""","When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete."
2562,55464541,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Your model training is complete.
2563,55464541,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""","Click here to view and test your model."""""
2564,55537358,,0,,"[{'score': 0.69521, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.624621, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.69521,FALSE,0,FALSE,0,TRUE,0.624621,FALSE,0,FALSE,0,FALSE,"""I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.I am getting a bad image error using the b64content.  However, if I use the non base64 content, my call to the Vision API succeeds:Does blob.download_as_string() return a byte string that is already base64 encoded?""","""I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.I am getting a bad image error using the b64content."
2565,55537358,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.I am getting a bad image error using the b64content.  However, if I use the non base64 content, my call to the Vision API succeeds:Does blob.download_as_string() return a byte string that is already base64 encoded?""","However, if I use the non base64 content, my call to the Vision API succeeds:Does blob.download_as_string()"
2566,55537358,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.I am getting a bad image error using the b64content.  However, if I use the non base64 content, my call to the Vision API succeeds:Does blob.download_as_string() return a byte string that is already base64 encoded?""","return a byte string that is already base64 encoded?"""
2567,41959043,,0,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""","""I am using Microsoft's cognitive services."
2568,41959043,,1,,"[{'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""","I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text."
2569,41959043,,2,,"[{'score': 0.874319, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874319,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""","However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services."
2570,41959043,,3,,"[{'score': 0.612272, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.612272,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""",Is there a better way to do it?
2571,41959043,,4,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""","Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?"""
2572,53739849,,0,,"[{'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,FALSE,0,TRUE,"""AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name. Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?""","""AWS provide  this Java code to perform video analysis in rekognition."
2573,53739849,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name. Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?""",However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name.
2574,53739849,,2,,"[{'score': 0.515549, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.515549,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name. Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?""","Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?"""
2575,34925092,,0,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""","""I was working on a QRCode reader within my app using the Vision API."
2576,34925092,,1,,"[{'score': 0.771129, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.771129,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",I realized the API can't detect negative colors.
2577,34925092,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes.
2578,34925092,,3,,"[{'score': 0.942582, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.942582,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",The first works perfectly (default colors).
2579,34925092,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",The second one doesn't (white on blue).
2580,34925092,,5,,"[{'score': 0.60959, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.60959,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,FALSE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I
2581,34925092,,6,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",really want to avoid using third party libs and apps to do this.
2582,34925092,,7,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""","So far, I'm using the NEGATIVE effect on the camera."
2583,34925092,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it.
2584,34925092,,9,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.75152,TRUE,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""","Suggestions?Thank you for your attention."""
2585,38480586,,0,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""","""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each."
2586,38480586,,1,,"[{'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,TRUE,0.704642,FALSE,0,TRUE,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",Then the costs would beis my understanding correct?
2587,38480586,,2,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",The $4 per call seems too high.
2588,38480586,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""","Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?"""
2589,42041693,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""","""I have tried so many way but i can't succeed."
2590,42041693,,1,,"[{'score': 0.572005, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.572005,FALSE,0,FALSE,0,TRUE,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""",I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i
2591,42041693,,2,,"[{'score': 0.619765, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.619765,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""","try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS."
2592,42041693,,3,,"[{'score': 0.544592, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.544592,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""","sorry for my bad EnglishThank you in advance."""
2593,43631861,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.The error says""","""I have been able to run Google's Vision API successfully on locally stored images."
2594,43631861,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.The error says""","However, whenever I run my script on an image stored on an external server."
2595,43631861,,2,,"[{'score': 0.838846, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.838846,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,FALSE,"""I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.The error says""","I get an error.The error says"""
2596,50500341,,0,,"[{'score': 0.62928, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.62928,TRUE,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""","""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start."
2597,50500341,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""","I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated."""
2598,51009080,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting data in this get_item . how can I get this data in scan query where EventName='newevent'  and 'RekognitionId': {'S': match['Face']['FaceId']""","""I am getting data in this get_item ."
2599,51009080,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting data in this get_item . how can I get this data in scan query where EventName='newevent'  and 'RekognitionId': {'S': match['Face']['FaceId']""","how can I get this data in scan query where EventName='newevent'  and 'RekognitionId': {'S': match['Face']['FaceId']"""
2600,37742610,,0,,"[{'score': 0.505247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.505247,FALSE,0,FALSE,0,TRUE,"""I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.In the main.js file in server folder I run:But that causes an error in the terminal:My site does not load so I run:But then I get an error I'n can't find anything about in the internet:I've been been trying to solve this issue for 2 days now, without luck. Any suggestions?""","""I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.In the main.js"
2601,37742610,,1,,"[{'score': 0.770229, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.6731, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.770229,FALSE,0,FALSE,0,TRUE,0.6731,FALSE,0,TRUE,0.594263,FALSE,"""I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.In the main.js file in server folder I run:But that causes an error in the terminal:My site does not load so I run:But then I get an error I'n can't find anything about in the internet:I've been been trying to solve this issue for 2 days now, without luck. Any suggestions?""","file in server folder I run:But that causes an error in the terminal:My site does not load so I run:But then I get an error I'n can't find anything about in the internet:I've been been trying to solve this issue for 2 days now, without luck."
2602,37742610,,2,,"[{'score': 0.999857, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999857,TRUE,"""I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.In the main.js file in server folder I run:But that causes an error in the terminal:My site does not load so I run:But then I get an error I'n can't find anything about in the internet:I've been been trying to solve this issue for 2 days now, without luck. Any suggestions?""","Any suggestions?"""
2603,53756545,,0,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""","""I am new to AWS.. Now I am ok with Recognizing faces in static images."
2604,53756545,,1,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""",I would like to Recognize faces in a streaming video..
2605,53756545,,2,,"[{'score': 0.911475, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.911475,FALSE,0,FALSE,0,TRUE,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""",I refer this linkBut I don't understand how to connect the live video into kinesis video stream.
2606,53756545,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,TRUE,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""","Anyone can help me to understand the processor ."""
2607,50866887,,0,,"[{'score': 0.536507, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.536507,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.return base64 of image that has captured.Below is my code so far:Thanks in advance!""","""I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.return"
2608,50866887,,1,,"[{'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,TRUE,"""I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.return base64 of image that has captured.Below is my code so far:Thanks in advance!""","base64 of image that has captured.Below is my code so far:Thanks in advance!"""
2609,46078769,,0,,"[{'score': 0.806276, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.806276,FALSE,0,FALSE,0,TRUE,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""","""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored?"
2610,46078769,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""","Can I view them or download them?Thanks!"""
2611,54644237,,0,,"[{'score': 0.566258, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.566258,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,FALSE,"""I have dumped response from google vision api for several images in text file. Now I need to do some manipulations, but response dumped is in string format.So, how can I convert str to AnnotateImageResponse or JSON in python?""","""I have dumped response from google vision api for several images in text file."
2612,54644237,,1,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I have dumped response from google vision api for several images in text file. Now I need to do some manipulations, but response dumped is in string format.So, how can I convert str to AnnotateImageResponse or JSON in python?""","Now I need to do some manipulations, but response dumped is in string format.So, how can I convert str to AnnotateImageResponse or JSON in python?"""
2613,40450515,,0,,"[{'score': 0.504649, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.504649,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?""","""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key."
2614,40450515,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?""","I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image."
2615,40450515,,2,,"[{'score': 0.783905, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.772649, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.783905,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.772649,FALSE,"""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?""","The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?"""
2616,42657315,,0,,"[{'score': 0.804142, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804142,FALSE,0,FALSE,0,TRUE,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""","""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device."
2617,42657315,,1,,"[{'score': 0.652411, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.652411,FALSE,0,FALSE,0,TRUE,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""","Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple"""
2618,55019870,,0,,"[{'score': 0.767921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.767921,FALSE,0,FALSE,0,TRUE,"""I'm trying to detect handwritten dates using the Google Vision API. Do you know if it is possible to force it to detect dates (DD/MM/YYYY), or at least numbers only to increase reliablity?The function I use, takes an Image as np.array as input:""","""I'm trying to detect handwritten dates using the Google Vision API."
2619,55019870,,1,,"[{'score': 0.519411, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.519411,TRUE,"""I'm trying to detect handwritten dates using the Google Vision API. Do you know if it is possible to force it to detect dates (DD/MM/YYYY), or at least numbers only to increase reliablity?The function I use, takes an Image as np.array as input:""","Do you know if it is possible to force it to detect dates (DD/MM/YYYY), or at least numbers only to increase reliablity?The function I use, takes an Image as np.array as input:"""
2620,44373059,,0,,"[{'score': 0.85332, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.85332,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""","""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content'"
2621,44373059,,1,,"[{'score': 0.512342, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.512342,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""","(TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical."
2622,44373059,,2,,"[{'score': 0.536673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.536673,FALSE,0,FALSE,0,TRUE,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""","The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?"""
2623,50199393,,0,,"[{'score': 0.634822, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.634822,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""","""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision."
2624,50199393,,1,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""","However the JSON dump looks like:Yes, it's an eyesore to look at."
2625,50199393,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,TRUE,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""",I am only wanting to extract the likelihood.
2626,50199393,,3,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""","Preferably in this format:Python code can be found here:"""
2627,51590523,,0,,"[{'score': 0.82167, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.82167,FALSE,0,FALSE,0,TRUE,"""The question has beenand the commercial solution from BlinkID is working well for me.I am trying to extend the application to recognise more type of document. For the moment I am using Google Cloud Vision with aalgorithm to (heuristic) detect the keyword (,,etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short () and not always visible etc.Having no background in computer vision, I'm looking for a way to improve the current solution or a better one.""","""The question has beenand the commercial solution from BlinkID is working well for me.I am trying to extend the application to recognise more type of document."
2628,51590523,,1,,"[{'score': 0.699237, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.699237,TRUE,"""The question has beenand the commercial solution from BlinkID is working well for me.I am trying to extend the application to recognise more type of document. For the moment I am using Google Cloud Vision with aalgorithm to (heuristic) detect the keyword (,,etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short () and not always visible etc.Having no background in computer vision, I'm looking for a way to improve the current solution or a better one.""","For the moment I am using Google Cloud Vision with aalgorithm to (heuristic) detect the keyword (,,etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short () and not always visible etc.Having no background in computer vision, I'm looking for a way to improve the current solution or a better one."""
2629,49648719,,0,,"[{'score': 0.701262, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.664718, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.701262,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664718,FALSE,"""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl asandboth failed with error:I am using Windows 7 64""","""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"","
2630,49648719,,1,,"[{'score': 0.730014, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.730014,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl asandboth failed with error:I am using Windows 7 64""",I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl
2631,49648719,,2,,"[{'score': 0.794708, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.794708,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl asandboth failed with error:I am using Windows 7 64""","asandboth failed with error:I am using Windows 7 64"""
2632,52468352,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""","""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses."
2633,52468352,,1,,"[{'score': 0.573356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.573356,FALSE,0,FALSE,0,TRUE,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""","Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64."
2634,52468352,,2,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""","Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence."
2635,52468352,,3,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.741413, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.741413,TRUE,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""","But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?"""
2636,35519689,,0,,"[{'score': 0.610552, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.610552,FALSE,0,FALSE,0,TRUE,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""","""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests."
2637,35519689,,1,,"[{'score': 0.560701, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.579545, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.560701,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579545,FALSE,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""","While recognition quality is good, characters are returned without any hint of the original layout."
2638,35519689,,2,,"[{'score': 0.624621, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.624621,FALSE,0,TRUE,0.681699,TRUE,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""","Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR."
2639,35519689,,3,,"[{'score': 0.994057, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994057,FALSE,0,FALSE,0,TRUE,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""","For example, [1] and [2]."
2640,35519689,,4,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""","There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]"""
2641,55086411,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""","""My Azure custom vision video object has high detection latency in the FO Tier."
2642,55086411,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""",How can I minimize response time?
2643,55086411,,2,,"[{'score': 0.508625, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.508625,FALSE,0,FALSE,0,TRUE,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""",Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API.
2644,55086411,,3,,"[{'score': 0.500695, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.500695,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.5538,FALSE,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""",This has a lot of latency in response time.
2645,55086411,,4,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""","If I send a 1-minute video of 20FPS it takes 2+ hours to process it."""
2646,36729360,,0,,"[{'score': 0.939116, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.939116,FALSE,0,FALSE,0,TRUE,"""Is there anyway I can analyse URL's using Google Cloud Vision. I know how to analyse images that I store locally, but I can't seem to analyse jpg's that exist on the internet:Is there anyway I can analyse a URL and get an answer as to whether there are any logo's in them?""","""Is there anyway I can analyse URL's using Google Cloud Vision."
2647,36729360,,1,,"[{'score': 0.900876, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.900876,FALSE,0,FALSE,0,TRUE,"""Is there anyway I can analyse URL's using Google Cloud Vision. I know how to analyse images that I store locally, but I can't seem to analyse jpg's that exist on the internet:Is there anyway I can analyse a URL and get an answer as to whether there are any logo's in them?""","I know how to analyse images that I store locally, but I can't seem to analyse jpg's that exist on the internet:Is there anyway I can analyse a URL and get an answer as to whether there are any logo's in them?"""
2648,40671175,,0,,"[{'score': 0.743682, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743682,FALSE,0,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""","""I will be developing an app that uses Google Vision API in order to scan barcode."
2649,40671175,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",I am successfully able to write and test the app.
2650,40671175,,2,,"[{'score': 0.520338, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,TRUE,0.520338,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""","However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above."
2651,40671175,,3,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",I am using Google Play Services 8.4 version.
2652,40671175,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",Will I be able to use this app?
2653,40671175,,5,,"[{'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.834975, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.834975,FALSE,0,TRUE,0.804675,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",I have just created a prototype of app only.In short is there any relationship between google play services and android version?
2654,40671175,,6,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,FALSE,0,TRUE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",If yes where can I find it.
2655,40671175,,7,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""","Thanks."""
2656,43069723,,0,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""We are using Google Vision API to extract text from image. Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.Can someone explain why we are getting that error and how we can rectify it?""","""We are using Google Vision API to extract text from image."
2657,43069723,,1,,"[{'score': 0.721421, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.721421,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""We are using Google Vision API to extract text from image. Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.Can someone explain why we are getting that error and how we can rectify it?""","Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.Can someone explain why we are getting that error and how we can rectify it?"""
2658,53827914,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.I want to detect faces after every 1 second in the video that means run CF.face.detect once/second on video frame.Please tell how to do itThanks in advance""","""I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.I want to detect faces after every 1 second in the video that means run CF.face.detect"
2659,53827914,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.I want to detect faces after every 1 second in the video that means run CF.face.detect once/second on video frame.Please tell how to do itThanks in advance""","once/second on video frame.Please tell how to do itThanks in advance"""
2660,51527259,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""","""I am currently creating an android version of an IOS app that is built."
2661,51527259,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json'
2662,51527259,,2,,"[{'score': 0.678179, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.678179,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""","file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing."
2663,51527259,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio).
2664,51527259,,4,,"[{'score': 0.676659, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.676659,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""","But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background."
2665,51527259,,5,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.944551, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.944551,FALSE,0,TRUE,0.822231,TRUE,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""","Any help is greatly appreciated."""
2666,44025456,,0,,"[{'score': 0.563171, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.563171,TRUE,"""Is there any API for Google translate app like feature for website in which user upload an image and start selecting image textual area and he/she get the text?I found google vision API but its not just what i want. I want that image should be showing on website and user is selecting textual part of the image with mouse and user is getting the same text in some variable so that he/she can print the text wherever required.""","""Is there any API for Google translate app like feature for website in which user upload an image and start selecting image textual area and he/she get the text?I found google vision API but its not just what i want."
2667,44025456,,1,,"[{'score': 0.608261, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.608261,FALSE,0,FALSE,0,TRUE,"""Is there any API for Google translate app like feature for website in which user upload an image and start selecting image textual area and he/she get the text?I found google vision API but its not just what i want. I want that image should be showing on website and user is selecting textual part of the image with mouse and user is getting the same text in some variable so that he/she can print the text wherever required.""","I want that image should be showing on website and user is selecting textual part of the image with mouse and user is getting the same text in some variable so that he/she can print the text wherever required."""
2668,36120746,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""","""I'm trying to use Microsoft Face API."
2669,36120746,,1,,"[{'score': 0.830989, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.830989,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""","For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format..."
2670,36120746,,2,,"[{'score': 0.733505, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.733505,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""","If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng."
2671,36120746,,3,,"[{'score': 0.996505, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.996505,TRUE,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""","Anyone might know whats the problem?UPDATEThe variable:"""
2672,54082512,,0,,"[{'score': 0.790954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.790954,FALSE,0,FALSE,0,TRUE,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""","""I'm not able to call the Google Vision API due to authorization issues."
2673,54082512,,1,,"[{'score': 0.918619, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.918619,FALSE,0,FALSE,0,TRUE,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""",The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json
2674,54082512,,2,,"[{'score': 0.593355, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.851788, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.593355,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,TRUE,0.851788,FALSE,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""","file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?"""
2675,45300613,,0,,"[{'score': 0.563896, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.563896,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient. As I try to initialize it, I get the error:I have tried everything but I can not find my error. Can you help me?Thank you!""","""I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient."
2676,45300613,,1,,"[{'score': 0.813746, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.813746,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,FALSE,"""I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient. As I try to initialize it, I get the error:I have tried everything but I can not find my error. Can you help me?Thank you!""","As I try to initialize it, I get the error:I have tried everything but I can not find my error."
2677,45300613,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient. As I try to initialize it, I get the error:I have tried everything but I can not find my error. Can you help me?Thank you!""","Can you help me?Thank you!"""
2678,39712648,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""I am using google vision api to recognise  text from image. The image in Japanese language.But response is not in Japanese language it is in English. Can any body tell me how to change english to Japanese.""","""I am using google vision api to recognise  text from image."
2679,39712648,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am using google vision api to recognise  text from image. The image in Japanese language.But response is not in Japanese language it is in English. Can any body tell me how to change english to Japanese.""",The image in Japanese language.But response is not in Japanese language it is in English.
2680,39712648,,2,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,TRUE,"""I am using google vision api to recognise  text from image. The image in Japanese language.But response is not in Japanese language it is in English. Can any body tell me how to change english to Japanese.""","Can any body tell me how to change english to Japanese."""
2681,51747822,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""","""I'm using the Microsoft Custom Vision service for object detection with the Python SDK."
2682,51747822,,1,,"[{'score': 0.668544, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.668544,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""","I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below."
2683,51747822,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""","Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex."
2684,51747822,,3,,"[{'score': 0.734369, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.734369,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""","The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?"""
2685,44615959,,0,,"[{'score': 0.63698, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,TRUE,0.63698,TRUE,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""","""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized."
2686,44615959,,1,,"[{'score': 0.67512, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.67512,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""","I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option."
2687,44615959,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.946222,TRUE,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""",Any idea why this is happening?
2688,44615959,,3,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""","I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries."""
2689,52541577,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""","""I am trying to create an angular project with Google Vision, but angular refuses to compile it."
2690,52541577,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",Here's my app.component.ts
2691,52541577,,2,,"[{'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,TRUE,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""","fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance."""
2692,52471647,,0,,"[{'score': 0.542239, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.542239,FALSE,0,FALSE,0,TRUE,"""currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?or maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.i have tried getboundingbox(), but i dont know what to do nextthis is my code for the text detector""","""currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?or"
2693,52471647,,1,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.626109, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.626109,FALSE,0,TRUE,0.57374,TRUE,"""currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?or maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.i have tried getboundingbox(), but i dont know what to do nextthis is my code for the text detector""","maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.i"
2694,52471647,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?or maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.i have tried getboundingbox(), but i dont know what to do nextthis is my code for the text detector""","have tried getboundingbox(), but i dont know what to do nextthis is my code for the text detector"""
2695,48412094,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","""So I'm using the Google Cloud Vision API to get back data on an image."
2696,48412094,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","As I was testing, I came across a large image that apparently exceeded the max byte size length."
2697,48412094,,2,,"[{'score': 0.771302, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.771302,FALSE,0,FALSE,0,TRUE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google."
2698,48412094,,3,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.512886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512886,FALSE,0,TRUE,0.57374,TRUE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now."
2699,48412094,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","I do apologize, I like to provide all information but I can't even put it in a gist to post here."
2700,48412094,,5,,"[{'score': 0.733478, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.733478,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.647986,FALSE,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""","Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?"""
2701,51977903,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.615352,TRUE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""","""I'm working on a face tracking thought video()."
2702,51977903,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",It will return the job id.
2703,51977903,,2,,"[{'score': 0.765977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765977,FALSE,0,FALSE,0,TRUE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",I have setup everythingparts.Here my problemis i can't able to get a message from SQS using.
2704,51977903,,3,,"[{'score': 0.709665, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.709665,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",Evenalso not working.
2705,51977903,,4,,"[{'score': 0.702056, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.702056,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",It gives this below error.The code as follow.
2706,51977903,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""","AWS PHP SDK ver 3.64.11.Thanks in advance!"""
2707,42713068,,0,,"[{'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,TRUE,"""I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.""","""I am a novice at ionic 2. Is there any way I could use google's vision API in my program?"
2708,42713068,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.""","If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene."
2709,42713068,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.""","Thanks in advance."""
2710,55301066,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","""I'm trying to use Google Vision's BarcodeDetector to work in my app."
2711,55301066,,1,,"[{'score': 0.826883, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.826883,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes)."
2712,55301066,,2,,"[{'score': 0.718494, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.718494,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is"
2713,55301066,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.984926, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,TRUE,0.984926,FALSE,0,TRUE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","always false, never true."
2714,55301066,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection."
2715,55301066,,5,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""","Both conditions are fulfilled, I simply can't get it to work."""
2716,43748559,,0,,"[{'score': 0.780932, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.780932,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""","""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library."
2717,43748559,,1,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""","Is it possible?"""
2718,48375623,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""What is the syntax for calling the watson visual recognition api in python?I've looked around a lot but have not been able to find a proper syntax for the call of the api. What parameters have to be defined within the call of the api?Thanks for the help.""","""What is the syntax for calling the watson visual recognition api in python?I've looked around a lot but have not been able to find a proper syntax for the call of the api."
2719,48375623,,1,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,TRUE,0.80026,FALSE,0,TRUE,"""What is the syntax for calling the watson visual recognition api in python?I've looked around a lot but have not been able to find a proper syntax for the call of the api. What parameters have to be defined within the call of the api?Thanks for the help.""","What parameters have to be defined within the call of the api?Thanks for the help."""
2720,54554676,,0,,"[{'score': 0.645291, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.645291,FALSE,0,FALSE,0,TRUE,"""Using Raspberry pi got the live video streaming using Kinesis Video Streaming Parser library and want to process stream to Kinesis Video Rekognition for detecting persons.Set the required details of ARN, got the video stream an set to Frame Viewer. Then trying to integrate Kinesis Video Stream with Rekognition.""","""Using Raspberry pi got the live video streaming using Kinesis Video Streaming Parser library and want to process stream to Kinesis Video Rekognition for detecting persons.Set the required details of ARN, got the video stream an set to Frame Viewer."
2721,54554676,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""Using Raspberry pi got the live video streaming using Kinesis Video Streaming Parser library and want to process stream to Kinesis Video Rekognition for detecting persons.Set the required details of ARN, got the video stream an set to Frame Viewer. Then trying to integrate Kinesis Video Stream with Rekognition.""","Then trying to integrate Kinesis Video Stream with Rekognition."""
2722,45696336,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""","""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request."
2723,45696336,,1,,"[{'score': 0.543581, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.815943, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.543581,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,TRUE,0.815943,FALSE,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""",I am not sure if this problem is caused by the headers or binary data I created.
2724,45696336,,2,,"[{'score': 0.631525, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.631525,TRUE,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""",I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there.
2725,45696336,,3,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.994446,TRUE,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""",Could some one help to explain?
2726,45696336,,4,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""","ThanksThis is the api reference"""
2727,47281901,,0,,"[{'score': 0.668127, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.668127,FALSE,0,FALSE,0,TRUE,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""","""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column."
2728,47281901,,1,,"[{'score': 0.719382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719382,FALSE,0,FALSE,0,TRUE,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""","Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-"""
2729,51926971,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.681699,TRUE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""","""please read the full question before marking it as duplicate or down-vote it.i"
2730,51926971,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk.
2731,51926971,,2,,"[{'score': 0.607235, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.607235,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory.
2732,51926971,,3,,"[{'score': 0.630798, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.630798,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image.
2733,51926971,,4,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",How can i do so without uploading on cloud fetch it again for process since it might take long.
2734,51926971,,5,,"[{'score': 0.751343, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.974578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.751343,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974578,FALSE,0,FALSE,0,FALSE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",any alternative solution is very welcome as well.
2735,51926971,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""","i am stuck on this for quite a while.This is the image type i want to slice in pieces"""
2736,53543793,,0,,"[{'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839577,TRUE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","""I am attempting to run multiple images through the AWS system using python code, just a basic for loop."
2737,53543793,,1,,"[{'score': 0.763429, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.763429,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""",When I run the code I am getting an error.
2738,53543793,,2,,"[{'score': 0.706564, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.706564,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"","
2739,53543793,,3,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})"
2740,53543793,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"","
2741,53543793,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","line 320, in _api_call    return self._make_api_call(operation_name,"
2742,53543793,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"","
2743,53543793,,7,,"[{'score': 0.805839, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.53546, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.805839,FALSE,0,FALSE,0,TRUE,0.53546,FALSE,0,FALSE,0,FALSE,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""","line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions."""
2744,51399511,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""","""I can't able to set multiple region for AWS Service Manager."
2745,51399511,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",(Why multiple region?
2746,51399511,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""","because S3,rekognition->APSoutheast2, Lex -> USWest1.)When"
2747,51399511,,3,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80026,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",I have used Face Rekognition other Lex always worked on APSoutheast2 region.
2748,51399511,,4,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",Check below image.
2749,51399511,,5,,"[{'score': 0.961411, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,TRUE,0.961411,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",Its seems like able set default only once.
2750,51399511,,6,,"[{'score': 0.848326, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.848326,FALSE,0,FALSE,0,TRUE,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""","How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance."""
2751,55029634,,0,,"[{'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,TRUE,"""So, I intend to perform a face to face verification using Azure Face Api. In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database. My question is: can store faces in a faceList in a way that I can use thepersistedFaceIdfor the faces coming from my database instead of performing a detect on the same picture just to get it's Id?""","""So, I intend to perform a face to face verification using Azure Face Api."
2752,55029634,,1,,"[{'score': 0.764487, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.764487,FALSE,0,FALSE,0,TRUE,"""So, I intend to perform a face to face verification using Azure Face Api. In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database. My question is: can store faces in a faceList in a way that I can use thepersistedFaceIdfor the faces coming from my database instead of performing a detect on the same picture just to get it's Id?""","In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database."
2753,55029634,,2,,"[{'score': 0.755313, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.755313,FALSE,0,FALSE,0,TRUE,"""So, I intend to perform a face to face verification using Azure Face Api. In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database. My question is: can store faces in a faceList in a way that I can use thepersistedFaceIdfor the faces coming from my database instead of performing a detect on the same picture just to get it's Id?""","My question is: can store faces in a faceList in a way that I can use thepersistedFaceIdfor the faces coming from my database instead of performing a detect on the same picture just to get it's Id?"""
2754,48924950,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?""","""I am trying to use a QR Code reader in multiple tabs."
2755,48924950,,1,,"[{'score': 0.560144, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.560144,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,FALSE,"""I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?""",After having problems with Google Vision API i tried to switch to zxing.
2756,48924950,,2,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?""",First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview.
2757,48924950,,3,,"[{'score': 0.601228, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.592037, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.601228,FALSE,0,FALSE,0,TRUE,0.592037,FALSE,0,FALSE,0,FALSE,"""I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?""","If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?"""
2758,52848759,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""","""I would like to analyse pictures from Instagram with Google Vision."
2759,52848759,,1,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""","Up to now, I collected posts from Instagram by hashtag."
2760,52848759,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""","So I have a CSV/XLS where the information per post is ""stored""."
2761,52848759,,3,,"[{'score': 0.656677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656677,FALSE,0,FALSE,0,TRUE,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""","Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision?"
2762,52848759,,4,,"[{'score': 0.904649, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904649,FALSE,0,FALSE,0,TRUE,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""","In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?"""
2763,54236946,,0,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""","""I'm using Microsoft Face API and I have many photos of persons there."
2764,54236946,,1,,"[{'score': 0.798497, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.798497,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""","I know that in azure database its saved only geometry of the face, not the whole photo."
2765,54236946,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""",Now I want to see that data.
2766,54236946,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""","I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group."
2767,54236946,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""","But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else."
2768,54236946,,5,,"[{'score': 0.784247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.784247,TRUE,0.704642,FALSE,0,TRUE,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""","And my question is:Can I see all my data which is saved in azure's database?"""
2769,51712998,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","""I am trying to run annotation on a video using the Google Cloud Video Intelligence API."
2770,51712998,,1,,"[{'score': 0.836129, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.836129,TRUE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine."
2771,51712998,,2,,"[{'score': 0.829929, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.829929,FALSE,0,FALSE,0,TRUE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields."
2772,51712998,,3,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275""."
2773,51712998,,4,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes."
2774,51712998,,5,,"[{'score': 0.678888, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.678888,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",The APIs will return a response with all parameters on some occasions and be missing one on others.
2775,51712998,,6,,"[{'score': 0.642626, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.642626,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.968123,FALSE,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""","I am wondering if there is anything I am missing here or something on my end that is causing this?"""
2776,51117724,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""","""When I submit a file upload form on my website, why would I get the following 404 NotFoundError?"
2777,51117724,,1,,"[{'score': 0.800733, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.660103, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.800733,FALSE,0,FALSE,0,TRUE,0.660103,FALSE,0,FALSE,0,FALSE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""","I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js,"
2778,51117724,,2,,"[{'score': 0.81444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.81444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""",I still get the same error.These are my routes (The uploadresume/post was having this error.):These
2779,51117724,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""",are my controllers.
2780,51117724,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""",The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js.
2781,51117724,,5,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""","Is the way I imported and added my bodyParser and urlencodedParser middleware correct?"""
2782,54794136,,0,,"[{'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai,"
2783,54794136,,1,,"[{'score': 0.731459, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731459,FALSE,0,FALSE,0,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?"
2784,54794136,,2,,"[{'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,FALSE,0,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"")."
2785,54794136,,3,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.898327, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,TRUE,0.898327,FALSE,0,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","Overhead fixed camera, similar/near-identical setup every time."
2786,54794136,,4,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","At the end of the day, I'll want the analysis to be local and not cloud-based."
2787,54794136,,5,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.809841, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.809841,FALSE,0,TRUE,0.58393,TRUE,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""","And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me."""
2788,46289477,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?""","""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake."
2789,46289477,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?""","What can I do to improve object detection?"""
2790,48135978,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""","""I am a programmer with minimal understanding of."
2791,48135978,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""",I have a large set ofimages from which I need to extract text through.
2792,48135978,,2,,"[{'score': 0.510629, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.510629,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""","Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data."
2793,48135978,,3,,"[{'score': 0.611045, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.611045,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,FALSE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""",Each block has a blank photograph section.
2794,48135978,,4,,"[{'score': 0.658517, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.502925, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.658517,FALSE,0,FALSE,0,TRUE,0.502925,FALSE,0,FALSE,0,FALSE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""",I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images.
2795,48135978,,5,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.751512,FALSE,0,TRUE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""",I want to join all user data images from 30-40 images into single image.
2796,48135978,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""","So its going to be like 900 user data blocks in one image.I request someexperts to help me out."""
2797,40020225,,0,,"[{'score': 0.843013, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.750047, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.750047,FALSE,0,TRUE,0.843013,TRUE,"""This question has been asked before () but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.So I would need to extract some text from a photo to perform some analysis on it, for an Android App. I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...Do you know if there is any API that would allow this kind of feature for standalone apps ?""","""This question has been asked before () but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.So I would need to extract some text from a photo to perform some analysis on it, for an Android App."
2798,40020225,,1,,"[{'score': 0.644449, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.606657, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.606657,FALSE,0,TRUE,0.644449,TRUE,"""This question has been asked before () but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.So I would need to extract some text from a photo to perform some analysis on it, for an Android App. I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...Do you know if there is any API that would allow this kind of feature for standalone apps ?""","I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...Do you know if there is any API that would allow this kind of feature for standalone apps ?"""
2799,37935601,,0,,"[{'score': 0.731949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731949,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""","""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange."
2800,37935601,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""","I was wondering if the highest score is actually 0, and 1 the lowest."
2801,37935601,,2,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.953007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,TRUE,0.953007,TRUE,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""","But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?"""
2802,47176260,,0,,"[{'score': 0.732258, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.732258,FALSE,0,FALSE,0,TRUE,"""I am building an face recognition php application with web camara Using Amazon Rekognition API.i did basic face matches using the API from below documents.Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches. I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.Is there any way to overcome this issue? i want to detect whether the captured image was captured real or from captured from another image?""","""I am building an face recognition php application with web camara Using Amazon Rekognition API.i did basic face matches using the API from below documents.Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches."
2803,47176260,,1,,"[{'score': 0.617627, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.621662, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.617627,FALSE,0,TRUE,0.621662,TRUE,"""I am building an face recognition php application with web camara Using Amazon Rekognition API.i did basic face matches using the API from below documents.Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches. I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.Is there any way to overcome this issue? i want to detect whether the captured image was captured real or from captured from another image?""","I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.Is there any way to overcome this issue?"
2804,47176260,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building an face recognition php application with web camara Using Amazon Rekognition API.i did basic face matches using the API from below documents.Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches. I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.Is there any way to overcome this issue? i want to detect whether the captured image was captured real or from captured from another image?""","i want to detect whether the captured image was captured real or from captured from another image?"""
2805,51354969,,0,,"[{'score': 0.660207, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660207,FALSE,0,TRUE,"""I need a count of all thecarsincluded in a image with the Google Cloud Vision API in Python. I take only the labels of the image right now.""","""I need a count of all thecarsincluded in a image with the Google Cloud Vision API in Python."
2806,51354969,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need a count of all thecarsincluded in a image with the Google Cloud Vision API in Python. I take only the labels of the image right now.""","I take only the labels of the image right now."""
2807,53671813,,0,,"[{'score': 0.744572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.744572,FALSE,0,FALSE,0,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""","""I have trained a custom Google Cloud Vision model using AutoML."
2808,53671813,,1,,"[{'score': 0.583856, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.583856,FALSE,0,FALSE,0,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API.
2809,53671813,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,TRUE,0.704642,FALSE,0,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""","This works perfectly fine, however the time it takes to get a response is 13 seconds."
2810,53671813,,3,,"[{'score': 0.691695, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.691695,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,FALSE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",This seems extremely slow and inefficient to me.
2811,53671813,,4,,"[{'score': 0.739487, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.739487,FALSE,0,FALSE,0,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""","I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this?"
2812,53671813,,5,,"[{'score': 0.839577, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.839577,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g.
2813,53671813,,6,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""","Imagenet and build an API around that."""
2814,46848590,,0,,"[{'score': 0.708196, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708196,FALSE,0,FALSE,0,TRUE,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""","""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size."
2815,46848590,,1,,"[{'score': 0.566506, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.670709, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.566506,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670709,FALSE,0,TRUE,0.5538,FALSE,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""","Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated."
2816,46848590,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""","Thanks in advance."""
2817,50237770,,0,,"[{'score': 0.780888, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.780888,FALSE,0,FALSE,0,TRUE,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""","""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces."
2818,50237770,,1,,"[{'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,FALSE,0,TRUE,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",The images are not corrupt and have been tested using jpeginfo.
2819,50237770,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",They are then converted to binary and send to rekognition via the sdk.
2820,50237770,,3,,"[{'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,FALSE,0,TRUE,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging?
2821,50237770,,4,,"[{'score': 0.887937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.887937,FALSE,0,TRUE,0.873263,TRUE,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""","Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480"""
2822,44517510,,0,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80026,FALSE,0,TRUE,"""I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?""","""I have never used Jmeter before."
2823,44517510,,1,,"[{'score': 0.598502, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.598502,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?""",I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error.
2824,44517510,,2,,"[{'score': 0.807761, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.807761,FALSE,0,FALSE,0,TRUE,"""I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?""","My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?"""
2825,51493545,,0,,"[{'score': 0.61732, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61732,FALSE,0,FALSE,0,TRUE,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""","""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word."
2826,51493545,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""",After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned.
2827,51493545,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""","For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things"
2828,51493545,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""",I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line.
2829,51493545,,4,,"[{'score': 0.854284, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.854284,TRUE,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""","I got an image something like this..Not sure how to proceed and get the groups on single line.."""
2830,50657314,,0,,"[{'score': 0.69934, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69934,FALSE,0,FALSE,0,TRUE,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""","""I am trying to use Google Cloud Vision via a rest http request using c#."
2831,50657314,,1,,"[{'score': 0.728404, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.591808, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.728404,FALSE,0,FALSE,0,TRUE,0.591808,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""","As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created."
2832,50657314,,2,,"[{'score': 0.60402, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.60402,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""","I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message."
2833,50657314,,3,,"[{'score': 0.822952, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.822952,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""","Same goes with curl:What am I missing out?"""
2834,44966573,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,TRUE,0.615352,TRUE,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""","""I am having some issues using the cloudvisreq python script for the Google Vision Python API."
2835,44966573,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""","I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up."
2836,44966573,,2,,"[{'score': 0.708528, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,TRUE,0.708528,FALSE,0,FALSE,0,FALSE,0,TRUE,0.534455,FALSE,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""","I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data."
2837,44966573,,3,,"[{'score': 0.594723, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.594723,FALSE,0,FALSE,0,TRUE,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""","The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor"""
2838,39029331,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will workin atleast oneof these languages. (the webpage is being hosted at google appengine)what i found:1:google cloud vision API , i also found their project on githubthe code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)2:i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all.3:i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ? can someone help me ?thanks""","""i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will workin atleast oneof these languages."
2839,39029331,,1,,"[{'score': 0.55324, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.55324,FALSE,0,FALSE,0,TRUE,"""i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will workin atleast oneof these languages. (the webpage is being hosted at google appengine)what i found:1:google cloud vision API , i also found their project on githubthe code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)2:i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all.3:i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ? can someone help me ?thanks""","(the webpage is being hosted at google appengine)what i found:1:google cloud vision API , i also found their project on githubthe code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)2:i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all.3:i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ?"
2840,39029331,,2,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will workin atleast oneof these languages. (the webpage is being hosted at google appengine)what i found:1:google cloud vision API , i also found their project on githubthe code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)2:i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all.3:i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ? can someone help me ?thanks""","can someone help me ?thanks"""
2841,37985715,,0,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""","""I want to useto generate thumbnails for my Wordpress site."
2842,37985715,,1,,"[{'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,TRUE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""","I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ?"
2843,37985715,,2,,"[{'score': 0.585604, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.767921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.585604,FALSE,0,FALSE,0,TRUE,0.767921,FALSE,0,FALSE,0,FALSE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""",It returns a thumbnail in really bad quality and default 500x500px.
2844,37985715,,3,,"[{'score': 0.600854, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.908301, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.600854,FALSE,0,FALSE,0,FALSE,0,TRUE,0.908301,FALSE,0,TRUE,0.716301,FALSE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""",Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right!
2845,37985715,,4,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.571959, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,TRUE,0.571959,FALSE,0,TRUE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""","Now the cropping is correct, but i got a huge problem with the quality!"
2846,37985715,,5,,"[{'score': 0.745225, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.745225,FALSE,0,FALSE,0,TRUE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""",I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users.
2847,37985715,,6,,"[{'score': 0.645756, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.880052, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.645756,TRUE,0.880052,FALSE,0,FALSE,0,FALSE,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""","They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct."""
2848,50580902,,0,,"[{'score': 0.824794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824794,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""","""I'm using Google Vision API for text & logo detection."
2849,50580902,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""","When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking"
2850,50580902,,2,,"[{'score': 0.667816, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.867767, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.752532, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.667816,FALSE,0,FALSE,0,TRUE,0.752532,FALSE,0,TRUE,0.867767,FALSE,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""","at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue?"
2851,50580902,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""","I would like to reach 300req/min threshold (at least)."""
2852,56068100,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","""I made an Android library that I use locally via .aarIn"
2853,56068100,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar"
2854,56068100,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""",file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar
2855,56068100,,3,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar"
2856,56068100,,4,,"[{'score': 0.707547, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.961633, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.707547,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961633,FALSE,0,FALSE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","doesnt work, at all.2-"
2857,56068100,,5,,"[{'score': 0.786445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786445,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar"
2858,56068100,,6,,"[{'score': 0.59928, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.718921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.59928,FALSE,0,FALSE,0,TRUE,0.718921,FALSE,0,FALSE,0,FALSE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar"
2859,56068100,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""","work the same as the library imported via source code?"""
2860,49181313,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********could you please help with an issue I am having?I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?""","""***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********could"
2861,49181313,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********could you please help with an issue I am having?I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?""","you please help with an issue I am having?I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?"""
2862,44740363,,0,,"[{'score': 0.961411, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961411,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""","""Recently something about the Google Vision API changed."
2863,44740363,,1,,"[{'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,FALSE,0,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",I am using it to recognize text on receipts.
2864,44740363,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.97759,FALSE,0,FALSE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",All good until now.
2865,44740363,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""","Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past)."
2866,44740363,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""","I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries."
2867,44740363,,5,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",However when I post from my app I get an array of 113 entries.
2868,44740363,,6,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.615352,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response.
2869,44740363,,7,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.998976,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",Is that even possible?
2870,44740363,,8,,"[{'score': 0.793846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.793846,FALSE,0,TRUE,0.976993,TRUE,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""","Has anyone stumbled upon this kind of issue before?"""
2871,49868678,,0,,"[{'score': 0.685315, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.632275, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.685315,FALSE,0,FALSE,0,FALSE,0,TRUE,0.632275,FALSE,0,FALSE,0,FALSE,"""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much""","""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis."
2872,49868678,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much""","I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket?"
2873,49868678,,2,,"[{'score': 0.516399, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.511851, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.99291, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.511851,TRUE,0.516399,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.99291,FALSE,"""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much""","I hardly find any, can anyone please help?"
2874,49868678,,3,,"[{'score': 1.0, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,1,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much""","thank you so much"""
2875,44845273,,0,,"[{'score': 0.87766, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87766,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""","""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof."
2876,44845273,,1,,"[{'score': 0.810878, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.810878,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""",Is there a way to crop the image in such a way that only the part with concentrated text is retained?
2877,44845273,,2,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.75152,TRUE,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""","I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:"""
2878,56175881,,0,,"[{'score': 0.792052, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.730335, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.730335,FALSE,0,TRUE,0.792052,TRUE,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""","""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels."
2879,56175881,,1,,"[{'score': 0.990161, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.974578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.974578,FALSE,0,TRUE,0.990161,TRUE,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""","For example, it may return ""dress"" or ""Dress""."
2880,56175881,,2,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,TRUE,0.984352,TRUE,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""","Does anyone know if this signifies any difference?"""
2881,52046473,,0,,"[{'score': 0.759933, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.759933,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,FALSE,"""The aws docs for this are really confusing. Following the steps from here, I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess.""","""The aws docs for this are really confusing."
2882,52046473,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The aws docs for this are really confusing. Following the steps from here, I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess.""","Following the steps from here, I created the awsconfiguration.json"
2883,52046473,,2,,"[{'score': 0.796955, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.662372, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.656319, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.796955,FALSE,0,FALSE,0,TRUE,0.662372,FALSE,0,TRUE,0.656319,FALSE,"""The aws docs for this are really confusing. Following the steps from here, I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess.""","using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what."
2884,52046473,,3,,"[{'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.976993,TRUE,"""The aws docs for this are really confusing. Following the steps from here, I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess.""","This whole setup just seems to be a mess."""
2885,56032884,,0,,"[{'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,FALSE,0,TRUE,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""","""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back."
2886,56032884,,1,,"[{'score': 0.509192, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.509192,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""","I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway."
2887,56032884,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""",I have clicked onEnable CORSit works for my other API Resources but not this one.
2888,56032884,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""","after clicking that I then clickDeploy API.Angular code:"""
2889,48527260,,0,,"[{'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,TRUE,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""","""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage)."
2890,48527260,,1,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,TRUE,0.681699,TRUE,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""","However, I'm curious about how it's authenticating."
2891,48527260,,2,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,TRUE,0.647986,TRUE,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",In the python script that I'm running locally I do not provide any authentication information.
2892,48527260,,3,,"[{'score': 0.626312, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.515797, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.626312,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515797,FALSE,0,FALSE,0,FALSE,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""","From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?"
2893,48527260,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""","Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:"""
2894,45654972,,0,,"[{'score': 0.708653, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.567542, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.708653,FALSE,0,TRUE,0.567542,TRUE,"""I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).The problem is that I have to train at least 700 images but I have just something like 30.I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games. Do you know a generator like this? Or do you have a good idea for me?Thanks.""","""I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).The problem is that I have to train at least 700 images but I have just something like 30.I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games."
2895,45654972,,1,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,FALSE,0,TRUE,0.681699,TRUE,"""I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).The problem is that I have to train at least 700 images but I have just something like 30.I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games. Do you know a generator like this? Or do you have a good idea for me?Thanks.""",Do you know a generator like this?
2896,45654972,,2,,"[{'score': 0.833168, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.833168,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.822231,FALSE,"""I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).The problem is that I have to train at least 700 images but I have just something like 30.I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games. Do you know a generator like this? Or do you have a good idea for me?Thanks.""","Or do you have a good idea for me?Thanks."""
2897,56099062,,0,,"[{'score': 0.614233, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.614233,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,FALSE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""","""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one?"
2898,56099062,,1,,"[{'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,FALSE,0,TRUE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",as i really do not know what it is.
2899,56099062,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",I also want to createplaylistsin my application.
2900,56099062,,3,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",is there any proper userguide to use it?
2901,56099062,,4,,"[{'score': 0.529304, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.529304,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,FALSE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application.
2902,56099062,,5,,"[{'score': 0.834759, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.834759,FALSE,0,FALSE,0,TRUE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""","I am really a beginner with android.If mediaStore is not right, then guide me what is."
2903,56099062,,6,,"[{'score': 0.734027, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.734027,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",I want a simple music player that will fetch music from your internal storage and let people create their playlist.
2904,56099062,,7,,"[{'score': 0.576715, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.570746, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.576715,FALSE,0,FALSE,0,TRUE,0.570746,FALSE,0,FALSE,0,FALSE,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""","this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood."""
2905,35651277,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library.""","""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string."
2906,35651277,,1,,"[{'score': 0.575112, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.575112,FALSE,0,FALSE,0,TRUE,"""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library.""","I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library."""
2907,50490782,,0,,"[{'score': 0.505973, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.505973,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler. To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get. For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?""","""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler."
2908,50490782,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler. To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get. For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?""","To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get."
2909,50490782,,2,,"[{'score': 0.841451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841451,FALSE,0,FALSE,0,TRUE,"""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler. To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get. For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?""","For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?"""
2910,53961202,,0,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,FALSE,0,TRUE,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""","""I am using the Microsoft Face API to detect faces and emotions in Android."
2911,53961202,,1,,"[{'score': 0.841998, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.841998,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.733853,FALSE,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries.
2912,53961202,,2,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""","For example, when I choose to get the emotion attributes of a face that issmiling,returns2."
2913,53961202,,3,,"[{'score': 0.806276, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.806276,FALSE,0,FALSE,0,TRUE,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""","When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing?"
2914,53961202,,4,,"[{'score': 0.515711, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.75131, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75131,FALSE,0,TRUE,0.515711,TRUE,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""","I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to."""
2915,48690047,,0,,"[{'score': 0.738513, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.738513,FALSE,0,FALSE,0,TRUE,"""I am testing Google Cloud Vision with Java in Eclipse.I have copied the java code fromWhere can I download the .JAR's from so that everything compiles properly?Google themselves say (see the link above)However, I am not using Maven (nor Gradle or SBT which are their other suggestions).So I thought to open a New Maven project in Eclipse, which would then download all the JAR's automatically, and then copy them across to my project.So I did ""new Maven Project"" in Eclipse and then when it said ""enter a group id for the artifact"" I entered the details from Google as I pasted above, but it did not download anything.Any ideas how I can get the JARs so that the code will compile?The pom.xml file which was auto-generated by Eclipse is""","""I am testing Google Cloud Vision with Java in Eclipse.I have copied the java code fromWhere can I download the .JAR's from so that everything compiles properly?Google themselves say (see the link above)However, I am not using Maven (nor Gradle or SBT which are their other suggestions).So"
2916,48690047,,1,,"[{'score': 0.665333, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.665333,FALSE,0,FALSE,0,TRUE,"""I am testing Google Cloud Vision with Java in Eclipse.I have copied the java code fromWhere can I download the .JAR's from so that everything compiles properly?Google themselves say (see the link above)However, I am not using Maven (nor Gradle or SBT which are their other suggestions).So I thought to open a New Maven project in Eclipse, which would then download all the JAR's automatically, and then copy them across to my project.So I did ""new Maven Project"" in Eclipse and then when it said ""enter a group id for the artifact"" I entered the details from Google as I pasted above, but it did not download anything.Any ideas how I can get the JARs so that the code will compile?The pom.xml file which was auto-generated by Eclipse is""","I thought to open a New Maven project in Eclipse, which would then download all the JAR's automatically, and then copy them across to my project.So I did ""new Maven Project"" in Eclipse and then when it said ""enter a group id for the artifact"" I entered the details from Google as I pasted above, but it did not download anything.Any ideas how I can get the JARs so that the code will compile?The pom.xml file which was auto-generated by Eclipse is"""
2917,50907959,,0,,"[{'score': 0.73362, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73362,FALSE,0,FALSE,0,TRUE,"""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly: null""","""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly:"
2918,50907959,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly: null""","null"""
2919,55037756,,0,,"[{'score': 0.787944, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.657517, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.657517,TRUE,0.787944,FALSE,0,TRUE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""","""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2'"
2920,55037756,,1,,"[{'score': 0.991736, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.991736,FALSE,0,FALSE,0,TRUE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""",module not found.
2921,55037756,,2,,"[{'score': 0.942582, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.942582,FALSE,0,TRUE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""",I've pip installed all the dependcies.
2922,55037756,,3,,"[{'score': 0.838593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.838593,FALSE,0,FALSE,0,TRUE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""","To check this, I've imported google.oauth2"
2923,55037756,,4,,"[{'score': 0.712629, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.804906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804906,TRUE,0.712629,FALSE,0,TRUE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""",module in command line Python and It's working there.
2924,55037756,,5,,"[{'score': 0.540444, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.540444,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""","Please help me with this."""
2925,47946770,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.I import the Google Cloud vision client library as followand then I callHowever I get an errorTypeError: vision.detectText is not a functionInitially I usedfrom this examplebut I got the exact same error. I then read that textDetection has been replaced by detectText but no more successThanks in advance""","""I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.I import the Google Cloud vision client library as followand then I callHowever I get an errorTypeError: vision.detectText is not a functionInitially I usedfrom this examplebut I got the exact same error."
2926,47946770,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.I import the Google Cloud vision client library as followand then I callHowever I get an errorTypeError: vision.detectText is not a functionInitially I usedfrom this examplebut I got the exact same error. I then read that textDetection has been replaced by detectText but no more successThanks in advance""","I then read that textDetection has been replaced by detectText but no more successThanks in advance"""
2927,54916175,,0,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,FALSE,0,TRUE,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""","""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label."
2928,54916175,,1,,"[{'score': 0.714297, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.719538, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.714297,FALSE,0,FALSE,0,TRUE,0.719538,FALSE,0,FALSE,0,FALSE,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""","However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:"""
2929,51767916,,0,,"[{'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.745225, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.745225,FALSE,0,TRUE,0.727988,TRUE,"""I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none"". But when I try the same image with Google the vision API it gives me a good response.Is it possible to integrate tika with Google vision API? If yes then how using python?""","""I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none""."
2930,51767916,,1,,"[{'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.534455,TRUE,"""I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none"". But when I try the same image with Google the vision API it gives me a good response.Is it possible to integrate tika with Google vision API? If yes then how using python?""",But when I try the same image with Google the vision API it gives me a good response.Is it possible to integrate tika with Google vision API?
2931,51767916,,2,,"[{'score': 0.972852, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.972852,FALSE,0,FALSE,0,TRUE,"""I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none"". But when I try the same image with Google the vision API it gives me a good response.Is it possible to integrate tika with Google vision API? If yes then how using python?""","If yes then how using python?"""
2932,53954761,,0,,"[{'score': 0.510804, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.950378, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.510804,FALSE,0,FALSE,0,TRUE,0.950378,FALSE,0,FALSE,0,FALSE,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""","""I have an issue with my logic trying to invoke theRecognition Compare Faces api using."
2933,53954761,,1,,"[{'score': 0.841393, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.841393,FALSE,0,TRUE,0.873263,TRUE,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""","There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?"""
2934,42245229,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am working on parse server with AWS , i am using microsoft face API  '' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger"" when i test the code there for the Parse.Cloud.httpRequest() all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text) not as JSON object but as char array , that means i get array of thousands chars.""","""i am working on parse server with AWS , i am using microsoft face API  '' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger"" when i test the code there for the Parse.Cloud.httpRequest()"
2935,42245229,,1,,"[{'score': 0.631014, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.631014,FALSE,0,FALSE,0,TRUE,"""i am working on parse server with AWS , i am using microsoft face API  '' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger"" when i test the code there for the Parse.Cloud.httpRequest() all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text) not as JSON object but as char array , that means i get array of thousands chars.""",all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text)
2936,42245229,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am working on parse server with AWS , i am using microsoft face API  '' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger"" when i test the code there for the Parse.Cloud.httpRequest() all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text) not as JSON object but as char array , that means i get array of thousands chars.""","not as JSON object but as char array , that means i get array of thousands chars."""
2937,46630157,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""","""I want to try one of google vision api on android."
2938,46630157,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false.
2939,46630157,,2,,"[{'score': 0.932977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.932977,FALSE,0,FALSE,0,TRUE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",I looked to other questions and found nothing that helped me.
2940,46630157,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""","(Device storage left 4 gb, so it's not storage issue)."
2941,46630157,,4,,"[{'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.916667, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.916667,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",What I'm missing?
2942,46630157,,5,,"[{'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638987,FALSE,0,TRUE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly.
2943,46630157,,6,,"[{'score': 0.523574, 'tone_id': 'anger', 'tone_name': 'Anger'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.523574,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""","So why G FLex2 fails?"""
2944,51189901,,0,,"[{'score': 0.939052, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.939052,TRUE,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""","""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything."
2945,51189901,,1,,"[{'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.976993,TRUE,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""","Also it does not go in any, not even."
2946,51189901,,2,,"[{'score': 0.861677, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.649361, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.861677,FALSE,0,FALSE,0,TRUE,0.649361,FALSE,0,TRUE,0.704683,FALSE,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""","Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page."""
2947,49890225,,0,,"[{'score': 0.524333, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.524333,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""","""i have followed the steps of the documentation but i received:The code fails en."
2948,49890225,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""",Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user.
2949,49890225,,2,,"[{'score': 0.640174, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.57706, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.640174,FALSE,0,FALSE,0,TRUE,0.57706,FALSE,0,FALSE,0,FALSE,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""","Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)"""
2950,56052040,,0,,"[{'score': 0.587457, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587457,FALSE,0,FALSE,0,TRUE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""","""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image."
2951,56052040,,1,,"[{'score': 0.657159, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.657159,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",so first thing is the empty vertices.
2952,56052040,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",what are they?
2953,56052040,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",also i was hoping all the pairs will have a x and y value to draw on 2D space.
2954,56052040,,4,,"[{'score': 0.767713, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.767713,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.856622,FALSE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",but some has only x and some has only y.finally how should i get my final image?
2955,56052040,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is
2956,56052040,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""","there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?"""
2957,42229158,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""","""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above."
2958,42229158,,1,,"[{'score': 0.824794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.824794,FALSE,0,FALSE,0,TRUE,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""",I am using Service Account key file to authenticate.
2959,42229158,,2,,"[{'score': 0.651326, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.804675, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.651326,FALSE,0,TRUE,0.804675,TRUE,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""","As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set."
2960,42229158,,3,,"[{'score': 0.904038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904038,FALSE,0,FALSE,0,TRUE,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""","Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned."""
2961,50642047,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am calling the google vision OCR api from a spring boot maven project to extract text from an image.The call on the line below(i.e below line 80) does not return a response.It gets stuck for a very long time after which it throws the below exception.The console has the below logs printed. Kindly help me  resolve the issue.""","""I am calling the google vision OCR api from a spring boot maven project to extract text from an image.The call on the line below(i.e"
2962,50642047,,1,,"[{'score': 0.585893, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.585893,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am calling the google vision OCR api from a spring boot maven project to extract text from an image.The call on the line below(i.e below line 80) does not return a response.It gets stuck for a very long time after which it throws the below exception.The console has the below logs printed. Kindly help me  resolve the issue.""",below line 80) does not return a response.It gets stuck for a very long time after which it throws the below exception.The console has the below logs printed.
2963,50642047,,2,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""I am calling the google vision OCR api from a spring boot maven project to extract text from an image.The call on the line below(i.e below line 80) does not return a response.It gets stuck for a very long time after which it throws the below exception.The console has the below logs printed. Kindly help me  resolve the issue.""","Kindly help me  resolve the issue."""
2964,51811837,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""","""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it."
2965,51811837,,1,,"[{'score': 0.646387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646387,FALSE,0,TRUE,0.91961,TRUE,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""",It went something like this :I don't think there's a method like that anymore.
2966,51811837,,2,,"[{'score': 0.573015, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.596761, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.573015,FALSE,0,FALSE,0,TRUE,0.596761,FALSE,0,FALSE,0,FALSE,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""","I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app?"
2967,51811837,,3,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""",Whether it be with my api key or with the service account json.
2968,51811837,,4,,"[{'score': 0.581519, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.581519,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""","I've been stuck on this for several days."""
2969,52048829,,0,,"[{'score': 0.667144, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.667144,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""","""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project."
2970,52048829,,1,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.752532, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.752532,FALSE,0,TRUE,0.57374,TRUE,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project.
2971,52048829,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""","However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do"
2972,52048829,,3,,"[{'score': 0.674728, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,TRUE,0.674728,FALSE,0,TRUE,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""","I really have to change the environment variable every time I change the project?"""
2973,43129395,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have gone through all the documentation provided for the running Django Application on app engine.I have a Django Application where I am using Vision and Storage clients and my app name is pvd.I have been constantly getting below errors in error logs.Below is my app.yamlBelow is my requirement.txtFor deploying I am using:What am I doing wrong?""","""I have gone through all the documentation provided for the running Django Application on app engine.I have a Django Application where I am using Vision and Storage clients and my app name is pvd.I have been constantly getting below errors in error logs.Below is my app.yamlBelow is my requirement.txtFor"
2974,43129395,,1,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I have gone through all the documentation provided for the running Django Application on app engine.I have a Django Application where I am using Vision and Storage clients and my app name is pvd.I have been constantly getting below errors in error logs.Below is my app.yamlBelow is my requirement.txtFor deploying I am using:What am I doing wrong?""","deploying I am using:What am I doing wrong?"""
2975,47104465,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hello guys i try to implement a registration functionality using asp.net identity.One of my required properties is the new user's photo  which  i pass as an argument in a  action method where finally  i callmethod of azure face api.I grab the photo using ain my view and then in account controller of identity in register action method i use ato read it.The problem is thatneeds as first argument a stream object or a string(imagePath) but in my case i cant figure it out how i can give a stream object or the image pathdoesn't retrieve the image path but to get the image i need this type of object.And then even if i try to passas argument i get null by break pointing thecommand lineTo be specificAccount Controllerand myFaceRecognitionControllerAs you can see i just check if validation api find any face and just return true to complete the registrationany thoughts on how i can overcome this??""","""Hello guys i try to implement a registration functionality using asp.net"
2976,47104465,,1,,"[{'score': 0.801159, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801159,FALSE,0,FALSE,0,TRUE,"""Hello guys i try to implement a registration functionality using asp.net identity.One of my required properties is the new user's photo  which  i pass as an argument in a  action method where finally  i callmethod of azure face api.I grab the photo using ain my view and then in account controller of identity in register action method i use ato read it.The problem is thatneeds as first argument a stream object or a string(imagePath) but in my case i cant figure it out how i can give a stream object or the image pathdoesn't retrieve the image path but to get the image i need this type of object.And then even if i try to passas argument i get null by break pointing thecommand lineTo be specificAccount Controllerand myFaceRecognitionControllerAs you can see i just check if validation api find any face and just return true to complete the registrationany thoughts on how i can overcome this??""","identity.One of my required properties is the new user's photo  which  i pass as an argument in a  action method where finally  i callmethod of azure face api.I grab the photo using ain my view and then in account controller of identity in register action method i use ato read it.The problem is thatneeds as first argument a stream object or a string(imagePath) but in my case i cant figure it out how i can give a stream object or the image pathdoesn't retrieve the image path but to get the image i need this type of object.And then even if i try to passas argument i get null by break pointing thecommand lineTo be specificAccount Controllerand myFaceRecognitionControllerAs you can see i just check if validation api find any face and just return true to complete the registrationany thoughts on how i can overcome this??"""
2977,56362468,,0,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:This is the part of the image with the issue:As you can see, there are overlapping rectangles on the 'M' and the 'a'.The result is something like this:   '''MMaay 10, 2019'''Why this is happening?Is there a way to fix it?I tried to change the image format from bmp to png. The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'.""","""I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:This is the part of the image with the issue:As you can see, there are overlapping rectangles on the 'M' and the 'a'.The result is something like this:   '''MMaay 10, 2019'''Why this is happening?Is there a way to fix it?I tried to change the image format from bmp to png."
2978,56362468,,1,,"[{'score': 0.500977, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.500977,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:This is the part of the image with the issue:As you can see, there are overlapping rectangles on the 'M' and the 'a'.The result is something like this:   '''MMaay 10, 2019'''Why this is happening?Is there a way to fix it?I tried to change the image format from bmp to png. The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'.""","The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'."""
2979,54513958,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to use the Microsoft Custom Vision. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:""","""I am trying to use the Microsoft Custom Vision."
2980,54513958,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Microsoft Custom Vision. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:""",I need to make an HTTP request to send the image to be analyzed.
2981,54513958,,2,,"[{'score': 0.609749, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.609749,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Microsoft Custom Vision. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:""","I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:"""
2982,52285908,,0,,"[{'score': 0.512648, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.512648,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""","""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error)."
2983,52285908,,1,,"[{'score': 0.677069, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677069,FALSE,0,TRUE,0.873263,TRUE,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",I can even see warnings from the gem:Isn't require: false not supposed to load the Gem?
2984,52285908,,2,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""","And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?"""
2985,51133277,,0,,"[{'score': 0.539105, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.798336, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.539105,FALSE,0,FALSE,0,TRUE,0.798336,FALSE,0,FALSE,0,FALSE,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""","""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code."
2986,51133277,,1,,"[{'score': 0.539581, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.539581,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""","In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in."
2987,51133277,,2,,"[{'score': 0.929993, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.929993,FALSE,0,FALSE,0,TRUE,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""","However, the solution offered in that discussion did not solve the problem I am encountering."
2988,51133277,,3,,"[{'score': 0.754865, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.754865,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.75152,FALSE,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""","I would appreciate any help and tips that can solve this issue."""
0,42176137,,0,,"[{'score': 0.626861, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.626861,FALSE,0,FALSE,0,TRUE,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""","""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to."
1,42176137,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""","So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?"
2,42176137,,2,,"[{'score': 0.752684, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.752684,FALSE,0,FALSE,0,TRUE,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also
3,42176137,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""","I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear."
4,42176137,,4,,"[{'score': 0.544385, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.805137, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.544385,FALSE,0,FALSE,0,TRUE,0.805137,FALSE,0,FALSE,0,FALSE,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""","The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!"""
5,45245748,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""","""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified."
6,45245748,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""","Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in."
7,45245748,,2,,"[{'score': 0.630413, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.866306, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.630413,FALSE,0,FALSE,0,TRUE,0.866306,FALSE,0,FALSE,0,FALSE,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""",Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection.
8,45245748,,3,,"[{'score': 0.608527, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.742086, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.608527,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.742086,FALSE,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""",So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection?
9,45245748,,4,,"[{'score': 0.589643, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.541591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.589643,FALSE,0,FALSE,0,TRUE,0.541591,FALSE,0,FALSE,0,FALSE,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""","I have listed the error list below forIndexFacesin case it helps."""
10,40032758,,0,,"[{'score': 0.890767, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890767,FALSE,0,FALSE,0,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running."
11,40032758,,1,,"[{'score': 0.82452, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.696092, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.82452,FALSE,0,FALSE,0,TRUE,0.696092,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead.
12,40032758,,2,,"[{'score': 0.549202, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.563072, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.549202,TRUE,0.563072,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","In the spirit of hacking through this, that's what I did to get it mostly working*."
13,40032758,,3,,"[{'score': 0.896651, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.601471, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.896651,FALSE,0,FALSE,0,TRUE,0.601471,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using:"
14,40032758,,4,,"[{'score': 0.70018, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.942582, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.70018,FALSE,0,FALSE,0,TRUE,0.620279,TRUE,0.942582,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",This code always returns the error.
15,40032758,,5,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.5538,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Theoretically it should work because the URL is public.
16,40032758,,6,,"[{'score': 0.669696, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.669696,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns."
17,40032758,,7,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL)."
18,40032758,,8,,"[{'score': 0.842528, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.842528,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","Unfortunately, that gives the PEM error from above."
19,40032758,,9,,"[{'score': 0.64222, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.64222,FALSE,0,FALSE,0,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly."
20,40032758,,10,,"[{'score': 0.617467, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.617467,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,FALSE,0,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff.
21,40032758,,11,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",These images aren't more than 1MB.*The
22,40032758,,12,,"[{'score': 0.697148, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.746119, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.697148,FALSE,0,FALSE,0,TRUE,0.746119,FALSE,0,TRUE,0.704683,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem."
23,40032758,,13,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.568262, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568262,FALSE,0,TRUE,0.88939,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box?"
24,40032758,,14,,"[{'score': 0.562457, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.747419, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.562457,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747419,FALSE,0,TRUE,0.716301,FALSE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome."
25,40032758,,15,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,TRUE,0.618451,FALSE,0,TRUE,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""","Any clues you all can provide would be greatly appreciated."""
26,50358189,,0,,"[{'score': 0.646247, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.646247,FALSE,0,FALSE,0,TRUE,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""","""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg."
27,50358189,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""","Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before?"
28,50358189,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""","Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?"""
29,47182750,,0,,"[{'score': 0.547206, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.547206,FALSE,0,FALSE,0,TRUE,"""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst.""","""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images."
30,47182750,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst.""","Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst."""
31,47157038,,0,,"[{'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638987,FALSE,0,TRUE,"""I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error.""","""I am not getting the labels and other properties on all the URL's when using the Google Vision API."
32,47157038,,1,,"[{'score': 0.697086, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.697086,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786991,FALSE,"""I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error.""","It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error."""
33,45341208,,0,,"[{'score': 0.73362, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73362,FALSE,0,FALSE,0,TRUE,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""","""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs."
34,45341208,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""",See the background.js
35,45341208,,2,,"[{'score': 0.676717, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.798791, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.676717,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.798791,FALSE,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""","file inHow could I increase the limit?I have tried to addorand it did not work."""
36,51216224,,0,,"[{'score': 0.568262, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568262,FALSE,0,FALSE,0,TRUE,"""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally? As a third party or globally? How will it work on Google when I deploy?""","""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally?"
37,51216224,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally? As a third party or globally? How will it work on Google when I deploy?""",As a third party or globally?
38,51216224,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally? As a third party or globally? How will it work on Google when I deploy?""","How will it work on Google when I deploy?"""
39,49450500,,0,,"[{'score': 0.880245, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.574817, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.574817,FALSE,0,TRUE,0.880245,TRUE,"""This is my first question here so I'll try to be as relevant as possible.I am interested in using Cloud Vision to process some documents, as I need OCR capabilities. I also happen to need bar code reading, which I currently have implemented using ZXing.I stumbled upon the BARCODE blocktype in the OCR () but I did not manage to produce such a block, even with an image containingonlya bar code.Hence the question: is the feature implemented, and if so, how can we get it to work ? Thank you for your time !Note:I have seen those related questions:But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.Edit:I have tried for example with:I also tried with""","""This is my first question here so I'll try to be as relevant as possible.I am interested in using Cloud Vision to process some documents, as I need OCR capabilities."
40,49450500,,1,,"[{'score': 0.845878, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.845878,FALSE,0,FALSE,0,TRUE,"""This is my first question here so I'll try to be as relevant as possible.I am interested in using Cloud Vision to process some documents, as I need OCR capabilities. I also happen to need bar code reading, which I currently have implemented using ZXing.I stumbled upon the BARCODE blocktype in the OCR () but I did not manage to produce such a block, even with an image containingonlya bar code.Hence the question: is the feature implemented, and if so, how can we get it to work ? Thank you for your time !Note:I have seen those related questions:But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.Edit:I have tried for example with:I also tried with""","I also happen to need bar code reading, which I currently have implemented using ZXing.I stumbled upon the BARCODE blocktype in the OCR () but I did not manage to produce such a block, even with an image containingonlya bar code.Hence the question: is the feature implemented, and if so, how can we get it to work ?"
41,49450500,,2,,"[{'score': 0.787156, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.787156,FALSE,0,FALSE,0,TRUE,"""This is my first question here so I'll try to be as relevant as possible.I am interested in using Cloud Vision to process some documents, as I need OCR capabilities. I also happen to need bar code reading, which I currently have implemented using ZXing.I stumbled upon the BARCODE blocktype in the OCR () but I did not manage to produce such a block, even with an image containingonlya bar code.Hence the question: is the feature implemented, and if so, how can we get it to work ? Thank you for your time !Note:I have seen those related questions:But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.Edit:I have tried for example with:I also tried with""","Thank you for your time !Note:I have seen those related questions:But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.Edit:I have tried for example with:I also tried with"""
42,56010701,,0,,"[{'score': 0.519491, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.519491,FALSE,0,FALSE,0,TRUE,"""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image.""","""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form."
43,56010701,,1,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image.""",This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory.
44,56010701,,2,,"[{'score': 0.616179, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.616179,FALSE,0,FALSE,0,TRUE,"""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image.""","In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image."""
45,37741299,,0,,"[{'score': 0.69431, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.69431,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help""","""Have been trying to read data out of an Govt."
46,37741299,,1,,"[{'score': 0.617138, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.617138,FALSE,0,FALSE,0,TRUE,"""Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help""","Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help"""
47,50415121,,0,,"[{'score': 0.806335, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.806335,FALSE,0,FALSE,0,TRUE,"""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes.""","""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays?"
48,50415121,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes.""","The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes."""
49,41167490,,0,,"[{'score': 0.662817, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.662817,FALSE,0,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errorsI found a similar question asked(except for the change in the third question) but no answers.I tried deleting thefile to fix the last error, but no luck""","""I am trying to implement Microsoft emotion api in C# using code available on github."
50,41167490,,1,,"[{'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.92125,FALSE,0,TRUE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errorsI found a similar question asked(except for the change in the third question) but no answers.I tried deleting thefile to fix the last error, but no luck""",I followed all the steps given in.
51,41167490,,2,,"[{'score': 0.762367, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.938935, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.762367,FALSE,0,FALSE,0,TRUE,0.938935,FALSE,0,FALSE,0,FALSE,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errorsI found a similar question asked(except for the change in the third question) but no answers.I tried deleting thefile to fix the last error, but no luck""","I have 3 errorsI found a similar question asked(except for the change in the third question) but no answers.I tried deleting thefile to fix the last error, but no luck"""
52,54367776,,0,,"[{'score': 0.774376, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.774376,FALSE,0,FALSE,0,TRUE,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""","""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular."
53,54367776,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""","From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation""."
54,54367776,,2,,"[{'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,TRUE,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""",Because one detect only objects / creatures and the other one only text.
55,54367776,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""","So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?"""
56,44362759,,0,,"[{'score': 0.846863, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.846863,FALSE,0,FALSE,0,TRUE,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""","""The same image leads to different text detection results in the google cloud vision API demo versus the actual API."
57,44362759,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""","In the demo, the accuracy is much higher."
58,44362759,,2,,"[{'score': 0.840481, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840481,FALSE,0,FALSE,0,TRUE,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""","More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity."
59,44362759,,3,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""","Am I doing something wrong, or is this a bug?"""
60,51209404,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I already integrated the AWSRekognitionthem into my project. and I make sure I'm  connected to AWS, by this codeand the respnse isbut when I write this line it give me an error""","""I already integrated the AWSRekognitionthem into my project."
61,51209404,,1,,"[{'score': 0.511977, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.573342, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.511977,FALSE,0,FALSE,0,FALSE,0,TRUE,0.573342,FALSE,0,FALSE,"""I already integrated the AWSRekognitionthem into my project. and I make sure I'm  connected to AWS, by this codeand the respnse isbut when I write this line it give me an error""","and I make sure I'm  connected to AWS, by this codeand the respnse isbut when I write this line it give me an error"""
62,41560252,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""","""My project has a OCR requirement and I want to use the google cloud Vision API."
63,41560252,,1,,"[{'score': 0.772113, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.772113,FALSE,0,FALSE,0,TRUE,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""","I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer."
64,41560252,,2,,"[{'score': 0.991756, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.991756,TRUE,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""","Has anyone met this kind of issue before?Could you please give me any suggestion?"""
65,49932603,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use the @google-cloud/vision package to send requests to the Google Vision API. Is there a way to do multiple detections without doing something like:Thanks!""","""I am trying to use the @google-cloud/vision package to send requests to the Google Vision API."
66,49932603,,1,,"[{'score': 0.743803, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.867767, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.743803,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867767,FALSE,"""I am trying to use the @google-cloud/vision package to send requests to the Google Vision API. Is there a way to do multiple detections without doing something like:Thanks!""","Is there a way to do multiple detections without doing something like:Thanks!"""
67,49665196,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""","""I want to use text-detection from image (OCR) of google cloud vision api."
68,49665196,,1,,"[{'score': 0.713125, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713125,TRUE,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""",But i dont know how to get the subscription key from and how to authenticate and make calls in C#.
69,49665196,,2,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716301,TRUE,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""",Can some body tell me the step by step procedure to do that.
70,49665196,,3,,"[{'score': 0.679057, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.961633, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.679057,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961633,FALSE,0,FALSE,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""","Im very new this btw."""
71,56341410,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the Google Api Vision sample code but I'm getting this error:These are the dependencies that imported into my project.Code that I'm using. Which is provided google Vision API from:""","""I'm trying to run the Google Api Vision sample code but I'm getting this error:These are the dependencies that imported into my project.Code that I'm using."
72,56341410,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the Google Api Vision sample code but I'm getting this error:These are the dependencies that imported into my project.Code that I'm using. Which is provided google Vision API from:""","Which is provided google Vision API from:"""
73,49842698,,0,,"[{'score': 0.682143, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682143,FALSE,0,FALSE,0,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""","""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results."
74,49842698,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Videos might be lnong duration.
75,49842698,,2,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Hence I dont want to rely on promise.
76,49842698,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Rather in 1st Call I initiate video annotation.
77,49842698,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""","In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side."
78,49842698,,5,,"[{'score': 0.774376, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.774376,FALSE,0,FALSE,0,TRUE,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""","Doesnt GCP has similar feature?"""
79,47542341,,0,,"[{'score': 0.575431, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.5449, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.575431,FALSE,0,FALSE,0,TRUE,0.5449,FALSE,0,FALSE,0,FALSE,"""Relevant Code:However,When I run this, I get an error:How do I get my image to be the right format for detect_text? The documentation says it has to be base64 encoding.""","""Relevant Code:However,When I run this, I get an error:How do I get my image to be the right format for detect_text?"
80,47542341,,1,,"[{'score': 0.833309, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.833309,FALSE,0,TRUE,"""Relevant Code:However,When I run this, I get an error:How do I get my image to be the right format for detect_text? The documentation says it has to be base64 encoding.""","The documentation says it has to be base64 encoding."""
81,41488436,,0,,"[{'score': 0.71364, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.71364,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""","""I am using the Google Cloud Vision API to detect text in receipts."
82,41488436,,1,,"[{'score': 0.931106, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.931106,TRUE,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""",In some cases not all text on the receipt is detected.
83,41488436,,2,,"[{'score': 0.806276, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.806276,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""","Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface."
84,41488436,,3,,"[{'score': 0.813105, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.813105,FALSE,0,FALSE,0,TRUE,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""","As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected?"
85,41488436,,4,,"[{'score': 0.559533, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.990161, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.559533,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.990161,FALSE,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""","Any hints or help are much appreciated."""
86,54410031,,0,,"[{'score': 0.588094, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.588094,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am followingto use Google Vision API, but even configuring the authentication credentials I get the following error:My code in Visual Studio 2017:What can I do to fix this? Do I have to create a trial account to use the Google Cloud API?""","""I am followingto use Google Vision API, but even configuring the authentication credentials I get the following error:My code in Visual Studio 2017:What can I do to fix this?"
87,54410031,,1,,"[{'score': 0.674728, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.674728,FALSE,0,TRUE,"""I am followingto use Google Vision API, but even configuring the authentication credentials I get the following error:My code in Visual Studio 2017:What can I do to fix this? Do I have to create a trial account to use the Google Cloud API?""","Do I have to create a trial account to use the Google Cloud API?"""
88,53417035,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I read an image with text, Google Vision inserts line breaks in the middle of the sentence. How can I do to avoid this. Here's an example of the image text and Google Vision return:Text in the image:Google Vision Return:Thank you,""","""When I read an image with text, Google Vision inserts line breaks in the middle of the sentence."
89,53417035,,1,,"[{'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}]",FALSE,0,FALSE,0,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""When I read an image with text, Google Vision inserts line breaks in the middle of the sentence. How can I do to avoid this. Here's an example of the image text and Google Vision return:Text in the image:Google Vision Return:Thank you,""",How can I do to avoid this.
90,53417035,,2,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""When I read an image with text, Google Vision inserts line breaks in the middle of the sentence. How can I do to avoid this. Here's an example of the image text and Google Vision return:Text in the image:Google Vision Return:Thank you,""","Here's an example of the image text and Google Vision return:Text in the image:Google Vision Return:Thank you,"""
91,45427109,,0,,"[{'score': 0.736466, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.736466,FALSE,0,FALSE,0,TRUE,"""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help""","""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type""."
92,45427109,,1,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help""",so when I try to get response.face_annotations
93,45427109,,2,,"[{'score': 0.816846, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.633859, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.816846,FALSE,0,TRUE,0.633859,TRUE,"""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help""","I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help"""
94,51294008,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""","""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue."
95,51294008,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""",(The tag code is a regular 24 bytes String.
96,51294008,,2,,"[{'score': 0.777709, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.628717, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.777709,FALSE,0,FALSE,0,TRUE,0.628717,FALSE,0,FALSE,0,FALSE,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""","It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac."
97,51294008,,3,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""","Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?"""
98,45518029,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""","""I have a Gallery and Attachment models."
99,45518029,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via.
100,45518029,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",This works OK.
101,45518029,,3,,"[{'score': 0.621983, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.621983,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,FALSE,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""","However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error."
102,45518029,,4,,"[{'score': 0.648356, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.548179, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.829972, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.648356,FALSE,0,FALSE,0,TRUE,0.548179,FALSE,0,TRUE,0.829972,FALSE,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""","I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?"""
103,38227082,,0,,"[{'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.555401, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.555401,FALSE,0,TRUE,0.775166,TRUE,"""Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.Personal experience with Italian, French, English, Chinese, Spanish works. What about the ones like Hindi, Urdu etc?Thanks and appreciate your help!Suman""","""Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.Personal experience with Italian, French, English, Chinese, Spanish works."
104,38227082,,1,,"[{'score': 0.5376, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.5376,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.681699,FALSE,"""Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.Personal experience with Italian, French, English, Chinese, Spanish works. What about the ones like Hindi, Urdu etc?Thanks and appreciate your help!Suman""","What about the ones like Hindi, Urdu etc?Thanks and appreciate your help!Suman"""
105,44419153,,0,,"[{'score': 0.884227, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.884227,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""","""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails."
106,44419153,,1,,"[{'score': 0.779453, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.812219, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.779453,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.812219,FALSE,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""","Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2),"
107,44419153,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""","I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:"""
108,47563346,,0,,"[{'score': 0.550576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.550576,FALSE,0,FALSE,0,TRUE,"""i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.so can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images? here is a simple code of how to detect faces""","""i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.so"
109,47563346,,1,,"[{'score': 0.694514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.694514,FALSE,0,FALSE,0,TRUE,"""i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.so can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images? here is a simple code of how to detect faces""","can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images?"
110,47563346,,2,,"[{'score': 0.583626, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.583626,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,FALSE,"""i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.so can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images? here is a simple code of how to detect faces""","here is a simple code of how to detect faces"""
111,47439799,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to enter to the API from Microsoft FACE API. I created an account on Azure and created the service that they provide me the keys.The point is that I am trying to get access  to the API and it's all time a 401 errorAll time returns the same error:There is no almost documentation for the API in IOS and then it's just objective-c, no swift. Can someone figure out why is this returning all time the 401 error???Edit:As well I tried  let client =but this one returns all time 404 error, resource was not found.""","""I am trying to enter to the API from Microsoft FACE API."
112,47439799,,1,,"[{'score': 0.640986, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.640986,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to enter to the API from Microsoft FACE API. I created an account on Azure and created the service that they provide me the keys.The point is that I am trying to get access  to the API and it's all time a 401 errorAll time returns the same error:There is no almost documentation for the API in IOS and then it's just objective-c, no swift. Can someone figure out why is this returning all time the 401 error???Edit:As well I tried  let client =but this one returns all time 404 error, resource was not found.""","I created an account on Azure and created the service that they provide me the keys.The point is that I am trying to get access  to the API and it's all time a 401 errorAll time returns the same error:There is no almost documentation for the API in IOS and then it's just objective-c, no swift."
113,47439799,,2,,"[{'score': 0.692468, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.692468,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I am trying to enter to the API from Microsoft FACE API. I created an account on Azure and created the service that they provide me the keys.The point is that I am trying to get access  to the API and it's all time a 401 errorAll time returns the same error:There is no almost documentation for the API in IOS and then it's just objective-c, no swift. Can someone figure out why is this returning all time the 401 error???Edit:As well I tried  let client =but this one returns all time 404 error, resource was not found.""","Can someone figure out why is this returning all time the 401 error???Edit:As well I tried  let client =but this one returns all time 404 error, resource was not found."""
114,51588864,,0,,"[{'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,TRUE,"""According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks""","""According to the documentation AWS Rekognition is processing maximum 1FPS."
115,51588864,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks""",But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream.
116,51588864,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks""","And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?"
117,51588864,,3,,"[{'score': 0.609785, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.609785,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,FALSE,"""According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks""","Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks"""
118,51972479,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""","""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API."
119,51972479,,1,,"[{'score': 0.834975, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.834975,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.
120,51972479,,2,,"[{'score': 0.700591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700591,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""","My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word."
121,51972479,,3,,"[{'score': 0.580239, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.580239,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,FALSE,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",This makes the JSON object quite unwieldy and very difficult to use.
122,51972479,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""","I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:"""
123,56217832,,0,,"[{'score': 0.803567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.803567,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API for ""TEXT_DETECTION"". Input for it is png image.(scanned document).When I am running this API , its causing numbering issue.Numbers for first two paragraphs are not visible.Tried with same code.It should extract exact numbering from original scanned document.""","""I am using Google Vision API for ""TEXT_DETECTION""."
124,56217832,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API for ""TEXT_DETECTION"". Input for it is png image.(scanned document).When I am running this API , its causing numbering issue.Numbers for first two paragraphs are not visible.Tried with same code.It should extract exact numbering from original scanned document.""",Input for it is png image.(scanned
125,56217832,,2,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I am using Google Vision API for ""TEXT_DETECTION"". Input for it is png image.(scanned document).When I am running this API , its causing numbering issue.Numbers for first two paragraphs are not visible.Tried with same code.It should extract exact numbering from original scanned document.""","document).When I am running this API , its causing numbering issue.Numbers for first two paragraphs are not visible.Tried with same code.It should extract exact numbering from original scanned document."""
126,35660357,,0,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.752266, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.752266,FALSE,0,TRUE,0.525007,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""","""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage."
127,35660357,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage.
128,35660357,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""","My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get"
129,35660357,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""","access to object mybucket/10001.jpg."""
130,35660357,,4,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.719382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.719382,FALSE,0,TRUE,0.91961,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""","} } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module."
131,35660357,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket?
132,35660357,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""","If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER"""
133,54733517,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""","""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage."
134,54733517,,1,,"[{'score': 0.537235, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.537235,FALSE,0,FALSE,0,TRUE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""","Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this?"
135,54733517,,2,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted?
136,54733517,,3,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost?
137,54733517,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",As when I run container the storage should not need to be paid.
138,54733517,,5,,"[{'score': 0.62971, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.926735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.62971,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926735,FALSE,0,TRUE,0.615352,FALSE,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""","I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me"""
139,49717845,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""","""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.."
140,49717845,,1,,"[{'score': 0.827997, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.827997,FALSE,0,FALSE,0,TRUE,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""","I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks."""
141,45792942,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""","""i'm trying to use the recognition service from aws."
142,45792942,,1,,"[{'score': 0.644826, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.644826,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,TRUE,0.5538,FALSE,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""",Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url.
143,45792942,,2,,"[{'score': 0.615166, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.753251, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.615166,FALSE,0,FALSE,0,TRUE,0.753251,FALSE,0,TRUE,0.681699,FALSE,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""","I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made."
144,45792942,,3,,"[{'score': 0.582784, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.582784,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""","Thanks in advancepython"""
145,44225909,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""","""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture."
146,44225909,,1,,"[{'score': 0.789698, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.598602, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.789698,TRUE,0.598602,FALSE,0,TRUE,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""","This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box."
147,44225909,,2,,"[{'score': 0.5687, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5687,TRUE,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""","below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture."
148,44225909,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""","currently it gives the x,y of the nose from inside the bounding box (i think)."
149,44225909,,4,,"[{'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,FALSE,0,TRUE,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""","My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:"""
150,45481935,,0,,"[{'score': 0.598448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.598448,FALSE,0,FALSE,0,TRUE,"""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?""","""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for"
151,45481935,,1,,"[{'score': 0.728394, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.728394,FALSE,0,FALSE,0,TRUE,"""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?""","example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?"""
152,53493720,,0,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am usingfor a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the""","""I am usingfor a face recognition project."
153,53493720,,1,,"[{'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,FALSE,0,FALSE,0,TRUE,"""I am usingfor a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the""",I can only send images using URL.
154,53493720,,2,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I am usingfor a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the""","But since images are stored on our on-premise servers, authorisation is required to access those images using the URL."
155,53493720,,3,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I am usingfor a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the""","How can I pass images using the"""
156,49296976,,0,,"[{'score': 0.873624, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873624,FALSE,0,FALSE,0,TRUE,"""I cannot find my build.gradle project file to add dependency only my android studio contains 1 module build.gradle but cannot find the other one to add dependency.i need to add the google vision dependency in my project but i am not getting where to write the dependency codelikecan please anyone help me out regarding this problem""","""I cannot find my build.gradle"
157,49296976,,1,,"[{'score': 0.743682, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743682,FALSE,0,FALSE,0,TRUE,"""I cannot find my build.gradle project file to add dependency only my android studio contains 1 module build.gradle but cannot find the other one to add dependency.i need to add the google vision dependency in my project but i am not getting where to write the dependency codelikecan please anyone help me out regarding this problem""",project file to add dependency only my android studio contains 1 module build.gradle
158,49296976,,2,,"[{'score': 0.608261, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.608261,FALSE,0,FALSE,0,TRUE,"""I cannot find my build.gradle project file to add dependency only my android studio contains 1 module build.gradle but cannot find the other one to add dependency.i need to add the google vision dependency in my project but i am not getting where to write the dependency codelikecan please anyone help me out regarding this problem""","but cannot find the other one to add dependency.i need to add the google vision dependency in my project but i am not getting where to write the dependency codelikecan please anyone help me out regarding this problem"""
159,46548182,,0,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""","""I am using the Google vision api to perform text recognition on receipt images."
160,46548182,,1,,"[{'score': 0.733511, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.733511,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,TRUE,0.775166,FALSE,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",I am getting some nice results returned but the format in which the return is quite unreliable.
161,46548182,,2,,"[{'score': 0.836264, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.836264,FALSE,0,FALSE,0,TRUE,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""","If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc."
162,46548182,,3,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses?
163,46548182,,4,,"[{'score': 0.667755, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.666324, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.743195, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.667755,FALSE,0,FALSE,0,FALSE,0,TRUE,0.666324,FALSE,0,TRUE,0.743195,FALSE,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""","I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth."""
164,52103546,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""","""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did"
165,52103546,,1,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""",I write something wrong ?
166,52103546,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""",also can i use Text in image to get image from local storage ?
167,52103546,,3,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""","and does it support Arabic or not ?"""
168,54881537,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ? Using,""","""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ?"
169,54881537,,1,,"[{'score': 0.997482, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.997482,FALSE,0,FALSE,0,TRUE,"""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ? Using,""","Using,"""
170,42842441,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have images of receipts and I want to store the text in the images separately. Is it possible to detect text from images using Amazon Rekognition?""","""I have images of receipts and I want to store the text in the images separately."
171,42842441,,1,,"[{'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,TRUE,0.786991,TRUE,"""I have images of receipts and I want to store the text in the images separately. Is it possible to detect text from images using Amazon Rekognition?""","Is it possible to detect text from images using Amazon Rekognition?"""
172,54803618,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""","""I created a new customer image classifier in IBM watson visual recognition."
173,54803618,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""","I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when"
174,54803618,,2,,"[{'score': 0.799476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.799476,FALSE,0,FALSE,0,TRUE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""","I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error."
175,54803618,,3,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.898327, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,TRUE,0.898327,FALSE,0,TRUE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",But it clearly shows i have 3458 samples.
176,54803618,,4,,"[{'score': 0.537946, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.537946,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.681699,FALSE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",and daisy class has 615 samples.I don't have any clue to solve this.
177,54803618,,5,,"[{'score': 0.928936, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.928936,TRUE,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""","What should I try now? any help would be appreciable."""
178,56143548,,0,,"[{'score': 0.543209, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.543209,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""","""I have images with important file metadata (e.g."
179,56143548,,1,,"[{'score': 0.744677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.744677,FALSE,0,FALSE,0,TRUE,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID.
180,56143548,,2,,"[{'score': 0.806974, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.806974,TRUE,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g.
181,56143548,,3,,"[{'score': 0.612897, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.612897,FALSE,0,FALSE,0,TRUE,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""","JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note"
182,56143548,,4,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""","that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!"""
183,42123633,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""","""I am having trouble using Microsoft Face API."
184,42123633,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.591155, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.591155,TRUE,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""",Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there.
185,42123633,,2,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""",Can someone help me on this?
186,42123633,,3,,"[{'score': 0.715783, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.989586, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.715783,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,TRUE,0.989586,FALSE,0,FALSE,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""","Very much appreciated."""
187,41400421,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.""","""I am using google vision API to scan the barcodes and qrcodes."
188,41400421,,1,,"[{'score': 0.546148, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.546148,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.""","Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this?"
189,41400421,,2,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.""","Because there are lots of app on google play store those are doing the same things."""
190,48286385,,0,,"[{'score': 0.528045, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.528045,FALSE,0,FALSE,0,TRUE,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""","""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn"
191,48286385,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""",file.
192,48286385,,2,,"[{'score': 0.790954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.790954,FALSE,0,FALSE,0,TRUE,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""",After set up a SceneView with a 3D object stored in a Node...
193,48286385,,3,,"[{'score': 0.780972, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.780972,FALSE,0,FALSE,0,TRUE,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""","I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ?"
194,48286385,,4,,"[{'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873263,TRUE,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""","an is what I am trying to do correct, or there is anything wrong in the reasoning?"""
195,36408010,,0,,"[{'score': 0.812219, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.656175, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.656175,FALSE,0,TRUE,0.812219,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""","""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language."
196,36408010,,1,,"[{'score': 0.813379, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.771739, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.771739,FALSE,0,TRUE,0.813379,TRUE,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""","The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated."""
197,44792573,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""","""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected."
198,44792573,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket.
199,44792573,,2,,"[{'score': 0.642838, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642838,FALSE,0,FALSE,0,TRUE,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""","A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective."
200,44792573,,3,,"[{'score': 0.903196, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.903196,TRUE,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""","I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!"""
201,51711390,,0,,"[{'score': 0.506158, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.727798, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.506158,FALSE,0,FALSE,0,TRUE,0.842108,TRUE,0.727798,FALSE,0,FALSE,"""I want to avoid all the spoofing and other violence related to face recognition security application. which provider would you prefer to use from Amazon Rekognition or Azure Cognitive Face?""","""I want to avoid all the spoofing and other violence related to face recognition security application."
202,51711390,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.681699,TRUE,"""I want to avoid all the spoofing and other violence related to face recognition security application. which provider would you prefer to use from Amazon Rekognition or Azure Cognitive Face?""","which provider would you prefer to use from Amazon Rekognition or Azure Cognitive Face?"""
203,40493285,,0,,"[{'score': 0.715667, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.715667,FALSE,0,FALSE,0,TRUE,"""Is there a way to get Watson Visual Recognition to return the location of the classified content (car, tree, etc.) when using image classification? This capability exists in the face recognition service and would be invaluable in general image classification.The currenthas no information on this topic.""","""Is there a way to get Watson Visual Recognition to return the location of the classified content (car, tree, etc.) when using image classification?"
204,40493285,,1,,"[{'score': 0.879891, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.879891,FALSE,0,FALSE,0,TRUE,"""Is there a way to get Watson Visual Recognition to return the location of the classified content (car, tree, etc.) when using image classification? This capability exists in the face recognition service and would be invaluable in general image classification.The currenthas no information on this topic.""","This capability exists in the face recognition service and would be invaluable in general image classification.The currenthas no information on this topic."""
205,56219425,,0,,"[{'score': 0.586538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.586538,TRUE,"""The google cloud vision api works fine on one pdfbut returns absolutely nothing on the other pdf. I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.I'm using the code given in their examples section by uploading these files in a google cloud bucket.""","""The google cloud vision api works fine on one pdfbut returns absolutely nothing on the other pdf."
206,56219425,,1,,"[{'score': 0.707622, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.707622,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The google cloud vision api works fine on one pdfbut returns absolutely nothing on the other pdf. I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.I'm using the code given in their examples section by uploading these files in a google cloud bucket.""","I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.I'm using the code given in their examples section by uploading these files in a google cloud bucket."""
207,19902877,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""","""I'm experimenting a bit with an API which can detect faces from an image."
208,19902877,,1,,"[{'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""",I'm using Python and want to be able to upload an image that specifies an argument (in the console).
209,19902877,,2,,"[{'score': 0.965509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.965509,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""",For example:This is meant to send filejack.jpgup
210,19902877,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""",to the API.
211,19902877,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""",And afterwards printJSONresponse.
212,19902877,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""","Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?"""
213,55831279,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""","""I'm trying to get Google Cloud Vision to work with node.js by following their documentation."
214,55831279,,1,,"[{'score': 0.716224, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716224,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""","Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]."
215,55831279,,2,,"[{'score': 0.651265, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.775079, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.651265,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775079,FALSE,0,FALSE,0,FALSE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config
216,55831279,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""","atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there?"
217,55831279,,4,,"[{'score': 0.754061, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.754061,FALSE,0,FALSE,0,TRUE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'.
218,55831279,,5,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.716301,TRUE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Any help will be appreciated as I'm still getting used to the gcloud-cli.
219,55831279,,6,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""","Thanks"""
220,41186458,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful.""","""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ."
221,41186458,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful.""","I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful."""
222,46966332,,0,,"[{'score': 0.541258, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.541258,FALSE,0,FALSE,0,TRUE,"""Im going to use Google Cloud Vision API.In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.So the scheme looks like this:Phone photo(Local Storage)--download--> GC Storage --get link-->Send request with this link to GC Vision API --get JSON--> work with JSONSo the question is.What for I need to storage image in cloud? Only for a link? Can I send the image direct to the Vision API without GC Storage?So the scheme:Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON""","""Im going to use Google Cloud Vision API.In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.So the scheme looks like this:Phone photo(Local Storage)--download--> GC Storage --get link-->Send request with this link to GC Vision API --get JSON--> work with JSONSo the question is.What for I need to storage image in cloud?"
223,46966332,,1,,"[{'score': 0.965509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.965509,FALSE,0,FALSE,0,TRUE,"""Im going to use Google Cloud Vision API.In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.So the scheme looks like this:Phone photo(Local Storage)--download--> GC Storage --get link-->Send request with this link to GC Vision API --get JSON--> work with JSONSo the question is.What for I need to storage image in cloud? Only for a link? Can I send the image direct to the Vision API without GC Storage?So the scheme:Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON""",Only for a link?
224,46966332,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Im going to use Google Cloud Vision API.In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.So the scheme looks like this:Phone photo(Local Storage)--download--> GC Storage --get link-->Send request with this link to GC Vision API --get JSON--> work with JSONSo the question is.What for I need to storage image in cloud? Only for a link? Can I send the image direct to the Vision API without GC Storage?So the scheme:Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON""","Can I send the image direct to the Vision API without GC Storage?So the scheme:Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON"""
225,49762291,,0,,"[{'score': 0.77731, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.852844, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.77731,FALSE,0,FALSE,0,TRUE,0.852844,FALSE,0,FALSE,0,FALSE,"""I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:an example of the code being used is:The last line throws the error. However, if I submit:There is no error and faces are matched. Does anybody have ideas as to the reason for this error?A full trace for this error:""","""I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:an example of the code being used is:The last line throws the error."
226,49762291,,1,,"[{'score': 0.907142, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.907142,FALSE,0,FALSE,0,TRUE,"""I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:an example of the code being used is:The last line throws the error. However, if I submit:There is no error and faces are matched. Does anybody have ideas as to the reason for this error?A full trace for this error:""","However, if I submit:There is no error and faces are matched."
227,49762291,,2,,"[{'score': 0.785206, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.873089, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.785206,FALSE,0,FALSE,0,TRUE,0.873089,FALSE,0,TRUE,0.58393,FALSE,"""I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:an example of the code being used is:The last line throws the error. However, if I submit:There is no error and faces are matched. Does anybody have ideas as to the reason for this error?A full trace for this error:""","Does anybody have ideas as to the reason for this error?A full trace for this error:"""
228,51320588,,0,,"[{'score': 0.775166, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775166,TRUE,"""I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc""","""I am trying to integrate Google Vision API on our platform and I am facing some difficulties."
229,51320588,,1,,"[{'score': 0.511684, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.511684,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,FALSE,"""I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc""",The thread blocks on the API call and doesn't return.
230,51320588,,2,,"[{'score': 0.593371, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.522484, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.593371,FALSE,0,FALSE,0,TRUE,0.522484,FALSE,0,FALSE,0,FALSE,"""I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc""","I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc"""
231,52143046,,0,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am really new to python and i am using Google vision API to detect some text from image,In the API output there two thing which are of interest to me text and text.vertices both of them i have stored in two variables p & c  however i dont know how to create a single dictionary which i can then use to do some post processing(sorting,dicing etc.) on that data below is code i am using, could anyone please suggest any solutionsHowever this creates seperate lines for each c & p i want everyting into a single dictionary so that i then sort it in ascending/descending""","""I am really new to python and i am using Google vision API to detect some text from image,In the API output there two thing which are of interest to me text and text.vertices"
232,52143046,,1,,"[{'score': 0.718644, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.864999, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718644,FALSE,0,TRUE,0.864999,TRUE,"""I am really new to python and i am using Google vision API to detect some text from image,In the API output there two thing which are of interest to me text and text.vertices both of them i have stored in two variables p & c  however i dont know how to create a single dictionary which i can then use to do some post processing(sorting,dicing etc.) on that data below is code i am using, could anyone please suggest any solutionsHowever this creates seperate lines for each c & p i want everyting into a single dictionary so that i then sort it in ascending/descending""","both of them i have stored in two variables p & c  however i dont know how to create a single dictionary which i can then use to do some post processing(sorting,dicing etc.) on that data below is code i am using, could anyone please suggest any solutionsHowever this creates seperate lines for each c & p i want everyting into a single dictionary so that i then sort it in ascending/descending"""
233,50907956,,0,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?""","""Well I'm creating a android application which uses web services as well."
234,50907956,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?""",For that I'm planing to create a web API using flask framework to communicate with my android application.
235,50907956,,2,,"[{'score': 0.663387, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.663387,FALSE,0,FALSE,0,TRUE,"""Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?""",I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app.
236,50907956,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?""","How can I make both two APIs to communicate with each other?"""
237,56293609,,0,,"[{'score': 0.766454, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.766454,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""","""I am using Microsoft custom vision service in object detection to extract the wanted objects."
238,56293609,,1,,"[{'score': 0.908301, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.908301,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",And I would like to make a regression test to compare the results.
239,56293609,,2,,"[{'score': 0.825633, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825633,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""","However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft ()."
240,56293609,,3,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Within this platform we can add the images and then tag the objects.
241,56293609,,4,,"[{'score': 0.894493, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.894493,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""","I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.)"
242,56293609,,5,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""","But I was not able to find them."""
243,36540684,,0,,"[{'score': 0.807862, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.807862,FALSE,0,FALSE,0,TRUE,"""I want to implement google-cloud-vision API for OCR on my Project.But due to compliance issues, I need to know does google-cloud-vision stores the uploaded image? if yes what is the privacy policy for that?Does anyone have any information regarding this?Thanks!""","""I want to implement google-cloud-vision API for OCR on my Project.But due to compliance issues, I need to know does google-cloud-vision stores the uploaded image?"
244,36540684,,1,,"[{'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.855572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.855572,FALSE,0,TRUE,0.873263,TRUE,"""I want to implement google-cloud-vision API for OCR on my Project.But due to compliance issues, I need to know does google-cloud-vision stores the uploaded image? if yes what is the privacy policy for that?Does anyone have any information regarding this?Thanks!""","if yes what is the privacy policy for that?Does anyone have any information regarding this?Thanks!"""
245,52825897,,0,,"[{'score': 0.58345, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.741964, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.58345,FALSE,0,FALSE,0,TRUE,0.724236,TRUE,0.741964,FALSE,0,FALSE,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""","""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS."
246,52825897,,1,,"[{'score': 0.808413, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.808413,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""","If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection."
247,52825897,,2,,"[{'score': 0.998976, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.998976,TRUE,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""","Anybody got any ideas?"""
248,54656996,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""","""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews."
249,54656996,,1,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.828638, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.828638,FALSE,0,TRUE,0.525007,TRUE,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""",However values are not retrieved from Firebase if a variable is passed as a child when referencing the database.
250,54656996,,2,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.71364, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.71364,FALSE,0,TRUE,0.716301,TRUE,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""",How to set a variable as a child when referencing the database?Note:
251,54656996,,3,,"[{'score': 0.682998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.682998,FALSE,0,FALSE,0,TRUE,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""",There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java
252,54656996,,4,,"[{'score': 0.565788, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.645985, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.565788,FALSE,0,FALSE,0,FALSE,0,TRUE,0.645985,FALSE,0,FALSE,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""","and I have to press the back button several times to go to Home page."""
253,55164321,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I am trying to run the code in. I have run the following terminal command correctly:However I am getting the following credential errors:""","""I am trying to run the code in."
254,55164321,,1,,"[{'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,TRUE,0.704642,FALSE,0,TRUE,"""I am trying to run the code in. I have run the following terminal command correctly:However I am getting the following credential errors:""","I have run the following terminal command correctly:However I am getting the following credential errors:"""
255,48672879,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3."
256,48672879,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",I tried it on Windows on version Python3.6.4 and it runs perfectly alright.
257,48672879,,2,,"[{'score': 0.747994, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.747994,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client)"
258,48672879,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",AND also python3.6.2
259,48672879,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail."
260,48672879,,5,,"[{'score': 0.670206, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.934524, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.670206,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.934524,FALSE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",It did not even print happiness which is supposed to return a float value.
261,48672879,,6,,"[{'score': 0.69164, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.6821, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.69164,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.6821,FALSE,0,FALSE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","Please advice why and how can I make it work on Raspberry Pi, thank you very much!"
262,48672879,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",Have it got to do with the python version?
263,48672879,,8,,"[{'score': 0.640413, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.640413,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,FALSE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read()
264,48672879,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","as print(data) returns with ""operation is timeout"" error""."
265,48672879,,10,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.57374,TRUE,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""","May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:"""
266,53475106,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to try face masking likeSnapchatfeature on my Android app. I've tried this face masking using Google vision, but I can't find how to record this face masking.Is there any solution to record face masking?""","""I want to try face masking likeSnapchatfeature on my Android app."
267,53475106,,1,,"[{'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.752532, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.752532,FALSE,0,TRUE,0.57374,TRUE,"""I want to try face masking likeSnapchatfeature on my Android app. I've tried this face masking using Google vision, but I can't find how to record this face masking.Is there any solution to record face masking?""","I've tried this face masking using Google vision, but I can't find how to record this face masking.Is there any solution to record face masking?"""
268,53457755,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image. But when i try it gives me result in matching them with english:Request code is as below:""","""Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image."
269,53457755,,1,,"[{'score': 0.51901, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.51901,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image. But when i try it gives me result in matching them with english:Request code is as below:""","But when i try it gives me result in matching them with english:Request code is as below:"""
270,55997760,,0,,"[{'score': 0.693858, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.733853, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.787769, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.693858,FALSE,0,FALSE,0,TRUE,0.787769,FALSE,0,TRUE,0.733853,FALSE,"""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?""","""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document."
271,55997760,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?""","There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find."
272,55997760,,2,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.77464, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.77464,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?""","I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?"""
273,53643788,,0,,"[{'score': 0.706744, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.706744,FALSE,0,FALSE,0,TRUE,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""","""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary."
274,53643788,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""",I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String.
275,53643788,,2,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""",I can put the values into an array but it is still an array of Any type.
276,53643788,,3,,"[{'score': 0.636458, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.63698, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.636458,FALSE,0,TRUE,0.63698,TRUE,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""","Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated."""
277,36289389,,0,,"[{'score': 0.931038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.931038,FALSE,0,FALSE,0,TRUE,"""I am having trouble understanding the concept of  API discovery  as used in Google products/services.  Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight.""","""I am having trouble understanding the concept of  API discovery  as used in Google products/services."
278,36289389,,1,,"[{'score': 0.794465, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.794465,FALSE,0,FALSE,0,TRUE,"""I am having trouble understanding the concept of  API discovery  as used in Google products/services.  Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight.""","Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious."
279,36289389,,2,,"[{'score': 0.91747, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91747,FALSE,0,FALSE,0,TRUE,"""I am having trouble understanding the concept of  API discovery  as used in Google products/services.  Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight.""","So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight."""
280,55568798,,0,,"[{'score': 0.633839, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.678467, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.633839,FALSE,0,FALSE,0,TRUE,0.678467,FALSE,0,FALSE,0,FALSE,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""","""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test."
281,55568798,,1,,"[{'score': 0.624557, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.734369, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.734369,FALSE,0,TRUE,0.624557,TRUE,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""","I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is."
282,55568798,,2,,"[{'score': 0.516914, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.516914,FALSE,0,FALSE,0,TRUE,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""",I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense).
283,55568798,,3,,"[{'score': 0.543514, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.543514,TRUE,0.620279,FALSE,0,FALSE,0,FALSE,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""","Instead, it callswhich throws an NPE."""
284,52477690,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""","""We are using Google vision for a while now."
285,52477690,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""","The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti!"
286,52477690,,2,,"[{'score': 0.651887, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.651887,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.786991,FALSE,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",Which is the good point ;-)Did anybody experience the same issue?
287,52477690,,3,,"[{'score': 0.517887, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.840228, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.517887,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840228,FALSE,0,FALSE,0,FALSE,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""","And found a way to return the result in a specific language?Thanks"""
288,47558371,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""","""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image?"
289,47558371,,1,,"[{'score': 0.738963, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.738963,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""","ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed."""
290,45306016,,0,,"[{'score': 0.502925, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.502925,FALSE,0,FALSE,0,TRUE,"""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically this code where the batch client.batchAnnotateImages occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any ideas ?""","""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically"
291,45306016,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically this code where the batch client.batchAnnotateImages occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any ideas ?""",this code where the batch client.batchAnnotateImages
292,45306016,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically this code where the batch client.batchAnnotateImages occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any ideas ?""",occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any
293,45306016,,3,,"[{'score': 0.997482, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.997482,FALSE,0,FALSE,0,TRUE,"""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically this code where the batch client.batchAnnotateImages occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any ideas ?""","ideas ?"""
294,50628383,,0,,"[{'score': 0.529904, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.541591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.529904,FALSE,0,FALSE,0,TRUE,0.541591,FALSE,0,FALSE,0,FALSE,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""","""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise."
295,50628383,,1,,"[{'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.704642,FALSE,0,TRUE,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",When I print the return within the Detect function everything looks correct.
296,50628383,,2,,"[{'score': 0.661496, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.739299, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.661496,FALSE,0,FALSE,0,TRUE,0.739299,FALSE,0,FALSE,0,FALSE,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition.
297,50628383,,3,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Anyone seen this before?****
298,50628383,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""","Workaround (working)aws.service}app.component"""
299,45505722,,0,,"[{'score': 0.786991, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.890872, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890872,FALSE,0,TRUE,0.786991,TRUE,"""I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?""","""I prepare some solution for grouping documents using Google Vision API."
300,45505722,,1,,"[{'score': 0.511119, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.511119,TRUE,"""I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?""","I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?"""
301,55920132,,0,,"[{'score': 0.601001, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.601001,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)AWS HTTP error: Client error: POSTresulted in a 400 Bad Request response:{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.) - {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'I am using the following PHP script.I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.I am not sure whether it is the right way of solving this problem.Please Help""","""I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)AWS HTTP error: Client error: POSTresulted in a 400 Bad Request response:{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)"
302,55920132,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)AWS HTTP error: Client error: POSTresulted in a 400 Bad Request response:{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.) - {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'I am using the following PHP script.I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.I am not sure whether it is the right way of solving this problem.Please Help""","- {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'I"
303,55920132,,2,,"[{'score': 0.663824, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.663824,FALSE,0,FALSE,0,TRUE,"""I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)AWS HTTP error: Client error: POSTresulted in a 400 Bad Request response:{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.) - {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'I am using the following PHP script.I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.I am not sure whether it is the right way of solving this problem.Please Help""","am using the following PHP script.I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.I am not sure whether it is the right way of solving this problem.Please Help"""
304,36602892,,0,,"[{'score': 0.704642, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.704642,FALSE,0,TRUE,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""","""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct."
305,36602892,,1,,"[{'score': 0.663507, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.840424, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.765702, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.663507,FALSE,0,FALSE,0,TRUE,0.840424,FALSE,0,TRUE,0.765702,FALSE,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""","I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it."""
306,41766196,,0,,"[{'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,TRUE,"""Face Tracker app based on. By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.This is the code for CameraSourcePreview that google vision provide:I call camera source with this method:Face Tracker front camera still too dark compare with default phone camera app.How to brighten front camera in face tracker google vision? Is it related with surface view?""","""Face Tracker app based on."
307,41766196,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Face Tracker app based on. By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.This is the code for CameraSourcePreview that google vision provide:I call camera source with this method:Face Tracker front camera still too dark compare with default phone camera app.How to brighten front camera in face tracker google vision? Is it related with surface view?""","By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.This is the code for CameraSourcePreview that google vision provide:I call camera source with this method:Face Tracker front camera still too dark compare with default phone camera app.How to brighten front camera in face tracker google vision?"
308,41766196,,2,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""Face Tracker app based on. By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.This is the code for CameraSourcePreview that google vision provide:I call camera source with this method:Face Tracker front camera still too dark compare with default phone camera app.How to brighten front camera in face tracker google vision? Is it related with surface view?""","Is it related with surface view?"""
309,38753678,,0,,"[{'score': 0.678855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.678855,FALSE,0,FALSE,0,TRUE,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""","""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify."
310,38753678,,1,,"[{'score': 0.527088, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.527088,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform.
311,38753678,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""","It works OK for female photos, but not with male ones."""
312,47154016,,0,,"[{'score': 0.93884, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.93884,TRUE,0.751512,FALSE,0,TRUE,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""","""I am currently using Microsoft Azure Emotion API to look at emotion of certain images."
313,47154016,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""","Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan."""
314,54640539,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to send an image file to Google Vision just after I saved the file using a promisifiedbut I am getting an error saying that the file or directory is not found.I have added a function to get the file size to confim that the file exists before I send the request to Vision and the file is already created.I've also called a pathname to an existing file and Google Vision just worked fine.So I guess that it's just a matter of setting a time between saving the file and sending request to Google Vision.Here's my code:Here's what I get in console:How to fix this? Should I just add a setTimeout? or is there any other better solution to overcome this error?""","""I am trying to send an image file to Google Vision just after I saved the file using a promisifiedbut I am getting an error saying that the file or directory is not found.I have added a function to get the file size to confim that the file exists before I send the request to Vision and the file is already created.I've also called a pathname to an existing file and Google Vision just worked fine.So I guess that it's just a matter of setting a time between saving the file and sending request to Google Vision.Here's my code:Here's what I get in console:How to fix this?"
315,54640539,,1,,"[{'score': 0.58693, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.954099, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.58693,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,TRUE,0.954099,FALSE,"""I am trying to send an image file to Google Vision just after I saved the file using a promisifiedbut I am getting an error saying that the file or directory is not found.I have added a function to get the file size to confim that the file exists before I send the request to Vision and the file is already created.I've also called a pathname to an existing file and Google Vision just worked fine.So I guess that it's just a matter of setting a time between saving the file and sending request to Google Vision.Here's my code:Here's what I get in console:How to fix this? Should I just add a setTimeout? or is there any other better solution to overcome this error?""","Should I just add a setTimeout? or is there any other better solution to overcome this error?"""
316,50265754,,0,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","""For our application we are using Azure Face API to build training set for each person."
317,50265754,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","We are using ""LargePersonGroup"" for our application."
318,50265754,,2,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80026,FALSE,0,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","We have created ""Person"" inside group for all user profiles we have."
319,50265754,,3,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.751512,FALSE,0,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","Every time new face is added for a person we ""Train"" the whole group."
320,50265754,,4,,"[{'score': 0.554824, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.757277, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.715056, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.554824,FALSE,0,FALSE,0,FALSE,0,TRUE,0.715056,TRUE,0.757277,FALSE,0,FALSE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images."
321,50265754,,5,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.561417, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.561417,FALSE,0,TRUE,0.647986,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API.
322,50265754,,6,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""","I checked with AWS and their results were far better.Please help."""
323,49168443,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url. The reference from the API () says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".I ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).It works fine if I use the url method, but I need to upload the image.This is the code I ve been using:My Bitmap works fine by the way.This is the error that the logcat gives me:So, finally, what do I have to do to send the proper image format that the api requires?Thank you in advance.""","""I ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url."
324,49168443,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url. The reference from the API () says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".I ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).It works fine if I use the url method, but I need to upload the image.This is the code I ve been using:My Bitmap works fine by the way.This is the error that the logcat gives me:So, finally, what do I have to do to send the proper image format that the api requires?Thank you in advance.""","The reference from the API () says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".I ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).It works fine if I use the url method, but I need to upload the image.This is the code I ve been using:My Bitmap works fine by the way.This is the error that the logcat gives me:So, finally, what do I have to do to send the proper image format that the api requires?Thank you in advance."""
325,35740396,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""","""I am usingfor face detection."
326,35740396,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""",I want to enable capture button when the face is detected in the camera otherwise disable.
327,35740396,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""","Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds."""
328,54580554,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection."
329,54580554,,1,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","How do I parse the results of the graph, which come back as a 4 dimensional array."
330,54580554,,2,,"[{'score': 0.620252, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.620252,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me."
331,54580554,,3,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read."
332,54580554,,4,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Now switching to a hopefully more accurate model I'm unsure how to read the results.
333,54580554,,5,,"[{'score': 0.506048, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.506048,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,FALSE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","I've been able to parse results into the 4 dimensional array, but can't make sense of it after that."
334,54580554,,6,,"[{'score': 0.849929, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.849929,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure."
335,54580554,,7,,"[{'score': 0.901894, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901894,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Which yields different results to read from the graph.
336,54580554,,8,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",I get undefined when trying the same fetch operations in the example from the link below.
337,54580554,,9,,"[{'score': 0.965509, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.965509,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",This is also confirmed by using graph.getEnumeration()
338,54580554,,10,,"[{'score': 0.881003, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.881003,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint.
339,54580554,,11,,"[{'score': 0.961593, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961593,FALSE,0,FALSE,0,TRUE,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""","Similar to results found in their documentation."""
340,52843616,,0,,"[{'score': 0.705401, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705401,FALSE,0,FALSE,0,TRUE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database."
341,52843616,,1,,"[{'score': 0.921944, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.921944,FALSE,0,FALSE,0,TRUE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead."
342,52843616,,2,,"[{'score': 0.792523, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.72137, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.792523,FALSE,0,FALSE,0,TRUE,0.72137,FALSE,0,FALSE,0,FALSE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server."
343,52843616,,3,,"[{'score': 0.643965, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.643965,FALSE,0,FALSE,0,TRUE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database?"
344,52843616,,4,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information."
345,52843616,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""","Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now."""
346,38804790,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""","""The Google Vision API has a limit of 10 requests per second."
347,38804790,,1,,"[{'score': 0.575629, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.575629,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""",I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below.
348,38804790,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.615352,TRUE,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""",The first request always works just fine.What could be the reason that this is happening.
349,38804790,,3,,"[{'score': 0.692177, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.692177,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,FALSE,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""",Is there something the documentation that I am missing ?.
350,38804790,,4,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""","The images I try to pass are in the range of 100-150 KB size only."""
351,52393200,,0,,"[{'score': 0.54839, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.54839,FALSE,0,FALSE,0,TRUE,"""i am trying to create a app which makes use of amazon rekogition in aws for identification of a person and retrieving the personal information for an internal storage system.i wanted to know how to connect the amazon rekognition partand the information stored in the database.The face detection  part will be done by amazon rekognition but how will store and retrieve the personal information after detection of face""","""i am trying to create a app which makes use of amazon rekogition in aws for identification of a person and retrieving the personal information for an internal storage system.i"
352,52393200,,1,,"[{'score': 0.805724, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.805724,FALSE,0,FALSE,0,TRUE,"""i am trying to create a app which makes use of amazon rekogition in aws for identification of a person and retrieving the personal information for an internal storage system.i wanted to know how to connect the amazon rekognition partand the information stored in the database.The face detection  part will be done by amazon rekognition but how will store and retrieve the personal information after detection of face""","wanted to know how to connect the amazon rekognition partand the information stored in the database.The face detection  part will be done by amazon rekognition but how will store and retrieve the personal information after detection of face"""
353,41872763,,0,,"[{'score': 0.707158, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.707158,FALSE,0,FALSE,0,TRUE,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""","""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text."
354,41872763,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""","but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation"""
355,49470158,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am following a tutorial () that utilizes AWS rekognition on a raspberry pi to make a robot. I am having trouble figuring out how to create logs to see where the code is hanging up.Here is the lambda function I need to debug:This lambda is called upon by the bot in AWS. Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.Here is the logs in AWS console:Please and Thank You for any help,David""","""I am following a tutorial () that utilizes AWS rekognition on a raspberry pi to make a robot."
356,49470158,,1,,"[{'score': 0.613881, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.613881,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am following a tutorial () that utilizes AWS rekognition on a raspberry pi to make a robot. I am having trouble figuring out how to create logs to see where the code is hanging up.Here is the lambda function I need to debug:This lambda is called upon by the bot in AWS. Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.Here is the logs in AWS console:Please and Thank You for any help,David""",I am having trouble figuring out how to create logs to see where the code is hanging up.Here is the lambda function I need to debug:This lambda is called upon by the bot in AWS.
357,49470158,,2,,"[{'score': 0.5687, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5687,TRUE,"""I am following a tutorial () that utilizes AWS rekognition on a raspberry pi to make a robot. I am having trouble figuring out how to create logs to see where the code is hanging up.Here is the lambda function I need to debug:This lambda is called upon by the bot in AWS. Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.Here is the logs in AWS console:Please and Thank You for any help,David""","Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.Here is the logs in AWS console:Please and Thank You for any help,David"""
358,47324768,,0,,"[{'score': 0.794075, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.750465, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.750465,FALSE,0,TRUE,0.794075,TRUE,"""I have been searching for a while, but have not found any information on whether there are limits on calls to Amazon Rekognition service.Does anyone know the numbers, or any source where I can look?What I want to know is if they limit the number of calls allowed per minute or second. I'm looking for information on the the paid (not free) service tier.""","""I have been searching for a while, but have not found any information on whether there are limits on calls to Amazon Rekognition service.Does anyone know the numbers, or any source where I can look?What I want to know is if they limit the number of calls allowed per minute or second."
359,47324768,,1,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""I have been searching for a while, but have not found any information on whether there are limits on calls to Amazon Rekognition service.Does anyone know the numbers, or any source where I can look?What I want to know is if they limit the number of calls allowed per minute or second. I'm looking for information on the the paid (not free) service tier.""","I'm looking for information on the the paid (not free) service tier."""
360,45048382,,0,,"[{'score': 0.87913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.87913,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage."
361,45048382,,1,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods."
362,45048382,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",It takes about 2-6 seconds per image.
363,45048382,,3,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,TRUE,0.968123,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",I'm wondering if there's any way to reduce response time.
364,45048382,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",I first used TinyPNG to compress the images.
365,45048382,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement.
366,45048382,,6,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",How can I improve response time?
367,45048382,,7,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,TRUE,0.968123,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",I'll list some ideas.1)
368,45048382,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image."
369,45048382,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Is it faster to send a base64 encoded image?
370,45048382,,10,,"[{'score': 0.616932, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.616932,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection."
371,45048382,,11,,"[{'score': 0.856606, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856606,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project?
372,45048382,,12,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","If so, should I limit responses?"
373,45048382,,13,,"[{'score': 0.65832, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.65832,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Labels with less than a 0.6 confidence score don't seem of much help.
374,45048382,,14,,"[{'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659112,TRUE,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""","Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision."""
375,56399486,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""","""When I make a web detection request to the Google Vision API, I get back a bunch of web entities."
376,56399486,,1,,"[{'score': 0.634219, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.634219,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""","The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null."
377,56399486,,2,,"[{'score': 0.880052, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.880052,FALSE,0,FALSE,0,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""","I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted?"
378,56399486,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""",Also...
379,56399486,,4,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.681699,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""","I know this is off topic, but what is the entityId even used for?"
380,56399486,,5,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""","I can't find much documentation on either of these other than the comments in the code:"""
381,47780934,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""","""I have quite a challenging use case for image recognition."
382,47780934,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",I want to detect composition of mixed recycling e.g.
383,47780934,,2,,"[{'score': 0.788532, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.755313, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.755313,FALSE,0,TRUE,0.788532,TRUE,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""","Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images?"
384,47780934,,3,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.538448, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.538448,FALSE,0,TRUE,0.58393,TRUE,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""","I guess I wouldn't need to use google for this as tensor flow is open source?Thanks."""
385,49635343,,0,,"[{'score': 0.802757, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802757,FALSE,0,FALSE,0,TRUE,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""","""I am having an issue with detecting Hindi fonts using Google Vision API OCR service."
386,49635343,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",The documentatio says that Hindi is supported.When
387,49635343,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""","I drag and drop an image on their demo page (), it works flawlessly."
388,49635343,,3,,"[{'score': 0.786751, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.786751,FALSE,0,FALSE,0,TRUE,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""","However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese."
389,49635343,,4,,"[{'score': 0.760584, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.760584,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.933436,FALSE,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""","Those seem to be working fine.Is there something I am missing here?"""
390,56313325,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links. Thanks""","""I am streaming video the amazon kinesis from raspberry pi (This is done)."
391,56313325,,1,,"[{'score': 0.69572, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69572,FALSE,0,FALSE,0,TRUE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links. Thanks""",Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links.
392,56313325,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links. Thanks""","Thanks"""
393,37152708,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""","""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app."
394,37152708,,1,,"[{'score': 0.727743, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727743,FALSE,0,FALSE,0,TRUE,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""","I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found."
395,37152708,,2,,"[{'score': 0.537081, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.537081,FALSE,0,FALSE,0,TRUE,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""","As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this?"
396,37152708,,3,,"[{'score': 0.849827, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.944551, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.944551,TRUE,0.849827,FALSE,0,TRUE,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""","I would greatly appreciate it."""
397,55570088,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""","""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image."
398,55570088,,1,,"[{'score': 0.748567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.748567,FALSE,0,FALSE,0,TRUE,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""","After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message."
399,55570088,,2,,"[{'score': 0.644721, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.644721,FALSE,0,FALSE,0,TRUE,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""","I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution."""
400,50733603,,0,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.then I want to call this function one by one for all images""","""my bucket on s3 is named as 'python' and its subfolder is  'boss' ."
401,50733603,,1,,"[{'score': 0.775702, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.775702,FALSE,0,TRUE,"""my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.then I want to call this function one by one for all images""",So I want to get all images of folder boss in lambda function.
402,50733603,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.then I want to call this function one by one for all images""",currently I am hard-coding values but putting image in root not in subfolder.then
403,50733603,,3,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80026,FALSE,0,TRUE,"""my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.then I want to call this function one by one for all images""","I want to call this function one by one for all images"""
404,49134014,,0,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""","""I am currently working on the response from the."
405,49134014,,1,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""","In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response."
406,49134014,,2,,"[{'score': 0.816427, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.940677, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.816427,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.940677,FALSE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""","Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness""."
407,49134014,,3,,"[{'score': 0.795748, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.795748,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",I tried to use json_decode onbut I got an error:.
408,49134014,,4,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",It seems that HTTP_Request2_Response provide me an object.
409,49134014,,5,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,TRUE,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""","How do I extract the data I want from the response?Here is theoffor your reference:"""
410,51102444,,0,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"" is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?""","""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application."
411,51102444,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"" is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?""","For now I can only implement one ""type"" in the POST, but I want to have more than one."
412,51102444,,2,,"[{'score': 0.540526, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.540526,FALSE,0,FALSE,0,TRUE,"""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"" is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?""","In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"""
413,51102444,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"" is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?""","is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?"""
414,51815006,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""","""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image)."
415,51815006,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes.
416,51815006,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",This is the image taken from the frame and sent to google vision logo detection.
417,51815006,,3,,"[{'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.97759,FALSE,0,TRUE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",It detects Walmart perfectly.
418,51815006,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",The red numbers are the vertices I get returned.
419,51815006,,5,,"[{'score': 0.574966, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.739299, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.574966,FALSE,0,FALSE,0,TRUE,0.739299,FALSE,0,FALSE,0,FALSE,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""","Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results."""
420,48019749,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""","""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html."
421,48019749,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""",You can see the nested JSON (data) as followed.
422,48019749,,2,,"[{'score': 0.758915, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.802215, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.758915,FALSE,0,FALSE,0,FALSE,0,TRUE,0.802215,FALSE,0,FALSE,0,FALSE,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""","On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!"""
423,35790590,,0,,"[{'score': 0.627156, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.627156,FALSE,0,FALSE,0,TRUE,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""","""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases."
424,35790590,,1,,"[{'score': 0.743447, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.743447,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.88939,FALSE,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""","However, sometimes I fail to have the intended result either when it's too wide or long."
425,35790590,,2,,"[{'score': 0.830239, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.877442, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.830239,FALSE,0,TRUE,0.877442,TRUE,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""","Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me."
426,35790590,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",":)Thank you in advance."""
427,38292746,,0,,"[{'score': 0.628439, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.628439,FALSE,0,FALSE,0,TRUE,"""Good afternoon!Tell me how to overlay an image (ImageView), anchoring the to the eyes?I want to track using Google Vision API.Maybe there is an example of how to put a hat on head?Added. (12 Jule 2016)Use the following code:not anchoring (blue square) strictly to the eye, you can see it in the screenshots with different distance from the photos:Please tell me why this happens?""","""Good afternoon!Tell me how to overlay an image (ImageView), anchoring the to the eyes?I want to track using Google Vision API.Maybe there is an example of how to put a hat on head?Added."
428,38292746,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Good afternoon!Tell me how to overlay an image (ImageView), anchoring the to the eyes?I want to track using Google Vision API.Maybe there is an example of how to put a hat on head?Added. (12 Jule 2016)Use the following code:not anchoring (blue square) strictly to the eye, you can see it in the screenshots with different distance from the photos:Please tell me why this happens?""","(12 Jule 2016)Use the following code:not anchoring (blue square) strictly to the eye, you can see it in the screenshots with different distance from the photos:Please tell me why this happens?"""
429,34310579,,0,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?""","""I'm using the Microsoft computer vision API."
430,34310579,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?""","The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender."
431,34310579,,2,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?""","However, I have a ""do"" loop which I can't ""rescue."""
432,34310579,,3,,"[{'score': 0.588818, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.588818,FALSE,0,FALSE,0,TRUE,"""I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?""","Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?"""
433,36405717,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?""","""I'm trying to implement Google Visions scanner into an app im working on."
434,36405717,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?""","By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window."
435,36405717,,2,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?""","For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?"""
436,42345567,,0,,"[{'score': 0.522484, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.522484,FALSE,0,FALSE,0,TRUE,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""","""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions?"
437,42345567,,1,,"[{'score': 0.939594, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.939594,FALSE,0,FALSE,0,TRUE,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""","If yes, please let us know the procedure."
438,42345567,,2,,"[{'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.821444, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821444,FALSE,0,TRUE,0.845297,TRUE,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""","If not, can you please suggest some other OCRs which can serve the purpose?"
439,42345567,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""","We need them as API, which we can integrate with our app.Thank you in Advance."""
440,52081400,,0,,"[{'score': 0.64, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.64,FALSE,0,FALSE,0,TRUE,"""The application only able to translate the image to text with using res/drawable image but i need to create an application that able to capture a photo and display it on ImageView and let the google vision api to translate the image to text. Can anyone tell me how to do it or other way to do it.}""","""The application only able to translate the image to text with using res/drawable image but i need to create an application that able to capture a photo and display it on ImageView and let the google vision api to translate the image to text."
441,52081400,,1,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""The application only able to translate the image to text with using res/drawable image but i need to create an application that able to capture a photo and display it on ImageView and let the google vision api to translate the image to text. Can anyone tell me how to do it or other way to do it.}""","Can anyone tell me how to do it or other way to do it.}"""
442,55024058,,0,,"[{'score': 0.523417, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.523417,FALSE,0,FALSE,0,TRUE,"""I am developing an application that uses theGoogle vision APIand I have a question about the colors properties.Is the color shown in the properties with the highest percentage is the dominant color? And how that works ?Because the color with highest percentage is not accurate.""","""I am developing an application that uses theGoogle vision APIand I have a question about the colors properties.Is the color shown in the properties with the highest percentage is the dominant color?"
443,55024058,,1,,"[{'score': 0.718563, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,TRUE,0.718563,TRUE,"""I am developing an application that uses theGoogle vision APIand I have a question about the colors properties.Is the color shown in the properties with the highest percentage is the dominant color? And how that works ?Because the color with highest percentage is not accurate.""","And how that works ?Because the color with highest percentage is not accurate."""
444,56403969,,0,,"[{'score': 0.510236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.510236,FALSE,0,FALSE,0,TRUE,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""","""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document."
445,56403969,,1,,"[{'score': 0.787363, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.787363,FALSE,0,FALSE,0,TRUE,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""","One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together."
446,56403969,,2,,"[{'score': 0.670488, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670488,FALSE,0,FALSE,0,TRUE,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""","Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening."
447,56403969,,3,,"[{'score': 0.836719, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.836719,FALSE,0,FALSE,0,TRUE,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""","We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:"""
448,49711906,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:But it gives me this response:""","""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs."
449,49711906,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:But it gives me this response:""",I have a base64 encoded image data.
450,49711906,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:But it gives me this response:""",I want to send it to the Face API.
451,49711906,,3,,"[{'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,FALSE,0,TRUE,"""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:But it gives me this response:""","I'm using the following code:But it gives me this response:"""
452,54839576,,0,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""","""I followed the above tutorial for using the Azure Custom Vision Python SDK."
453,54839576,,1,,"[{'score': 0.867256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867256,FALSE,0,FALSE,0,TRUE,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""","Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer."
454,54839576,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""",How can I do that?
455,54839576,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""","Thanks!"""
456,53918980,,0,,"[{'score': 0.603247, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.563256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.603247,FALSE,0,FALSE,0,TRUE,0.563256,FALSE,0,FALSE,0,FALSE,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""","""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their"
457,53918980,,1,,"[{'score': 0.840228, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840228,FALSE,0,FALSE,0,TRUE,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""",answer was to pass in the subscription key and endpoint for theconstructor.)Would
458,53918980,,2,,"[{'score': 0.762617, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.827595, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.797389, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.762617,FALSE,0,FALSE,0,TRUE,0.797389,FALSE,0,TRUE,0.827595,FALSE,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""","anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with."
459,53918980,,3,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,TRUE,0.615352,TRUE,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""",I also read that it may be due to the Cognitive Services pricing plan I have chosen.
460,53918980,,4,,"[{'score': 0.773514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.773514,FALSE,0,FALSE,0,TRUE,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""","However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people."""
461,53275683,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""","""I want to draw lines around face (including forehead) and cut that face out from the image."
462,53275683,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""",Can I use Google Vision API to realize my goal?
463,53275683,,2,,"[{'score': 0.512075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.512075,FALSE,0,TRUE,0.647986,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""","I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression."
464,53275683,,3,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""",It cannot detects the coordinates of outline around face.
465,53275683,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""",How to do that with Vision API?
466,53275683,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""","If Vision API cannot do it, than what library should I use?"""
467,53963357,,0,,"[{'score': 0.809111, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.809111,FALSE,0,FALSE,0,TRUE,"""What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?Ex.:The json response shows the four coordinates of the bounding boxes in a clockwise disposition. However, I haven't found the unit. I assume that it is pixels, but it's not written anywhere...The API is available here:""","""What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?Ex.:The json response shows the four coordinates of the bounding boxes in a clockwise disposition."
468,53963357,,1,,"[{'score': 0.961871, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.961871,FALSE,0,FALSE,0,TRUE,"""What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?Ex.:The json response shows the four coordinates of the bounding boxes in a clockwise disposition. However, I haven't found the unit. I assume that it is pixels, but it's not written anywhere...The API is available here:""","However, I haven't found the unit."
469,53963357,,2,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.615352,TRUE,"""What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?Ex.:The json response shows the four coordinates of the bounding boxes in a clockwise disposition. However, I haven't found the unit. I assume that it is pixels, but it's not written anywhere...The API is available here:""","I assume that it is pixels, but it's not written anywhere...The API is available here:"""
470,47537811,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I am trying to build an android application (in android studio platform) which extracts different text languages from image using google cloud vision, but I have a problem in starting.I don't know how to use google cloud files. Which files do I need to create or download and how to direct my API to extract multiple languages?I got the API and this source code :""","""I am trying to build an android application (in android studio platform) which extracts different text languages from image using google cloud vision, but I have a problem in starting.I don't know how to use google cloud files."
471,47537811,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build an android application (in android studio platform) which extracts different text languages from image using google cloud vision, but I have a problem in starting.I don't know how to use google cloud files. Which files do I need to create or download and how to direct my API to extract multiple languages?I got the API and this source code :""","Which files do I need to create or download and how to direct my API to extract multiple languages?I got the API and this source code :"""
472,56405595,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""","""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4,"
473,56405595,,1,,"[{'score': 0.544782, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.544782,FALSE,0,TRUE,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""","""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]"""
474,50699149,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""","""Hi I want to write a lambda function which will work like."
475,50699149,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""","I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures."
476,50699149,,2,,"[{'score': 0.928991, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.6821, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.928991,TRUE,0.6821,FALSE,0,TRUE,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""",I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture .
477,50699149,,3,,"[{'score': 0.591103, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.591103,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.822231,FALSE,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""","Atm I am lost in API of face detection and doing some thing  like this"""
478,36977715,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""With the IBM Watson Visual Recognition API, how do you know the Classifier ID when we create the train? How do you do multiple classifying?""","""With the IBM Watson Visual Recognition API, how do you know the Classifier ID when we create the train?"
479,36977715,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""With the IBM Watson Visual Recognition API, how do you know the Classifier ID when we create the train? How do you do multiple classifying?""","How do you do multiple classifying?"""
480,43882577,,0,,"[{'score': 0.735063, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.683782, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.735063,FALSE,0,FALSE,0,TRUE,0.683782,FALSE,0,FALSE,0,FALSE,"""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result. It is not getting populated.""","""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result."
481,43882577,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result. It is not getting populated.""","It is not getting populated."""
482,41923595,,0,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I'm trying to post data to Google Vision API. Butt i always get 400 response code - bad request. I have no ideas already.I have read and tried to use those links:And i came up to this:Here is my data to post:And here is my post:Here is data from console (THE CONTENT IS BLANK FOR EXAMPLE( not to post wole base64 )):And here is the response:Where is my mistake here?""","""I'm trying to post data to Google Vision API."
483,41923595,,1,,"[{'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874372,FALSE,0,TRUE,"""I'm trying to post data to Google Vision API. Butt i always get 400 response code - bad request. I have no ideas already.I have read and tried to use those links:And i came up to this:Here is my data to post:And here is my post:Here is data from console (THE CONTENT IS BLANK FOR EXAMPLE( not to post wole base64 )):And here is the response:Where is my mistake here?""",Butt i always get 400 response code - bad request.
484,41923595,,2,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I'm trying to post data to Google Vision API. Butt i always get 400 response code - bad request. I have no ideas already.I have read and tried to use those links:And i came up to this:Here is my data to post:And here is my post:Here is data from console (THE CONTENT IS BLANK FOR EXAMPLE( not to post wole base64 )):And here is the response:Where is my mistake here?""","I have no ideas already.I have read and tried to use those links:And i came up to this:Here is my data to post:And here is my post:Here is data from console (THE CONTENT IS BLANK FOR EXAMPLE( not to post wole base64 )):And here is the response:Where is my mistake here?"""
485,56026129,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""","""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point."
486,56026129,,1,,"[{'score': 0.590156, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.590156,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""","I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great."""
487,55158595,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""","""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels."
488,55158595,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""","Why does it show 'filtered' and how can I show the labels?"""
489,48037316,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""","""I am trying to build an android application where I am usingfor detecting faces."
490,48037316,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""",I am followingtutorial.
491,48037316,,2,,"[{'score': 0.820626, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.519491, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.820626,FALSE,0,FALSE,0,TRUE,0.519491,FALSE,0,FALSE,0,FALSE,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""","The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe"
492,48037316,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""",picture is stored inlocation as.
493,48037316,,4,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""","I have tried setting the types as,,but none of it works.Can anyone help me in this?"""
494,52186137,,0,,"[{'score': 0.723505, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.723505,FALSE,0,FALSE,0,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""","""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json,"
495,52186137,,1,,"[{'score': 0.963382, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.963382,FALSE,0,FALSE,0,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",which the Google client libraries use in order to construct API requests.
496,52186137,,2,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries.
497,52186137,,3,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,TRUE,0.647986,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""","The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json"
498,52186137,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",file has been uploaded to the app service.
499,52186137,,5,,"[{'score': 0.501524, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.501524,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""","Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong?"
500,52186137,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""","How can I successfully use the Google Cloud Vision API on an Azure web app?"""
501,56014699,,0,,"[{'score': 0.718038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718038,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for detect text in image.Now, I want to know font size of text in image. How do it calculate ?or Do google vision api support?""","""I am using google vision api for detect text in image.Now, I want to know font size of text in image."
502,56014699,,1,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""I am using google vision api for detect text in image.Now, I want to know font size of text in image. How do it calculate ?or Do google vision api support?""",How do it calculate ?or
503,56014699,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using google vision api for detect text in image.Now, I want to know font size of text in image. How do it calculate ?or Do google vision api support?""","Do google vision api support?"""
504,55979551,,0,,"[{'score': 0.591068, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.591068,FALSE,0,FALSE,0,TRUE,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""","""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results."
505,55979551,,1,,"[{'score': 0.660103, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660103,FALSE,0,FALSE,0,TRUE,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""","I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script."
506,55979551,,2,,"[{'score': 0.996505, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.996505,TRUE,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""","Has anyone tried it?"""
507,53269405,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""","""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK."
508,53269405,,1,,"[{'score': 0.508513, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.508513,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""","I do, however have a problem with it."
509,53269405,,2,,"[{'score': 0.663914, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.663914,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,FALSE,0,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",After a long time trying to figure out what's wrong and I still haven't figured it out.
510,53269405,,3,,"[{'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,TRUE,0.874372,FALSE,0,TRUE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",I'm making a request as follows;That's all fine.
511,53269405,,4,,"[{'score': 0.584466, 'tone_id': 'anger', 'tone_name': 'Anger'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.584466,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",No excepton is thrown.
512,53269405,,5,,"[{'score': 0.557154, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.970179, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.557154,FALSE,0,FALSE,0,TRUE,0.970179,FALSE,0,FALSE,0,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""","However, the result ofis empty."
513,53269405,,6,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",It is null.
514,53269405,,7,,"[{'score': 0.584466, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,TRUE,0.584466,TRUE,0.762356,FALSE,0,TRUE,0.984352,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",Without any error thrown.
515,53269405,,8,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,TRUE,0.525007,TRUE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""","I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am"
516,53269405,,9,,"[{'score': 0.633464, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.633464,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,FALSE,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""","I doing something wrong or am I missing something?"""
517,53591219,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""","""this is my code."
518,53591219,,1,,"[{'score': 0.873624, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873624,FALSE,0,FALSE,0,TRUE,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""",I'm using mobile google vision API.
519,53591219,,2,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""",I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please
520,53591219,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""",tell me how to get text in proper sequence.
521,53591219,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""","Thank you in advance"""
522,33687536,,0,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""","""I am trying to try the."
523,33687536,,1,,"[{'score': 0.502426, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.502426,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",I am running a simple python web server with CORS enabled.
524,33687536,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",Below is my server python file with which I start the server:python-server.pyI have an index.html
525,33687536,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response.
526,33687536,,4,,"[{'score': 0.608306, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.608306,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",The http request code was taken from the emotion api's page I linked earlier.
527,33687536,,5,,"[{'score': 0.798791, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,TRUE,0.798791,TRUE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",I wonder whether I need a real server or is there a mistake in the code?
528,33687536,,6,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""","Thanks."""
529,49652382,,0,,"[{'score': 0.548266, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.548266,FALSE,0,FALSE,0,TRUE,"""Being locked on .NET 2.0, i can't use the GOOGLE VISION C# API which is only available since.NET 4.0.So I wanted to use this API with web request like this :The problem is that i permanently have a 400 return from google and after a lot of searching i can't find a solution.Can you give me a way to proceed to solve this problem please ?Thank you very much.(Sorry for bad english...)""","""Being locked on .NET 2.0, i can't use the GOOGLE VISION C# API which is only available since.NET 4.0.So I wanted to use this API with web request like this :The problem is that i permanently have a 400 return from google and after a lot of searching i can't find a solution.Can you give me a way to proceed to solve this problem please ?Thank you very much.(Sorry"
530,49652382,,1,,"[{'score': 0.691873, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.691873,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Being locked on .NET 2.0, i can't use the GOOGLE VISION C# API which is only available since.NET 4.0.So I wanted to use this API with web request like this :The problem is that i permanently have a 400 return from google and after a lot of searching i can't find a solution.Can you give me a way to proceed to solve this problem please ?Thank you very much.(Sorry for bad english...)""","for bad english...)"""
531,38336213,,0,,"[{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,TRUE,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""","""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for."
532,38336213,,1,,"[{'score': 0.618118, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.618118,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,FALSE,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""","I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online."
533,38336213,,2,,"[{'score': 0.51536, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.51536,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,TRUE,0.946222,FALSE,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""","Any advice or pointing me in the right direction would be appreciated."""
534,54295015,,0,,"[{'score': 0.803567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.803567,FALSE,0,FALSE,0,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""","""I'm using thefeature of Google Cloud Vision API."
535,54295015,,1,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.88939,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""","However, for some images, the JSON response I receive don't have description parameters for some entities."
536,54295015,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""","On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/""."
537,54295015,,3,,"[{'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.976993,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""",Can anyone suggest how should I go about this?
538,54295015,,4,,"[{'score': 0.96417, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.96417,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""",Is this a bug or is this supposed to behave like this only?
539,54295015,,5,,"[{'score': 0.539235, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.539235,TRUE,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""","Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description."""
540,40957513,,0,,"[{'score': 0.926735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926735,FALSE,0,FALSE,0,TRUE,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""","""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator."
541,40957513,,1,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.983522, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.983522,TRUE,0.543112,FALSE,0,TRUE,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""",I need to generate distracters (Wrong answers) from the Key (correct answer).
542,40957513,,2,,"[{'score': 0.896021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.896021,FALSE,0,FALSE,0,TRUE,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""",The MCQ is generated from educational texts that users input.
543,40957513,,3,,"[{'score': 0.515711, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515711,TRUE,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""","We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help?"
544,40957513,,4,,"[{'score': 0.936807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.936807,FALSE,0,FALSE,0,TRUE,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""","I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python."""
545,55120982,,0,,"[{'score': 0.582768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.582768,FALSE,0,FALSE,0,TRUE,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""","""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1)"
546,55120982,,1,,"[{'score': 0.808152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.808152,FALSE,0,FALSE,0,TRUE,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3)
547,55120982,,2,,"[{'score': 0.929993, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.929993,FALSE,0,FALSE,0,TRUE,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""","Is there a way to train the platform to aggregate the info according to what we want to extract?"""
548,47831050,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to create a collection of faces from 1500 face images and thenthis collection with one reference face image. The final goal is to find which face from the collection is the most similar one to the reference face image.So I want to retrieve one number for similarity for each pair of images (reference image and one face from the collection) each time.So does this amount to 1500faces x 1similarity_metadata = 1500metadata or the similarity attribute is counted as one metadata for any number of face images?In other words, does my request amount to 1500 metadata or 1 metadata for the 1500 faces?I am using the free version and AWS specifies that:So I am asking this because I do not want to exceed the limit of 1000 face metadata each month.""","""I want to create a collection of faces from 1500 face images and thenthis collection with one reference face image."
549,47831050,,1,,"[{'score': 0.516492, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.516492,FALSE,0,FALSE,0,TRUE,"""I want to create a collection of faces from 1500 face images and thenthis collection with one reference face image. The final goal is to find which face from the collection is the most similar one to the reference face image.So I want to retrieve one number for similarity for each pair of images (reference image and one face from the collection) each time.So does this amount to 1500faces x 1similarity_metadata = 1500metadata or the similarity attribute is counted as one metadata for any number of face images?In other words, does my request amount to 1500 metadata or 1 metadata for the 1500 faces?I am using the free version and AWS specifies that:So I am asking this because I do not want to exceed the limit of 1000 face metadata each month.""","The final goal is to find which face from the collection is the most similar one to the reference face image.So I want to retrieve one number for similarity for each pair of images (reference image and one face from the collection) each time.So does this amount to 1500faces x 1similarity_metadata = 1500metadata or the similarity attribute is counted as one metadata for any number of face images?In other words, does my request amount to 1500 metadata or 1 metadata for the 1500 faces?I am using the free version and AWS specifies that:So I am asking this because I do not want to exceed the limit of 1000 face metadata each month."""
550,40695516,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""","""I am trying to use OCR feature in Google Vision API but not able to receive expected result."
551,40695516,,1,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""","I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results."
552,40695516,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""","Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German."
553,40695516,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample.
554,40695516,,4,,"[{'score': 0.853486, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.853486,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,FALSE,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""","Thank you for any advices."""
555,50360498,,0,,"[{'score': 0.577448, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.577448,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,FALSE,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""","""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis."
556,50360498,,1,,"[{'score': 0.941308, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.941308,FALSE,0,FALSE,0,TRUE,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""","After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected."
557,50360498,,2,,"[{'score': 0.941001, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.941001,FALSE,0,FALSE,0,TRUE,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""","But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different."
558,50360498,,3,,"[{'score': 0.999857, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999857,TRUE,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""","Any suggestions?"""
559,42044047,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""","""I am trying to reproduce in R the Microsoft Emotion API program located."
560,42044047,,1,,"[{'score': 0.835627, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.835627,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""","I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid."
561,42044047,,2,,"[{'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",I tried to regenerate the key and run it again but received the same result.
562,42044047,,3,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,TRUE,0.647986,TRUE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Maybe it has something to do with the fact that Microsoft gives me 2 keys?
563,42044047,,4,,"[{'score': 0.560998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560998,FALSE,0,FALSE,0,TRUE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""","Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds."
564,42044047,,5,,"[{'score': 0.68626, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.825338, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.68626,FALSE,0,FALSE,0,TRUE,0.825338,FALSE,0,TRUE,0.58393,FALSE,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""","Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure."""
565,51746269,,0,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module. But I always get the following error. What am I doing wrong?I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.My service authentication informations (just test data):My Node.js code to create the VisualRecognizionV3 object:I will appreciate your help!""","""I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module."
566,51746269,,1,,"[{'score': 0.739676, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.739676,FALSE,0,FALSE,0,TRUE,0.560098,TRUE,0.92125,FALSE,0,FALSE,"""I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module. But I always get the following error. What am I doing wrong?I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.My service authentication informations (just test data):My Node.js code to create the VisualRecognizionV3 object:I will appreciate your help!""",But I always get the following error.
567,51746269,,2,,"[{'score': 0.818954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.818954,FALSE,0,FALSE,0,TRUE,"""I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module. But I always get the following error. What am I doing wrong?I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.My service authentication informations (just test data):My Node.js code to create the VisualRecognizionV3 object:I will appreciate your help!""","What am I doing wrong?I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.My service authentication informations (just test data):My Node.js code to create the VisualRecognizionV3 object:I will appreciate your help!"""
568,48748566,,0,,"[{'score': 0.562568, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.562568,FALSE,0,FALSE,0,TRUE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""","""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz"
569,48748566,,1,,"[{'score': 0.568222, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.579436, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.568222,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579436,FALSE,0,FALSE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",",and I run command,it was successful.I wrote test.pyand"
570,48748566,,2,,"[{'score': 0.54684, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.54684,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""","when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials."
571,48748566,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application.
572,48748566,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""","For moreinformation, please see. ."
573,48748566,,5,,"[{'score': 0.774085, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.774085,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,FALSE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",I really cannot understand why this error happens.I installed config.json
574,48748566,,6,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,FALSE,0,TRUE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",& index.js
575,48748566,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",& package.json in same directory as test.py
576,48748566,,8,,"[{'score': 0.791939, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.657517, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.791939,FALSE,0,FALSE,0,TRUE,0.657517,FALSE,0,FALSE,0,FALSE,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""","but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?"""
577,52323135,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a template like thisand wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?Thks""","""I have a template like thisand wanted to use google vision api to extract certain fields."
578,52323135,,1,,"[{'score': 0.837998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.837998,FALSE,0,FALSE,0,TRUE,"""I have a template like thisand wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?Thks""","Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field."
579,52323135,,2,,"[{'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,TRUE,"""I have a template like thisand wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?Thks""","Can someone suggest a path?Thks"""
580,49918950,,0,,"[{'score': 0.618451, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.618451,FALSE,0,TRUE,"""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened. Is it because the wrong format of sentin?""","""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g."
581,49918950,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened. Is it because the wrong format of sentin?""","rotating, cropping, resizing, etc.) before and during OCR."
582,49918950,,2,,"[{'score': 0.52517, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.625794, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.52517,FALSE,0,FALSE,0,TRUE,0.625794,FALSE,0,FALSE,0,FALSE,"""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened. Is it because the wrong format of sentin?""","Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened."
583,49918950,,3,,"[{'score': 0.527041, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.803567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.527041,FALSE,0,FALSE,0,TRUE,0.803567,FALSE,0,FALSE,0,FALSE,"""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened. Is it because the wrong format of sentin?""","Is it because the wrong format of sentin?"""
584,51866993,,0,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""We've got a pretty extensive BI system built on/. We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using. Here is the relevantif it helps:""","""We've got a pretty extensive BI system built on/."
585,51866993,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We've got a pretty extensive BI system built on/. We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using. Here is the relevantif it helps:""","We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using."
586,51866993,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""We've got a pretty extensive BI system built on/. We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using. Here is the relevantif it helps:""","Here is the relevantif it helps:"""
587,48188923,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.659112, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,TRUE,0.659112,TRUE,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""","""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION""."
588,48188923,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,TRUE,0.525007,TRUE,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""","I have figured out, that it happens when the image size differs only some pixels from a common ratio."
589,48188923,,2,,"[{'score': 0.525926, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525926,FALSE,0,TRUE,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""","I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel."
590,48188923,,3,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""",But only if the image ratio is close to a common ratio.
591,48188923,,4,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""","(16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks"""
592,43852275,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this.""","""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem."
593,43852275,,1,,"[{'score': 0.92354, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.92354,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,FALSE,"""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this.""",I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.
594,43852275,,2,,"[{'score': 0.628442, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.628442,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this.""","Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this."""
595,46281898,,0,,"[{'score': 0.641084, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.641084,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am usingin a project. My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result. Is this possible?""","""I am usingin a project."
596,46281898,,1,,"[{'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,TRUE,"""I am usingin a project. My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result. Is this possible?""",My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result.
597,46281898,,2,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""I am usingin a project. My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result. Is this possible?""","Is this possible?"""
598,40189866,,0,,"[{'score': 0.642915, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.642915,FALSE,0,FALSE,0,TRUE,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""","""I'm using google vision API for detecting names and numbers on running bibs."
599,40189866,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""",See below for a typical image.
600,40189866,,2,,"[{'score': 0.933436, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.67368, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.67368,FALSE,0,TRUE,0.933436,TRUE,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""","Any pointers of font or layout that would give the best detection result?"""
601,13617087,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Im using an API to CURL a submitted user image to a remote server, the response is in JSON and I'm not sure how to parse it.Here is my php CURL fileand here is a sample of the JSON data that comes back (Generic)Code:Where the json string says ""name"" I need my script to only print the username if the number (after the : is higher then a threshold (lets say .70 for now).How do I do this? I've worked with XML api's before and returning the data was simple with aCode:type thing.""","""Im using an API to CURL a submitted user image to a remote server, the response is in JSON and I'm not sure how to parse it.Here is my php CURL fileand here is a sample of the JSON data that comes back (Generic)Code:Where the json string says ""name"" I need my script to only print the username if the number (after the : is higher then a threshold (lets say .70 for now).How do I do this?"
602,13617087,,1,,"[{'score': 0.538972, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.538972,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Im using an API to CURL a submitted user image to a remote server, the response is in JSON and I'm not sure how to parse it.Here is my php CURL fileand here is a sample of the JSON data that comes back (Generic)Code:Where the json string says ""name"" I need my script to only print the username if the number (after the : is higher then a threshold (lets say .70 for now).How do I do this? I've worked with XML api's before and returning the data was simple with aCode:type thing.""","I've worked with XML api's before and returning the data was simple with aCode:type thing."""
603,53675809,,0,,"[{'score': 0.834329, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.778006, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.834329,FALSE,0,FALSE,0,TRUE,0.778006,FALSE,0,FALSE,0,FALSE,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""","""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time."
604,53675809,,1,,"[{'score': 0.750982, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.750982,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,FALSE,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""",There is no problem on aws keys and photos.
605,53675809,,2,,"[{'score': 0.735382, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.735382,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""","I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart"
606,53675809,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""","codeAWS rekognition code"""
607,49444278,,0,,"[{'score': 0.712952, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.712952,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,FALSE,"""I'm having a problem withusing Amazonaws-cpp-sdk. I'm gettingsegmentationfault in following program.So, how to provide an image file from my local system to amazon aws-cpp-sdk?""","""I'm having a problem withusing Amazonaws-cpp-sdk."
608,49444278,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having a problem withusing Amazonaws-cpp-sdk. I'm gettingsegmentationfault in following program.So, how to provide an image file from my local system to amazon aws-cpp-sdk?""","I'm gettingsegmentationfault in following program.So, how to provide an image file from my local system to amazon aws-cpp-sdk?"""
609,52326489,,0,,"[{'score': 0.931038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.931038,FALSE,0,FALSE,0,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""","""I'm exploring the Google Vision API for OCR."
610,52326489,,1,,"[{'score': 0.821913, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.821913,FALSE,0,FALSE,0,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",We have lots of forms that are computer generated and filled by users.
611,52326489,,2,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.681699,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Like the Medical Reports and Registration Forms.
612,52326489,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",We need to process those images and get the character out of it.
613,52326489,,4,,"[{'score': 0.581583, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.616932, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.581583,FALSE,0,FALSE,0,FALSE,0,TRUE,0.616932,FALSE,0,FALSE,0,FALSE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""","I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues."
614,52326489,,5,,"[{'score': 0.718038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718038,FALSE,0,FALSE,0,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Like If fill the form with the data a little above the y axis the words is considered as previous/next line.
615,52326489,,6,,"[{'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.680245, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,TRUE,0.680245,TRUE,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""","Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?"""
616,36570132,,0,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?""","""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call."
617,36570132,,1,,"[{'score': 0.63683, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.530561, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.862286, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.63683,FALSE,0,FALSE,0,TRUE,0.862286,FALSE,0,TRUE,0.530561,FALSE,"""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?""","I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?"""
618,53224704,,0,,"[{'score': 0.632275, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.632275,FALSE,0,FALSE,0,TRUE,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""","""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD."
619,53224704,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""",now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it.
620,53224704,,2,,"[{'score': 0.535408, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.535408,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""","You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)"""
621,55956173,,0,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""","""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task."
622,55956173,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",": this plan instance can have only 2 custom classifier(s), and 2 already exist."""
623,55956173,,2,,"[{'score': 0.804906, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.804906,FALSE,0,FALSE,0,TRUE,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",It will not allow me to train my model.
624,55956173,,3,,"[{'score': 0.994446, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994446,TRUE,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",Can anyone help?
625,55956173,,4,,"[{'score': 0.505036, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.505036,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""","Thank you in advance!"""
626,51187280,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building anofflineionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it takespicture as an inputandreturns a string.this returns an output asHow can we access the python script or how to make it as a service ?""","""I am building anofflineionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it"
627,51187280,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building anofflineionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it takespicture as an inputandreturns a string.this returns an output asHow can we access the python script or how to make it as a service ?""",takespicture as an inputandreturns a string.this
628,51187280,,2,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am building anofflineionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it takespicture as an inputandreturns a string.this returns an output asHow can we access the python script or how to make it as a service ?""","returns an output asHow can we access the python script or how to make it as a service ?"""
629,51737440,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:""","""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen."
630,51737440,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.647986,TRUE,"""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:""",Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye.
631,51737440,,2,,"[{'score': 0.734478, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.734478,FALSE,0,FALSE,0,TRUE,"""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:""",Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection?
632,51737440,,3,,"[{'score': 0.718038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718038,FALSE,0,FALSE,0,TRUE,"""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:""","These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:"""
633,50275116,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""","""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster."
634,50275116,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",My application receives a photo and sends it to the Google Vision API.
635,50275116,,2,,"[{'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,FALSE,0,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""","I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env."
636,50275116,,3,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster.
637,50275116,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong?
638,50275116,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""","Thx in advance!"""
639,41501511,,0,,"[{'score': 0.904038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.904038,FALSE,0,FALSE,0,TRUE,"""I have extracted tags from the given image using Clarifai and Google Vision APIs. Similar thing I want to achieve for videos.Can anyone suggest, if there are any APIs available to do so.Thanks!""","""I have extracted tags from the given image using Clarifai and Google Vision APIs."
640,41501511,,1,,"[{'score': 0.806226, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.876534, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.806226,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.876534,FALSE,"""I have extracted tags from the given image using Clarifai and Google Vision APIs. Similar thing I want to achieve for videos.Can anyone suggest, if there are any APIs available to do so.Thanks!""","Similar thing I want to achieve for videos.Can anyone suggest, if there are any APIs available to do so.Thanks!"""
641,45484524,,0,,"[{'score': 0.7922, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.7922,FALSE,0,FALSE,0,TRUE,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""","""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results)."
642,45484524,,1,,"[{'score': 0.668856, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.668856,FALSE,0,FALSE,0,TRUE,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""","Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)?"
643,45484524,,2,,"[{'score': 0.886021, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.886021,FALSE,0,FALSE,0,TRUE,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""","Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?"""
644,53105446,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""","""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition)."
645,53105446,,1,,"[{'score': 0.657083, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.657083,FALSE,0,FALSE,0,TRUE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js
646,53105446,,2,,"[{'score': 0.710598, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.710598,FALSE,0,FALSE,0,TRUE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3.
647,53105446,,3,,"[{'score': 0.726914, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.726914,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""","But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package."
648,53105446,,4,,"[{'score': 0.762078, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.727988, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.762078,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.727988,FALSE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?.
649,53105446,,5,,"[{'score': 0.70376, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.70376,FALSE,0,FALSE,0,TRUE,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""","I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!"""
650,41527687,,0,,"[{'score': 0.91961, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.91961,TRUE,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""","""I'm playing around with the Amazon Rekognition."
651,41527687,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""","I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library."
652,41527687,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""","This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes."
653,41527687,,3,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,TRUE,0.88939,TRUE,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""","In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas?"
654,41527687,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""",":)"""
655,51242545,,0,,"[{'score': 0.667217, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.6821, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.667217,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.6821,FALSE,0,FALSE,"""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the ScanDirs.class look like:}""","""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i"
656,51242545,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the ScanDirs.class look like:}""",have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the
657,51242545,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the ScanDirs.class look like:}""",ScanDirs.class
658,51242545,,3,,"[{'score': 0.63802, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.63802,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.984352,FALSE,"""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the ScanDirs.class look like:}""","look like:}"""
659,48343919,,0,,"[{'score': 0.760212, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.760212,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""","""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json"
660,48343919,,1,,"[{'score': 0.512078, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.92651, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.822231, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.512078,FALSE,0,FALSE,0,FALSE,0,TRUE,0.92651,FALSE,0,TRUE,0.822231,FALSE,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""","looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project."
661,48343919,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""","Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!"""
662,49562890,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""","""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo."
663,49562890,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""","Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second."
664,49562890,,2,,"[{'score': 0.609136, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.609136,FALSE,0,FALSE,0,TRUE,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""","We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?"""
665,46516766,,0,,"[{'score': 0.840583, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.840583,FALSE,0,FALSE,0,TRUE,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""","""I'm testing the OCR with Google cloud vision and I find the results are particularly bad."
666,46516766,,1,,"[{'score': 0.563853, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.768897, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.563853,FALSE,0,FALSE,0,TRUE,0.768897,FALSE,0,FALSE,0,FALSE,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""","My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?"""
667,56006562,,0,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""I need to create a Driving license scanner, but I don't know how to started.The process that I need:1 - Open Camera2 - Detect Driving license 3 - Autofocus, autocrop and take the picture4 - Extract and OCR data from the driving licenseI already create an application where I crop and use google vision to OCR. My problem is detect, autofocus and autocrop only the detected document.Reference apps:Regula:BlinkID:""","""I need to create a Driving license scanner, but I don't know how to started.The process that I need:1 - Open Camera2 - Detect Driving license 3 - Autofocus, autocrop and take the picture4 - Extract and OCR data from the driving licenseI already create an application where I crop and use google vision to OCR."
668,56006562,,1,,"[{'score': 0.875421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.875421,FALSE,0,FALSE,0,TRUE,"""I need to create a Driving license scanner, but I don't know how to started.The process that I need:1 - Open Camera2 - Detect Driving license 3 - Autofocus, autocrop and take the picture4 - Extract and OCR data from the driving licenseI already create an application where I crop and use google vision to OCR. My problem is detect, autofocus and autocrop only the detected document.Reference apps:Regula:BlinkID:""","My problem is detect, autofocus and autocrop only the detected document.Reference apps:Regula:BlinkID:"""
669,53737055,,0,,"[{'score': 0.731735, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.731735,FALSE,0,FALSE,0,TRUE,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""","""I am using AWS Rekognition to detect faces in an image."
670,53737055,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""",When a face is detected it outputs bound box information so that you can use it to draw one on the image.
671,53737055,,2,,"[{'score': 0.717675, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.717675,FALSE,0,FALSE,0,TRUE,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""","However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face."
672,53737055,,3,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""",Is there an easier way to convert these variables or calculate them?
673,53737055,,4,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,TRUE,0.968123,TRUE,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""","I have looked everywhere and could really use some guidance."""
674,43793934,,0,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""","""Using Google vision fromI was successfully able to create aand anusingandrespectively."
675,43793934,,1,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.874319, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874319,FALSE,0,TRUE,0.716301,TRUE,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""","And then send my image using, attempting to read the digits within the image."
676,43793934,,2,,"[{'score': 0.67676, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.67676,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""","however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in."""
677,42871797,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have an android app which on local WiFi and doesn't require an internet connection so I am looking for a solution which can scan QR codes without requiring Google Play Services as no internet connection is there so I do not want to update Google Play Services. Currently, I am doing it by using Google Vision API but somehow(if possible) I want to remove this dependency.""","""I have an android app which on local WiFi and doesn't require an internet connection so I am looking for a solution which can scan QR codes without requiring Google Play Services as no internet connection is there so I do not want to update Google Play Services."
678,42871797,,1,,"[{'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.704683, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713028,FALSE,0,TRUE,0.704683,TRUE,"""I have an android app which on local WiFi and doesn't require an internet connection so I am looking for a solution which can scan QR codes without requiring Google Play Services as no internet connection is there so I do not want to update Google Play Services. Currently, I am doing it by using Google Vision API but somehow(if possible) I want to remove this dependency.""","Currently, I am doing it by using Google Vision API but somehow(if possible) I want to remove this dependency."""
679,40025350,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""","""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image."
680,40025350,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation.
681,40025350,,2,,"[{'score': 0.720443, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.720443,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,FALSE,0,FALSE,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""","However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase."
682,40025350,,3,,"[{'score': 0.564841, 'tone_id': 'fear', 'tone_name': 'Fear'}, {'score': 0.718921, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,TRUE,0.564841,FALSE,0,TRUE,0.718921,FALSE,0,FALSE,0,FALSE,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""","Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):"""
683,44240464,,0,,"[{'score': 0.611312, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.611312,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""","""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6"
684,44240464,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""",to 4.1.32)
685,44240464,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""",Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3)
686,44240464,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""",Adding the pom.xml4)
687,44240464,,4,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""","Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account"""
688,56178717,,0,,"[{'score': 0.585282, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.81197, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.585282,FALSE,0,FALSE,0,FALSE,0,TRUE,0.81197,FALSE,0,FALSE,0,FALSE,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""","""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time."
689,56178717,,1,,"[{'score': 0.946087, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946087,FALSE,0,FALSE,0,TRUE,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""",Is there a way to upload and then analyze multiple files in parallel?
690,56178717,,2,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""","And how can I estimate how much time it is going to take?"""
691,37900554,,0,,"[{'score': 0.77105, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.77105,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""","""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image""."
692,37900554,,1,,"[{'score': 0.811578, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.811578,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""","The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data."
693,37900554,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""","Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console."
694,37900554,,3,,"[{'score': 0.735673, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.735673,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""",Is it because base64 encoded binary isn't an acceptable format?
695,37900554,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""","Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json."""
696,49732664,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""","""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs""."
697,49732664,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.515576, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.515576,FALSE,0,TRUE,0.5538,TRUE,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",We need to do some image processing operation on the uploaded image later using google cloud vision api.
698,49732664,,2,,"[{'score': 0.679399, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.679399,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,TRUE,0.681699,FALSE,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",The code is :However it shows the following error :Where is the probable error?
699,49732664,,3,,"[{'score': 0.905748, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.905748,FALSE,0,FALSE,0,TRUE,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",I looked up to this :However the problem is not similar.
700,49732664,,4,,"[{'score': 0.825947, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.825947,FALSE,0,FALSE,0,TRUE,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""","I'm facing issue with uploading images in the server whether that one have file path issue."""
701,54757580,,0,,"[{'score': 0.687975, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.902911, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.687975,FALSE,0,FALSE,0,TRUE,0.902911,FALSE,0,FALSE,0,FALSE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","""I'm receiving this error when trying to train my first custom model:I haven't done much at this point."
702,54757580,,1,,"[{'score': 0.913819, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.913819,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained."
703,54757580,,2,,"[{'score': 0.677247, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677247,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""",I'm not sure what credentials or quota limit would have to do with this.
704,54757580,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large""."
705,54757580,,4,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.75152,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","So I declassified 70 of them, and now I'm just trying to train this model with 3 categories."
706,54757580,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","Now I'm getting the ""Unauthorized"" message I originally mentioned."
707,54757580,,6,,"[{'score': 0.909883, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.909883,TRUE,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""","I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part."""
708,35672845,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""","""I'm trying to use the Google Vision API."
709,35672845,,1,,"[{'score': 0.638807, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638807,FALSE,0,FALSE,0,TRUE,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""",I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas?
710,35672845,,2,,"[{'score': 0.634068, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.634068,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.968123,FALSE,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""","Somebody have the same problem?"""
711,36774015,,0,,"[{'score': 0.519909, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.519909,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs."
712,36774015,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","Same image, same code, same libraries, but the extracted text is different.2)"
713,36774015,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",I get two differently detected locales for the two differently detected texts.
714,36774015,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",My original text is Italian text mixed with digits.
715,36774015,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","On the development machine, the detected locale is ""zh"" (Chinese)."
716,36774015,,5,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","On the ""production"" machine, the detected locale is ""fil""."
717,36774015,,6,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.716301,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","There isn't any ""fil"" code inso I don't know what it is (Filipino?)."
718,36774015,,7,,"[{'score': 0.926173, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.926173,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","In any case, I get a better result on the development machine when the detected locale is ""zh""."
719,36774015,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","So... same image, same code, but differently detected locale and differently detected text.3)"
720,36774015,,9,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.793559, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.793559,FALSE,0,TRUE,0.615352,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision."
721,36774015,,10,,"[{'score': 0.70275, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.70275,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!)."
722,36774015,,11,,"[{'score': 0.765599, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.765599,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?)."
723,36774015,,12,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","So... same image, same code, but differently detected locale and differently detected text."
724,36774015,,13,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint?"
725,36774015,,14,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""","Thanks."""
726,54881611,,0,,"[{'score': 0.691954, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.691954,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,FALSE,0,FALSE,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""","""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio?"
727,54881611,,1,,"[{'score': 0.526726, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.525193, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.526726,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525193,FALSE,0,TRUE,0.58393,FALSE,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""","I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note:"
728,54881611,,2,,"[{'score': 0.569129, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.569129,FALSE,0,FALSE,0,TRUE,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""",I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters.
729,54881611,,3,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""","I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters."""
730,47155510,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""","""I am trying to create a custom Watson Visual Recognition in java."
731,47155510,,1,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""",I have already a classifier created using Curl.
732,47155510,,2,,"[{'score': 0.803567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.803567,FALSE,0,FALSE,0,TRUE,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""",Currently I am using the default Watson Classifier.
733,47155510,,3,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""","Are there any examples where Watson API is used for custom creation and training of classifiers in Java?"""
734,52539984,,0,,"[{'score': 0.917265, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.917265,FALSE,0,FALSE,0,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","""I am a newbie to Android Development, so please excuse if this question has already been answered."
735,52539984,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server."
736,52539984,,2,,"[{'score': 0.806855, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.729586, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.806855,FALSE,0,FALSE,0,TRUE,0.729586,FALSE,0,FALSE,0,FALSE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error."
737,52539984,,3,,"[{'score': 0.901841, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901841,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""",With some phones i get a size of 180KB and on other phones i get something like 2.9MB.
738,52539984,,4,,"[{'score': 0.932169, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.932169,TRUE,0.638987,FALSE,0,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone?"
739,52539984,,5,,"[{'score': 0.668095, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.668095,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that."
740,52539984,,6,,"[{'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.506763,FALSE,0,TRUE,0.681699,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB)."
741,52539984,,7,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""","Can someone please help me?"""
742,39797164,,0,,"[{'score': 0.780202, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.780202,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""","""In this,, says this:Which was great."
743,39797164,,1,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.5538,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",But it seems that the Google Vision API has moved on and the open Sourced version has not.
744,39797164,,2,,"[{'score': 0.593611, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.593611,FALSE,0,FALSE,0,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""","The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere?"
745,39797164,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT]
746,39797164,,4,,"[{'score': 0.586028, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.586028,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""","Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes."
747,39797164,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""","First off, I add PDF417 to the scanable barcodes."
748,39797164,,6,,"[{'score': 0.664451, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.664451,FALSE,0,FALSE,0,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Then I set the processor to an autofocus-er.
749,39797164,,7,,"[{'score': 0.569636, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.602632, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.569636,FALSE,0,TRUE,0.602632,TRUE,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""","I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:"""
750,45624819,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""","""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit."
751,45624819,,1,,"[{'score': 0.890188, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.890188,FALSE,0,FALSE,0,TRUE,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""","All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise."
752,45624819,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""","Thanks."""
753,54451771,,0,,"[{'score': 0.787412, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.787412,FALSE,0,FALSE,0,TRUE,"""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.can someone please explain what im doing wrong and how to prevent this error  ?""","""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist."
754,54451771,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,TRUE,0.968123,TRUE,"""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.can someone please explain what im doing wrong and how to prevent this error  ?""",for some reason it only works for some websites.can
755,54451771,,2,,"[{'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,TRUE,0.716301,TRUE,"""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.can someone please explain what im doing wrong and how to prevent this error  ?""","someone please explain what im doing wrong and how to prevent this error  ?"""
756,40079213,,0,,"[{'score': 0.561818, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.561818,FALSE,0,FALSE,0,TRUE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""","""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality."
757,40079213,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",below is my code which is base onThe code works but i got some issues with it.
758,40079213,,2,,"[{'score': 0.641954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.641954,FALSE,0,FALSE,0,TRUE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",There are some colors that are obviously on the image but weren't included on the API response.
759,40079213,,3,,"[{'score': 0.586792, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.867933, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.867933,FALSE,0,TRUE,0.586792,TRUE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""","And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response."
760,40079213,,4,,"[{'score': 0.509967, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.864115, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.509967,FALSE,0,FALSE,0,TRUE,0.864115,FALSE,0,FALSE,0,FALSE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image.
761,40079213,,5,,"[{'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.946222,TRUE,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""","Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it."""
762,56176858,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""","""I need to send a PDF file to Google Vision to extract and return text."
763,56176858,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""","From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works."
764,56176858,,2,,"[{'score': 0.709907, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.709907,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""","After I redirect to another page that is suppose to get that file to Vision, and that's where it fails."
765,56176858,,3,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,FALSE,0,FALSE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",I found an.
766,56176858,,4,,"[{'score': 0.523195, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.523195,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",Here's the code:When I run the second script I get the following errors:How do I authenticate for this service?
767,56176858,,5,,"[{'score': 0.916667, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}]",FALSE,0,TRUE,0.916667,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""","What am I missing?"""
768,51361560,,0,,"[{'score': 0.568262, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.568262,FALSE,0,FALSE,0,TRUE,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""","""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console."
769,51361560,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""",().It is working fine and gives the detected celebrity on the console.
770,51361560,,2,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""","But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult""."
771,51361560,,3,,"[{'score': 0.5538, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.677676, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.677676,FALSE,0,TRUE,0.5538,TRUE,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""",Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file.
772,51361560,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""","Where will I get that whole JSON??"""
773,47543228,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""","""So I used the Google Vision API to detect the text from an image."
774,47543228,,1,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",The image has a question and 3 multiple choice answers.
775,47543228,,2,,"[{'score': 0.918912, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.918912,FALSE,0,FALSE,0,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""","The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately."
776,47543228,,3,,"[{'score': 0.80705, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.80705,FALSE,0,FALSE,0,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""","This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them?"
777,47543228,,4,,"[{'score': 0.649406, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,TRUE,0.649406,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""","These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated."
778,47543228,,5,,"[{'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,TRUE,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""","Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be"""
779,54039956,,0,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,TRUE,"""I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB.""","""I have a app client where the user upload a photo via native cam or canvas (PNG)."
780,54039956,,1,,"[{'score': 0.716147, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716147,FALSE,0,TRUE,"""I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB.""","In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB."
781,54039956,,2,,"[{'score': 0.505039, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.750465, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.505039,FALSE,0,FALSE,0,TRUE,0.750465,FALSE,0,FALSE,0,FALSE,"""I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB.""","This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB."""
782,35757052,,0,,"[{'score': 0.563256, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.563256,FALSE,0,FALSE,0,TRUE,"""I'm using Google Vision API's sample to make barcode reader in a webview of existing project.this is my error message:This is build.gradle file:when I created BarcodeCaptureActivity.java,I get tons of errors...BarcodeCaptureActivity.java:why can't android studio recognize import in java?I'm stuck here for 2 days...please help me!!!!""","""I'm using Google Vision API's sample to make barcode reader in a webview of existing project.this is my error message:This is build.gradle"
783,35757052,,1,,"[{'score': 0.595206, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.595206,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm using Google Vision API's sample to make barcode reader in a webview of existing project.this is my error message:This is build.gradle file:when I created BarcodeCaptureActivity.java,I get tons of errors...BarcodeCaptureActivity.java:why can't android studio recognize import in java?I'm stuck here for 2 days...please help me!!!!""","file:when I created BarcodeCaptureActivity.java,I get tons of errors...BarcodeCaptureActivity.java:why can't android studio recognize import in java?I'm stuck here for 2 days...please help me!!!!"""
784,35755940,,0,,"[{'score': 0.667092, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.667092,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""","""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build()"
785,35755940,,1,,"[{'score': 0.69934, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.69934,FALSE,0,FALSE,0,TRUE,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""","takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build()"
786,35755940,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""","function in the code.Here is the google code (slightly modified):"""
787,54802917,,0,,"[{'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873263,TRUE,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""","""I thought that I could by at of today from the docs it looks like I can't ()."
788,54802917,,1,,"[{'score': 0.907142, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.907142,FALSE,0,TRUE,0.88939,TRUE,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""","Seems like for video stream only face detection is supported, not analysis."
789,54802917,,2,,"[{'score': 0.948998, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.948998,FALSE,0,FALSE,0,TRUE,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",Analysis says it only works for stored media ().
790,54802917,,3,,"[{'score': 0.873274, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.873274,FALSE,0,FALSE,0,TRUE,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""","Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces?"
791,54802917,,4,,"[{'score': 0.670177, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.955445, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.670177,FALSE,0,FALSE,0,FALSE,0,TRUE,0.955445,FALSE,0,TRUE,0.75152,FALSE,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""","we kinda really need the sentiment analysis for video stream...many thanks!!"""
792,50423259,,0,,"[{'score': 0.573591, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.573591,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,FALSE,0,FALSE,"""I'm facing this issue today. Unable to access Microsoft FACE Api endpoint:Anyone facing the same issue today?Error:""","""I'm facing this issue today."
793,50423259,,1,,"[{'score': 0.839615, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.839615,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,TRUE,0.681699,FALSE,"""I'm facing this issue today. Unable to access Microsoft FACE Api endpoint:Anyone facing the same issue today?Error:""","Unable to access Microsoft FACE Api endpoint:Anyone facing the same issue today?Error:"""
794,49182513,,0,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings. But when I attempt to use the client, it doesn't work.The.The code I use:The exception I'm being thrown:I'm guessing there has to be something going on that i'm not aware of""","""I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings."
795,49182513,,1,,"[{'score': 0.689758, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.644304, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.689758,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.644304,FALSE,"""I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings. But when I attempt to use the client, it doesn't work.The.The code I use:The exception I'm being thrown:I'm guessing there has to be something going on that i'm not aware of""","But when I attempt to use the client, it doesn't work.The.The code I use:The exception I'm being thrown:I'm guessing there has to be something going on that i'm not aware of"""
796,51642038,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""","""I am trying to build a C# library that will act as a wrapper for a set of Google APIs."
797,51642038,,1,,"[{'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.858733, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858733,TRUE,0.638987,FALSE,0,TRUE,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""","When working with Google Vision API, I have found the API returns an empty response set for certain queries."
798,51642038,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""","For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated."""
799,49510330,,0,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.Detailed error:""","""I am using Google Cloud Vision API on my Raspberry PI."
800,49510330,,1,,"[{'score': 0.534889, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.534889,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.Detailed error:""",It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error.
801,49510330,,2,,"[{'score': 0.709766, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.711887, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.709766,FALSE,0,FALSE,0,TRUE,0.711887,FALSE,0,FALSE,0,FALSE,"""I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.Detailed error:""","I have synchronized the time using NTP but is of no help.Detailed error:"""
802,55982785,,0,,"[{'score': 0.572397, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.572397,FALSE,0,FALSE,0,TRUE,"""I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?""","""I am trying to find out what training data set is used for training the Google Cloud Vision API."
803,55982785,,1,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.781949, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.781949,FALSE,0,TRUE,0.647986,TRUE,"""I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?""","Do any of you know where the data is from and if it is accessible?"""
804,50174426,,0,,"[{'score': 0.543112, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.543112,FALSE,0,TRUE,"""How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc . ) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt . Thanks""","""How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc ."
805,50174426,,1,,"[{'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.743104, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.743104,FALSE,0,TRUE,0.58393,TRUE,"""How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc . ) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt . Thanks""",) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt .
806,50174426,,2,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc . ) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt . Thanks""","Thanks"""
807,51319671,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This is my gem file. But when i use bundle install i see this output.""","""This is my gem file."
808,51319671,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""This is my gem file. But when i use bundle install i see this output.""","But when i use bundle install i see this output."""
809,49466041,,0,,"[{'score': 0.629535, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.669292, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.629535,FALSE,0,FALSE,0,TRUE,0.669292,FALSE,0,FALSE,0,FALSE,"""I am getting the following error when trying to access my s3 bucket with aws rekognition:My hunch is it has something to do with the region.Here is the code:And here is my config file:I have given almost all the permissions to the user I can think of. Also the region for the s3 bucket appears to be in a place that can work with rekognition. What can I do?""","""I am getting the following error when trying to access my s3 bucket with aws rekognition:My hunch is it has something to do with the region.Here is the code:And here is my config file:I have given almost all the permissions to the user I can think of."
810,49466041,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting the following error when trying to access my s3 bucket with aws rekognition:My hunch is it has something to do with the region.Here is the code:And here is my config file:I have given almost all the permissions to the user I can think of. Also the region for the s3 bucket appears to be in a place that can work with rekognition. What can I do?""",Also the region for the s3 bucket appears to be in a place that can work with rekognition.
811,49466041,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am getting the following error when trying to access my s3 bucket with aws rekognition:My hunch is it has something to do with the region.Here is the code:And here is my config file:I have given almost all the permissions to the user I can think of. Also the region for the s3 bucket appears to be in a place that can work with rekognition. What can I do?""","What can I do?"""
812,44572133,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition. I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.While trying to add an inline policy to the role I use and get stuck at the ARN field. I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail.I always get this error:What is the correct ARN that I have to enter?""","""I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition."
813,44572133,,1,,"[{'score': 0.509368, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.509368,FALSE,0,TRUE,"""I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition. I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.While trying to add an inline policy to the role I use and get stuck at the ARN field. I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail.I always get this error:What is the correct ARN that I have to enter?""",I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.While trying to add an inline policy to the role I use and get stuck at the ARN field.
814,44572133,,2,,"[{'score': 0.73811, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.678932, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.73811,FALSE,0,FALSE,0,TRUE,0.532616,TRUE,0.678932,FALSE,0,FALSE,"""I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition. I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.While trying to add an inline policy to the role I use and get stuck at the ARN field. I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail.I always get this error:What is the correct ARN that I have to enter?""","I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail.I always get this error:What is the correct ARN that I have to enter?"""
815,35952647,,0,,"[{'score': 0.858259, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.858259,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""","""I'm trying to set up a visual recognition app using the Watson visual recognition api."
816,35952647,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""","To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html"
817,35952647,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",and api_request.js.This is my api_request.js
818,35952647,,3,,"[{'score': 0.579436, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579436,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",file:It is taken directly from the visual recognition api documentation.
819,35952647,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers.
820,35952647,,5,,"[{'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",However as it has node.js
821,35952647,,6,,"[{'score': 0.751512, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.895415, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.895415,TRUE,0.751512,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",functions I decided to use browserify to allow it to run in the browser.
822,35952647,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",I installed browserify and built bundle.js
823,35952647,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",out of api_request.js in the same directory as the api_request.js
824,35952647,,9,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",and index.html
825,35952647,,10,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",file.Once index.html
826,35952647,,11,,"[{'score': 0.8152, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.8152,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",was linked to bundle.js
827,35952647,,12,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.647986,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",I opened it in the browser and it didn't have any issues with node.js
828,35952647,,13,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud.
829,35952647,,14,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",To be specific index.js
830,35952647,,15,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",couldn't find v2-beta (I didn't edit the watson-developer-cloud files).
831,35952647,,16,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""","What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js"
832,35952647,,17,,"[{'score': 0.653849, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.715667, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.653849,FALSE,0,FALSE,0,TRUE,0.715667,FALSE,0,FALSE,0,FALSE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",logged the error that index.js
833,35952647,,18,,"[{'score': 0.547677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.547677,FALSE,0,FALSE,0,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",couldn't find v2-beta.^that is the script I used to build bundle.js.
834,35952647,,19,,"[{'score': 0.735778, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.908301, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.735778,FALSE,0,FALSE,0,TRUE,0.908301,FALSE,0,TRUE,0.716301,FALSE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",The only thing I can think could be causing this error is browserify.
835,35952647,,20,,"[{'score': 0.976993, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.976993,TRUE,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""","Is there something else that could be causing this?"""
836,51429447,,0,,"[{'score': 0.527318, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.527318,FALSE,0,FALSE,0,TRUE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""","""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region."
837,51429447,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""","Unfortunately, AWS Rekognition is not supported in that region."
838,51429447,,2,,"[{'score': 0.538871, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.58393, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.538871,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.58393,FALSE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""","I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata."
839,51429447,,3,,"[{'score': 0.950161, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.950161,FALSE,0,FALSE,0,TRUE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""","As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region."
840,51429447,,4,,"[{'score': 0.684403, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.984352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.684403,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.984352,FALSE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Any help or advice would be appreciated.EDIT:
841,51429447,,5,,"[{'score': 0.845297, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.845297,TRUE,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""","I thought it may be relevant to mention, I am running this on Python."""
842,49522705,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use google vision API to detect text from camera preview. However, I want the detected text to be within a specific region/rectangle in the camera review.""","""I am trying to use google vision API to detect text from camera preview."
843,49522705,,1,,"[{'score': 0.571567, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.571567,FALSE,0,FALSE,0,TRUE,"""I am trying to use google vision API to detect text from camera preview. However, I want the detected text to be within a specific region/rectangle in the camera review.""","However, I want the detected text to be within a specific region/rectangle in the camera review."""
844,38634409,,0,,"[{'score': 0.938962, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.938962,FALSE,0,FALSE,0,TRUE,"""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info.""","""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content."
845,38634409,,1,,"[{'score': 0.941612, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.941612,FALSE,0,FALSE,0,TRUE,"""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info.""","The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object."
846,38634409,,2,,"[{'score': 0.576305, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.576305,FALSE,0,FALSE,0,TRUE,"""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info.""","Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info."""
847,49589030,,0,,"[{'score': 0.514933, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.514933,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.""","""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails."
848,49589030,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.""","Please advise as I am new to this, thank you."""
849,46287956,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have created a collection in was CLI like so:I would like to rename that collection to another string. I can't find how to do that. Any suggestion?""","""I have created a collection in was CLI like so:I would like to rename that collection to another string."
850,46287956,,1,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I have created a collection in was CLI like so:I would like to rename that collection to another string. I can't find how to do that. Any suggestion?""",I can't find how to do that.
851,46287956,,2,,"[{'score': 0.999857, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.999857,TRUE,"""I have created a collection in was CLI like so:I would like to rename that collection to another string. I can't find how to do that. Any suggestion?""","Any suggestion?"""
852,52163842,,0,,"[{'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,FALSE,0,TRUE,"""I m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get' What I m doing wrong? Thank you so much.All other services e.g. Polly and Rekognition are working well.""","""I m trying to use the Amazon Comprehend API via aws JavaScript SDK."
853,52163842,,1,,"[{'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874372,FALSE,0,TRUE,"""I m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get' What I m doing wrong? Thank you so much.All other services e.g. Polly and Rekognition are working well.""",But I always get' What I m doing wrong?
854,52163842,,2,,"[{'score': 0.781574, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.874372, 'tone_id': 'confident', 'tone_name': 'Confident'}]",TRUE,0.781574,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.874372,FALSE,0,FALSE,"""I m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get' What I m doing wrong? Thank you so much.All other services e.g. Polly and Rekognition are working well.""",Thank you so much.All other services e.g.
855,52163842,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get' What I m doing wrong? Thank you so much.All other services e.g. Polly and Rekognition are working well.""","Polly and Rekognition are working well."""
856,54444114,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""","""i'm trying to make a webscrapper with aws image recogntion api."
857,54444114,,1,,"[{'score': 0.567034, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.567034,FALSE,0,TRUE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""",So I have to convert the image to a byte array in order for the api to work.
858,54444114,,2,,"[{'score': 0.736062, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.946222, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.736062,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.946222,FALSE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""","However, I'm getting some error saying."
859,54444114,,3,,"[{'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,TRUE,0.80026,FALSE,0,TRUE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""","If i use a local image file, then it works perfectly fine."
860,54444114,,4,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""",Can someone please help me ?
861,54444114,,5,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""","Thanks"""
862,50159443,,0,,"[{'score': 0.623738, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.623738,FALSE,0,FALSE,0,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""","""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work."
863,50159443,,1,,"[{'score': 0.968123, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.968123,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Can someone please help me?
864,50159443,,2,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.923658, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,TRUE,0.923658,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google.
865,50159443,,3,,"[{'score': 0.523822, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.523822,FALSE,0,FALSE,0,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""","Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml"
866,50159443,,4,,"[{'score': 0.936689, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.936689,FALSE,0,FALSE,0,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",(I'm using the Figaro gem) and then referring to it in a Google.yml
867,50159443,,5,,"[{'score': 0.732346, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.716301, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.732346,FALSE,0,TRUE,0.716301,TRUE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""","file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials?"
868,50159443,,6,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""","Thank you!"""
869,54725524,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""","""i want to crop face detected from image that i capture from my phone."
870,54725524,,1,,"[{'score': 0.85365, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.85365,FALSE,0,FALSE,0,TRUE,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""",i'm using google vision API to detect face.
871,54725524,,2,,"[{'score': 0.762356, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.762356,FALSE,0,TRUE,0.75152,TRUE,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""",i saw some questions similar to mine but they're using openCV.i
872,54725524,,3,,"[{'score': 0.788547, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.788547,FALSE,0,FALSE,0,TRUE,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""",tried adding Bitmap.createBitmap();but it can only accept int values.
873,54725524,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""","but my values has decimal so it's a float.this is my code for face detection :"""
874,50258562,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.""","""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder."
875,50258562,,1,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.659886, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.659886,FALSE,0,TRUE,0.856622,TRUE,"""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.""","However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder."""
876,50630045,,0,,"[{'score': 0.769135, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.769135,FALSE,0,FALSE,0,TRUE,"""I need to recognize image with Google Vision API. Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes. I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction""","""I need to recognize image with Google Vision API."
877,50630045,,1,,"[{'score': 0.716804, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.716804,FALSE,0,FALSE,0,TRUE,"""I need to recognize image with Google Vision API. Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes. I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction""","Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes."
878,50630045,,2,,"[{'score': 0.682736, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.682736,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I need to recognize image with Google Vision API. Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes. I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction""","I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction"""
879,41824724,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to use Amazon Rekognition Service with Node.js,I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.jsThe code is as below/* This operation detects faces in an image stored in an AWS S3 bucket.     */I coudn't get true data, this is the response I get:[Object] is written. Normally it should give a response like below.How can I get the face data?""","""I am trying to use Amazon Rekognition Service with Node.js,I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.jsThe code is as below/* This operation detects faces in an image stored in an AWS S3 bucket."
880,41824724,,1,,"[{'score': 0.61476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.61476,FALSE,0,FALSE,0,TRUE,"""I am trying to use Amazon Rekognition Service with Node.js,I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.jsThe code is as below/* This operation detects faces in an image stored in an AWS S3 bucket.     */I coudn't get true data, this is the response I get:[Object] is written. Normally it should give a response like below.How can I get the face data?""","*/I coudn't get true data, this is the response I get:[Object] is written."
881,41824724,,2,,"[{'score': 0.532616, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.532616,FALSE,0,FALSE,0,TRUE,"""I am trying to use Amazon Rekognition Service with Node.js,I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.jsThe code is as below/* This operation detects faces in an image stored in an AWS S3 bucket.     */I coudn't get true data, this is the response I get:[Object] is written. Normally it should give a response like below.How can I get the face data?""","Normally it should give a response like below.How can I get the face data?"""
882,49819964,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""","""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console."
883,49819964,,1,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""","I would like to disable them, but I don't know how."
884,49819964,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""","I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it."
885,49819964,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""","Thanks in advance."""
886,54521080,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""","""I am aware that it is better to use aws Rekognition for this."
887,54521080,,1,,"[{'score': 0.833824, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.833824,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""","However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them)."
888,54521080,,2,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",The text comes out misspelled and fragmented.I am new to ML and sagemaker.
889,54521080,,3,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615352,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""","From what I have seen, the use cases seem to be for prediction and image classification."
890,54521080,,4,,"[{'score': 0.647986, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.587989, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.587989,FALSE,0,TRUE,0.647986,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",I could not find one on training a model for detecting text in an image.
891,54521080,,5,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.88939,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Is it possible to to do it with Sagemaker?
892,54521080,,6,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.859009, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.859009,FALSE,0,TRUE,0.75152,TRUE,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""","I would appreciate it if someone pointed me in the right direction."""
893,38417738,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""","""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera)."
894,38417738,,1,,"[{'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,FALSE,0,TRUE,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""","However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels."
895,38417738,,2,,"[{'score': 0.566277, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.566277,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""",Both the sets of images are present on my local disk.
896,38417738,,3,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.525007,TRUE,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""","Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?"""
897,55555575,,0,,"[{'score': 0.581041, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.581041,TRUE,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""","""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work."
898,55555575,,1,,"[{'score': 0.604739, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.518058, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.518058,FALSE,0,TRUE,0.604739,TRUE,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""","I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to"""
899,44613751,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I'm using the following, to start working on Google Cloud Vision platform. I tried the following steps:1) Created the project2) Created the bucket3) I also ran the following code:4) Ran each code given in the tutorial line-by-lineWhen I'm running this part of the code, it's throwing the following error.Error:I'm configuring the pipeline as following:""","""I'm using the following, to start working on Google Cloud Vision platform."
900,44613751,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm using the following, to start working on Google Cloud Vision platform. I tried the following steps:1) Created the project2) Created the bucket3) I also ran the following code:4) Ran each code given in the tutorial line-by-lineWhen I'm running this part of the code, it's throwing the following error.Error:I'm configuring the pipeline as following:""","I tried the following steps:1) Created the project2) Created the bucket3) I also ran the following code:4) Ran each code given in the tutorial line-by-lineWhen I'm running this part of the code, it's throwing the following error.Error:I'm configuring the pipeline as following:"""
901,51189021,,0,,"[{'score': 0.667824, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.667824,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am running a simple code that utilizes google cloud vision api but it keeps on throwing this error. I have tries upgrading my packages shown here:but the error persists.PS:am using a virtual environment(virtualenv)""","""I am running a simple code that utilizes google cloud vision api but it keeps on throwing this error."
902,51189021,,1,,"[{'score': 0.599421, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.599421,FALSE,0,FALSE,0,TRUE,"""I am running a simple code that utilizes google cloud vision api but it keeps on throwing this error. I have tries upgrading my packages shown here:but the error persists.PS:am using a virtual environment(virtualenv)""","I have tries upgrading my packages shown here:but the error persists.PS:am using a virtual environment(virtualenv)"""
903,41299413,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong. But I have no idea what should I do about this. Would you give me an advice?""","""I am trying to call Google Cloud Vision from a PHP Script."
904,41299413,,1,,"[{'score': 0.539049, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.539049,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong. But I have no idea what should I do about this. Would you give me an advice?""","And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong."
905,41299413,,2,,"[{'score': 0.653099, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.653099,FALSE,0,FALSE,0,TRUE,"""I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong. But I have no idea what should I do about this. Would you give me an advice?""",But I have no idea what should I do about this.
906,41299413,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong. But I have no idea what should I do about this. Would you give me an advice?""","Would you give me an advice?"""
907,51899558,,0,,"[{'score': 0.672469, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.672469,FALSE,0,FALSE,0,TRUE,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""","""I want to implement a text-to-speech function for my application using Python."
908,51899558,,1,,"[{'score': 0.581467, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.500206, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.581467,FALSE,0,FALSE,0,FALSE,0,TRUE,0.500206,FALSE,0,FALSE,0,FALSE,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""","However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi"""
909,49416747,,0,,"[{'score': 0.521342, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.521342,FALSE,0,FALSE,0,TRUE,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""","""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container."
910,49416747,,1,,"[{'score': 0.683828, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.750047, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.683828,FALSE,0,FALSE,0,TRUE,0.750047,FALSE,0,FALSE,0,FALSE,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""",But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?
911,49416747,,2,,"[{'score': 0.99249, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.73677, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.73677,FALSE,0,TRUE,0.99249,TRUE,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""","Do I need to specify some sort of headers or something?Documentation link"""
912,47000735,,0,,"[{'score': 0.60456, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.60456,FALSE,0,FALSE,0,TRUE,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""","""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in."
913,47000735,,1,,"[{'score': 0.616902, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.616902,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""","But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3)"
914,47000735,,2,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""",requirements.txt4)
915,47000735,,3,,"[{'score': 0.528596, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.528596,FALSE,0,FALSE,0,TRUE,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""","lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?"""
916,50874265,,0,,"[{'score': 0.564476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.564476,FALSE,0,FALSE,0,TRUE,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""","""I am trying to convert the response from Google Cloud Vision API Client Library to a json format."
917,50874265,,1,,"[{'score': 0.630217, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.868982, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.630217,FALSE,0,FALSE,0,TRUE,0.868982,FALSE,0,FALSE,0,FALSE,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",However i get the following error:ResourceGoogleVision.pythe
918,50874265,,2,,"[{'score': 0.525007, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.705784, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.705784,FALSE,0,TRUE,0.525007,TRUE,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",labels variable is of typeAs you can see i am using message to json function on the labels response.
919,50874265,,3,,"[{'score': 0.703709, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.668281, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.703709,FALSE,0,FALSE,0,TRUE,0.668281,FALSE,0,FALSE,0,FALSE,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""","But i am getting the above error.Is there a way to convert the result to a json format?"""
920,44467350,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""","""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images."
921,44467350,,1,,"[{'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.681699, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.681699,TRUE,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""","The results are poor, so I think I may need to add Language Hints."
922,44467350,,2,,"[{'score': 0.638987, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.638987,FALSE,0,TRUE,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""",Here is the basic json call:How can i add it to the json code in a valid way.
923,44467350,,3,,"[{'score': 0.759619, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.759619,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""","I keep getting syntax errors!!"""
924,55846066,,0,,"[{'score': 0.579367, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.579367,FALSE,0,TRUE,0.615352,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera."
925,55846066,,1,,"[{'score': 0.810154, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.810154,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS.
926,55846066,,2,,"[{'score': 0.810167, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.538888, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.810167,FALSE,0,FALSE,0,TRUE,0.538888,FALSE,0,FALSE,0,FALSE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log"
927,55846066,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's
928,55846066,,4,,"[{'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","on by default anyway, this just ensures it.setTry"
929,55846066,,5,,"[{'score': 0.774376, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.774376,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background.
930,55846066,,6,,"[{'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus."
931,55846066,,7,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera:"
932,55846066,,8,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",^2.1.1 / 2.6.0react-native:
933,55846066,,9,,"[{'score': 0.931034, 'tone_id': 'anger', 'tone_name': 'Anger'}, {'score': 0.931034, 'tone_id': 'fear', 'tone_name': 'Fear'}]",FALSE,0,FALSE,0,TRUE,0.931034,TRUE,0.931034,FALSE,0,FALSE,0,FALSE,0,FALSE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",0.57.7react:
934,55846066,,10,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",16.6.1CodeI
935,55846066,,11,,"[{'score': 0.901841, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.901841,TRUE,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""","render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria."""
936,46287595,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""","""I am testing Image Recognition from was."
937,46287595,,1,,"[{'score': 0.716441, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.994057, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",TRUE,0.716441,FALSE,0,FALSE,0,FALSE,0,TRUE,0.994057,FALSE,0,FALSE,0,FALSE,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""",So far good.
938,46287595,,2,,"[{'score': 0.554456, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.620279, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.554456,FALSE,0,FALSE,0,TRUE,0.620279,FALSE,0,FALSE,0,FALSE,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""",What I am having problems with is indexing faces in the CLI.
939,46287595,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""","I can index one at the time, but, I would like to tell AWS to index all faces in a bucket."
940,46287595,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""","To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck."""
941,51731727,,0,,"[{'score': 0.615352, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.709009, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.709009,FALSE,0,TRUE,0.615352,TRUE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding."
942,51731727,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs."
943,51731727,,2,,"[{'score': 0.658346, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.57374, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.589295, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.658346,FALSE,0,FALSE,0,TRUE,0.589295,FALSE,0,TRUE,0.57374,FALSE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit"
944,51731727,,3,,"[{'score': 0.694514, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.694514,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them."
945,51731727,,4,,"[{'score': 0.806295, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.53881, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.806295,FALSE,0,FALSE,0,TRUE,0.53881,FALSE,0,FALSE,0,FALSE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane."
946,51731727,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""","Looking into it more."""
947,53564632,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.""","""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision?"
948,53564632,,1,,"[{'score': 0.594263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.882284, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.882284,FALSE,0,TRUE,0.594263,TRUE,"""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.""","In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime."""
949,45190421,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API?""","""I have a need to consume Azure FACE API."
950,45190421,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API?""",But it is still not available in India.
951,45190421,,2,,"[{'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,TRUE,0.88939,TRUE,"""I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API?""",However If I host my application in some US servers like AWS.
952,45190421,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API?""","Will I be able to consume the API?"""
953,40856101,,0,,"[{'score': 0.740384, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.740384,FALSE,0,FALSE,0,TRUE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","""I want to use OCR using Text Detection in Google Cloud Vision."
954,40856101,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using"
955,40856101,,2,,"[{'score': 0.568112, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.568112,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","local path :So, I immigrate that code to my server."
956,40856101,,3,,"[{'score': 0.596012, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.596012,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file."
957,40856101,,4,,"[{'score': 0.75152, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.702145, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.702145,FALSE,0,TRUE,0.75152,TRUE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","However, I don't know how to set environment variable in dotnet.So."
958,40856101,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""",I want to need help.
959,40856101,,6,,"[{'score': 0.534405, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.534405,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""",Thank you.My programming language is .net
960,40856101,,7,,"[{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.801827,FALSE,0,FALSE,0,TRUE,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""","using visual studio 2012 + IIS 6.0"""
961,52119949,,0,,"[{'score': 0.767592, 'tone_id': 'confident', 'tone_name': 'Confident'}, {'score': 0.713028, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.713028,TRUE,0.767592,FALSE,0,TRUE,"""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?""","""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols."
962,52119949,,1,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?""",The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION.
963,52119949,,2,,"[{'score': 0.660937, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.660937,FALSE,0,FALSE,0,TRUE,"""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?""","Is there some way I can configure Google Vision to accurately recognize these symbols?"""
964,50182544,,0,,"[{'score': 0.620937, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.761388, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.620937,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.761388,FALSE,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""","""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my"
965,50182544,,1,,"[{'score': 0.790954, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.80026, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.790954,TRUE,0.80026,FALSE,0,TRUE,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""",cloud functionI am using firebase cloud function and Google Vision API.actually
966,50182544,,2,,"[{'score': 0.696325, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.608452, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.696325,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.608452,FALSE,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""",I tried to pass the parameter like thisMy client side coeand it did not work.
967,50182544,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""","I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js"
968,50182544,,4,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""","it works."""
969,53381742,,0,,"[{'score': 0.807862, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.807862,FALSE,0,FALSE,0,TRUE,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""","""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow."
970,53381742,,1,,"[{'score': 0.875001, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.875001,FALSE,0,FALSE,0,TRUE,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""","The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request."
971,53381742,,2,,"[{'score': 0.654189, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.654189,FALSE,0,FALSE,0,TRUE,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""","If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help."""
972,48548552,,0,,"[{'score': 0.615741, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.615741,FALSE,0,FALSE,0,TRUE,"""I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.""","""I am using Google cloud vision web detection API for detecting where the images have been used."
973,48548552,,1,,"[{'score': 0.703409, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.703409,FALSE,0,FALSE,0,TRUE,"""I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.""",But I always get 10 responses maximum even for Google's logo.
974,48548552,,2,,"[{'score': 0.609643, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,TRUE,0.609643,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.856622,FALSE,"""I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.""","Is it limit of the API or I am missing something because there is nothing mentioned in documentation."""
975,51702859,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm getting an errorswhen trying to run a codecamera is working, tested it in an other application.what are the reasons of those errors?""","""I'm getting an errorswhen trying to run a codecamera is working, tested it in an other application.what"
976,51702859,,1,,"[{'score': 0.844003, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.842108, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.844003,FALSE,0,FALSE,0,TRUE,0.842108,FALSE,0,FALSE,0,FALSE,"""I'm getting an errorswhen trying to run a codecamera is working, tested it in an other application.what are the reasons of those errors?""","are the reasons of those errors?"""
977,37690111,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a."
978,37690111,,1,,"[{'score': 0.512843, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.512843,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now?"
979,37690111,,2,,"[{'score': 0.690222, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670709, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.690222,FALSE,0,FALSE,0,TRUE,0.670709,FALSE,0,FALSE,0,FALSE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.)."
980,37690111,,3,,"[{'score': 0.530547, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.530547,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ?
981,37690111,,4,,"[{'score': 0.56477, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.920855, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",TRUE,0.56477,FALSE,0,FALSE,0,FALSE,0,TRUE,0.920855,FALSE,0,TRUE,0.88939,FALSE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","If it is, any special reason for that ?"
982,37690111,,5,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally."
983,37690111,,6,,"[{'score': 0.724236, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.856622, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.724236,FALSE,0,TRUE,0.856622,TRUE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",I just noticed there is a newer version available.
984,37690111,,7,,"[{'score': 0.670204, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.670204,FALSE,0,FALSE,0,TRUE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version.
985,37690111,,8,,"[{'score': 0.982476, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.982476,FALSE,0,FALSE,0,TRUE,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""","See the accepted answer"""
986,48219196,,0,,"[{'score': 0.700591, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.700591,FALSE,0,FALSE,0,TRUE,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""","""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error"
987,48219196,,1,,"[{'score': 0.578416, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]",FALSE,0,TRUE,0.578416,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",code:401 message:Request had invalid authentication credentials.
988,48219196,,2,,"[{'score': 0.534455, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.560098, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.560098,FALSE,0,TRUE,0.534455,TRUE,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""","Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.-"
989,48219196,,3,,"[{'score': 0.552075, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.552075,FALSE,0,FALSE,0,TRUE,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""","I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it."""
990,43409733,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I using Google Vision OCR to get text from image with PHP language.When i used in a long time. I get error below:Please show me reason and how to fix it.Thank all so much.""","""I using Google Vision OCR to get text from image with PHP language.When i used in a long time."
991,43409733,,1,,"[{'score': 0.724035, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.6821, 'tone_id': 'confident', 'tone_name': 'Confident'}]",FALSE,0,TRUE,0.724035,FALSE,0,FALSE,0,FALSE,0,TRUE,0.6821,FALSE,0,FALSE,"""I using Google Vision OCR to get text from image with PHP language.When i used in a long time. I get error below:Please show me reason and how to fix it.Thank all so much.""","I get error below:Please show me reason and how to fix it.Thank all so much."""
992,50571761,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I'm trying to read in an image of morse code (dots and dashes) using Google Vision OCR but it's not picking up the symbols very well. Is there a way to make Google Vision pick up the dots and dashes better or is there a different OCR product that can do a better job of recognizing symbols?Currently I am using the nodejs example as provided by Google Cloud at the moment:""","""I'm trying to read in an image of morse code (dots and dashes) using Google Vision OCR but it's not picking up the symbols very well."
993,50571761,,1,,"[{'score': 0.869298, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.869298,FALSE,0,FALSE,0,TRUE,"""I'm trying to read in an image of morse code (dots and dashes) using Google Vision OCR but it's not picking up the symbols very well. Is there a way to make Google Vision pick up the dots and dashes better or is there a different OCR product that can do a better job of recognizing symbols?Currently I am using the nodejs example as provided by Google Cloud at the moment:""","Is there a way to make Google Vision pick up the dots and dashes better or is there a different OCR product that can do a better job of recognizing symbols?Currently I am using the nodejs example as provided by Google Cloud at the moment:"""
994,39212656,,0,,"[{'score': 0.718038, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.718038,FALSE,0,FALSE,0,TRUE,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""","""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please"
995,39212656,,1,,"[{'score': 0.880435, 'tone_id': 'joy', 'tone_name': 'Joy'}]",TRUE,0.880435,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""",help!
996,39212656,,2,,"[{'score': 0.657835, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.873263, 'tone_id': 'tentative', 'tone_name': 'Tentative'}, {'score': 0.765977, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.657835,FALSE,0,FALSE,0,TRUE,0.765977,FALSE,0,TRUE,0.873263,FALSE,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""",i can not even fix these syntax errors to see if this code even complies properly.thank
997,39212656,,3,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""","you in advance"""
998,50018491,,0,,[],FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,"""I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve. I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks.""","""I want to use Google cloud Vision for detecting image properties."
999,50018491,,1,,"[{'score': 0.54839, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,FALSE,0,FALSE,0,FALSE,0,TRUE,0.54839,FALSE,0,FALSE,0,TRUE,"""I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve. I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks.""",I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve.
1000,50018491,,2,,"[{'score': 0.572485, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]",FALSE,0,TRUE,0.572485,FALSE,0,FALSE,0,TRUE,0.687768,FALSE,0,FALSE,0,FALSE,"""I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve. I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks.""","I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks."""
