,QuestionID,Question Text,TrustCategory,TrustAbsRelevance,TrustRelevance,JoyCategory,JoyAbsRelevance,JoyRelevance,AnticipationCategory,AnticipationAbsRelevance,AnticipationRelevance,SurpriseCategory,SurpriseAbsRelevance,SurpriseRelevance,SadnessCategory,SadnessAbsRelevance,SadnessRelevance,FearCategory,FearAbsRelevance,FearRelevance,AngerCategory,AngerAbsRelevance,AngerRelevance,DisgustCategory,DisgustAbsRelevance,DisgustRelevance,NoEmotion
0,45033467,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1,45546462,"""I have problem with access to camera first time. I installed my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open.my AdnroidManifest.xmlmy class""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
2,49619897,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
3,55388663,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
4,52833231,"""How can i write a test case in Junit for amazon Rekognition.For the above program I wanted to write a Junit testcase. Kindly help me with the same""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
5,47919331,"""With AWS Rekognition I was able to get faces detected in a mp4 video with the following nodejs,And was able to get the results with the following cli,and outputs the faces in the following json format,How to extract those faces as images and dump them in an s3 bucket?Thanks""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
6,41895608,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
7,55122015,"""I am trying to use aws rekognition to compare faces but it will give an error saying check if the object and bucket exist in same regionwhile uploading the image i have set the content type to image/jpeg formatbut when i upload an image using aws console from computer the rekognition will work ! am i doing something wrong in this code""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
8,50145372,"""I'm pretty new to the google vision api and I am trying to make arequest.Currently I am reading from an image file, encoding it toand trying to pass it on to the requestBut I am getting aerror. Could anyone spot the error in the code?.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
9,46676980,"""I am working on a lambda function that needs to access,andservices from AWS.I gaveandpermissions via theand thepolicies respectively and it worked fineThe thing is that I could not access myinstance insidebecause it's inside a VPCI changed my lambda network configurations so it would be able to access the VPC, and theconnection worked as expected, but then the connection tostopped working, whenever I invokefor example it just hangs.Am I missing some permission?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
10,56094441,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
11,53815181,"""Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.I get an error :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
12,54683853,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
13,54122545,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
14,56065772,"""I want to extract handwritten text from an application form using Google Vision API's text detection feature.  It greatly extracts the handwritten text but gives very unorganized JSON type response, which I don't know how to parse because I want to extract only the specific fields like name, contact number, email, etc. and store them into MySQL database.Code ():Response from API:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
15,50133223,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
16,52419957,"""Through my IBM Cloud account, I have registered a Watson Visual Recognition service.Then I tried using their API to upload an image '' to my bucket associated with this service and then get some analysis on that image.I have tried the code available atTill the API and bucket validation, code works fine but throws errorI can see that this file is available in my bucket.Code:My current directory for IBM cloud notebook is""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
17,38811303,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""",True,1,100,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
18,56285629,"""I am usingto detect Japanese texts in the image. The response from Google contains texts like this: ""text"": ""\u5065\u5eb7\u4fdd\u967a."" I don't know which ""encoder"" Google is using for encoding japanese texts, UTF-8 or Unicode?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
19,55590797,"""I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related toJava Producer Library and SDKonly and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.Is there any alternative ofPutMediafor Android? Ifyes, what is it and how to implement it? And ifno, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.I have already referred the following links so far:Required Complete flow is as below:Detect the face from the streaming Video.If the match of the face is found then return True otherwise False.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
20,54612783,"""I am currently working on an Android app where I want to integrate the Google Cloud Vision API and do Facial Recognition on images.The code I implemented so far to make this work:The Layout XML File:The Log message:I basically don t know why the app crashes there with a null object reference and would appreciate any hints and feedback, thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
21,55685353,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False
22,48670839,"""I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
23,51336137,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
24,45260779,"""say that I have images and I want to generate labels for them in Spanish - does the Google Cloud Vision API allow to select which language to return the labels in?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
25,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",False,0,0,True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
26,49520402,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
27,47991101,"""Am trying to OCR around 50,000 documents(~20K size jpegs). Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracyBased on quotas published atmy calculation is600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.Has anyone done any bulk operations using google vision? We did some POCS where some of these bulk requests are taking ~5mins.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
28,52169264,"""I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:Code that I executed:the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
29,40921512,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
30,47982254,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""",False,0,0,False,0,0,True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
31,55606918,"""I am using below code to connect to Google Vision API. I have JSON from Google Vision.The code is giving me below error . Not Sure why..Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.Below is code and Error Details.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
32,52829583,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
33,45095486,"""I have downloaded the google vision api from. And I tried Barcode detector example, When I try to scan a linear and 2D barcodes, the scanned area(purple shape) shows in wrong location on preview layer.Note: This issue only occurs when I hold the device horizontally at the top of barcode.Herewith I have attached the screenshot which reflects this issue.Thank you!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
34,50805054,"""I am trying to perform OCR on PDF files using google cloud vision, to do some basic testing i am using the sample code provided in google documentation in the below link, but the line of code provided below is throwing unicode decode error given below, can anyone please help me fix this, i did an extensive search and tried different approaches but i am unable to fix it.Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
35,41285556,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
36,55330723,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
37,45122248,"""I was using ruby client of Google Cloud Vision, to extract the vehicle information on Automobile Original Titles.Observations:When I used the client API, i was getting 171 words.But, when I used the google's API demo here:, I got 459 words. It has much of the information I was looking for.Can anyone please explain, how to get the most out of the API ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
38,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",False,0,0,True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
39,50756150,"""I am trying to collect the SYS and DIA values from bp monitors ( Ex.) using Google Vision API, but i am not able to get the calculator font digits in the response ( I just get the other texts ).Is there a way to detect particular fonts using GCP Vision API?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
40,51076368,"""I am usingGoogle Visionforfacial detection, everything works fine but we have to know that if we are actually detecting actualHuman, One may try to play thevideo containing a Humanso we want to know if it is fromvideo/Imageor an originalhuman being?Is there any way toachieveit, I have search throughout for 2 days and did not get any lead on this,please guideme through thisthanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
41,46516494,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
42,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",True,2,67,False,0,0,True,3,100,False,0,0,False,0,0,True,1,33,False,0,0,False,0,0,False
43,54634576,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False
44,55665919,"""I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error. I have no idea what I am doing wrong.I tried to make this request via Postman and everything is working fine there but in c# it just won't work.Can you help me please?The C# request body looks like this:Whereas the Postman request body looks like this:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
45,55793092,"""I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
46,54353716,"""I'm testing IBM's Watson Visual Recognition using Node-RED, I've trained it to identify some elements in the image, but I wonder if it's possible to get the exact position of these elements.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
47,55276425,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
48,52126752,"""I am trying to call google cloud vision api from xamarin C# android application code.I have set environment variable but still I was not able to call api.So I decided to call it by passing credential json file but now I am getting error deserializing JSON credential datahere is my code""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
49,49732951,"""I am testing few sample code for OCR using Google Cloud Vision API. I observed the APIs can able to detectEnglishlanguage very easily from an Image but  in other language likeHindi, the APIs are not able to detect.MyCode :Image :But the same image, i have tried in Google Drive, all the text from the image is easily detected.Please let me know, same image how can i use in the code to detect the text?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
50,56216376,"""We are trying to extract the text from an image using google-cloud-vision API:In this code, we need to make the API read the image through the 'cv2' function only, instead of using the 'io' function:Any suggestion will be helpful""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
51,52848248,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
52,47361521,"""I'm using Python to make a query to Google's Vision API to obtain labels from an image, but I'm not able to set a timeout in case I don't receive a response within a given time.I'm using the following code based onof CallOptions.This is my code:I have tried passing directly the arguments into the call to Google without success, like this:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
53,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",True,4,100,False,0,0,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False
54,51195006,"""I am following the following Google Cloud Vision quickstart:This is using the API Explorer, and I getI have created a bucket named vision2018, and checked Share Publicly for the file.My portion of the request related to the file is:The response I get is:What do I need to specify in order to access files in my GCP storage?Alternatively, I read other Stack Overflows that talk about GOOGLE_APPLICATION_CREDENTIALS, Simple API Key, and ""Create Service account key and download the key in JSON format"", ...  but these seem to be giving commands in the shell, which this quickstart doesn't even open.Is there initial setup assumed prior to the quickstart?I am not ready to call the api from code""",True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False
55,50703766,"""I'm trying to use IBM Watson Visual Recognition tool with nodejs (express).I followed the instruction from the, but I can't connect with the tool.When I run my nodejs app, I got this messageDoes someone know the solution to this authentification problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
56,47436472,"""I am using the Socket IO-Swift library in my current project. After installing the Socket.IO-Client-Swift pod file into my project, my FMDB library path is not found, showing this error:I'm using the following pods in my podfile:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
57,48840806,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",False,0,0,True,1,50,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
58,50331196,"""In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following.Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB.Now, based onand, the maximum image file size should be20 MBfor images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB).I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:Is there something I did not notice?Note: I'm getting the same error when calling the Vision API directly (see).""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
59,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
60,46380748,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False
61,40103531,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
62,51381522,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
63,37908660,"""I have a similar question toand, neither of which have an accepted solution.I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.suggests using, but (having tried and failed) I'm not sure how to integrate it into my app.Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from mywhich I want to use to be able to toggle the flashlight:""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
64,54077087,"""I tried to create a custom model for my IBM Watson Visual Recognition API, by following the IBM's docs. I'm stuck at this point.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
65,47089134,"""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
66,52126381,"""I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable). Check the package spelling or your minimum-stability. What do that mean?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
67,51817443,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
68,49974375,"""I'm trying to install google's vision API on my system. I'm using the commandOn executing this command the following error is shown:Please help. How can I solve the problem?Note:-I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
69,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",False,0,0,True,8,100,True,2,25,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
70,43628002,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""",True,3,100,False,0,0,False,0,0,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False
71,40087567,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
72,44826567,"""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
73,41755822,"""I need to convertto aso that I can use it in the. I've tried the following...... but I get the following error:Google's example code references so the deprecatedAPI but I am usingwhich is why I cannot use them for help.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
74,53980744,"""i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document. My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can. Is this indeed the case.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
75,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",True,4,100,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
76,43410910,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
77,42102280,"""I am trying to get simple functionality from the Microsoft Face API, using this example provided ():Whenever I execute the code, I get a 400 bad request, of which I cannot how to view the specific cause. This is how mine looks:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
78,55463500,"""I am trying to set a React-native for detect text using amazon rekognition API.My guide is this tutoriali have configured the connection with AWS using awsmobile and amplify and in both cases i had the same error: API rekognition does not exist.My user has the corrects permissions and my modules and sdk are with the last version.My connection API.js is the next:Thank you!!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
79,53799283,"""Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen. This detection and placing ar node has to be on same activity and not two different activities. Is it possible to combine both (arcore and Vision) on same activity.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
80,45546546,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
81,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",True,2,100,True,1,50,True,1,50,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False
82,52276444,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
83,54432180,"""I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.Here is the link of Face API:And here is the code. In this code, I try to post a picture from a website:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
84,56302753,"""I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close). Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream. Although, I tried with OpenCV & Google Vision api, but they are not able to perform the task perfectly.Please suggest if you have any solution.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
85,50515317,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
86,51523206,"""I am trying to use VideoFrameAnalyzer library from Microsoft Face API product, so I cloned its Githuband tried to build BasicConsoleSample as well as LiveCameraSample.When I tried using Nuget to install opencvsharp, it gives me this error:An error occurred while tring to restore packages: The specified path, file name, or both are too long.For those who have got the near real time video face analysis working on Visual Studio 2017, any hint on getting it to work?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
87,55151128,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
88,36974179,"""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith. Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine. So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
89,55528723,"""I am trying to run a code that uses previous version of the google cloud vision api. How do I install the earlier version? Working on Ubuntu and using Ruby""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
90,45468418,"""I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text). I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better.Thank you.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
91,44812417,"""I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it? This way I can extract only the name.OBS: I open to using any kind o service that facilitates this""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
92,55095799,"""I am using this piece of code to fetch the data from vision API for matching pages, but I always got only 10 results, whereas in google vision official website the dataset is large than the same.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
93,35737830,"""How can this be completed with the Google Vision-API please?send image to vision-apirequest: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]receive the labels in particular the one I'm interest in is a ""clock""receive the boundingPoly so that I know the exact location of the clock within the imagehaving received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR libraryCurrently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
94,56407381,"""I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application. But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent).My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly.I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
95,47308379,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
96,53056817,"""Currently using the google cloud vision api for pulling text from images of documents.Current situation- the API works great, and returns tons of data including the bounding boxes of where the words are located.Desired outcome- to query only the words pulled from the image and not all the meta data about where the bounding boxes and vertices of the words are (it's like 99% of the response and comes out to be about 250k which is a huge waste when all I want are just the words)""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False
97,42122978,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""",True,2,100,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
98,49376836,"""I am implementing google cloud vision API from this.Below is my code where I am getting exception:I am getting errorMy graddle""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
99,54466934,"""I have a folder with 100+ images. I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.Using following code:I am usingto choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..Usedbut getting below errorfound one post but that is in python  unable to replicate it in R""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
100,41523004,"""I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error:Error1:Python Code1:The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct.The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.MetaData for the Object: Content-Type: image/jpegNot sure how to debug this. Any Suggestion to resolve this please ?Thanks,""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
101,33515465,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
102,54605435,"""I want to use Google Cloud Vision API for image recognition, Everything installed fine in my yii2 framework.I'm getting authentication error like :How to point my key.json file to GOOGLE_APPLICATION_CREDENTIALS environment variable In yii2 framework.Thanks""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
103,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",True,1,33,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
104,26010360,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
105,40346408,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
106,50763441,"""I have aws recognition code written in Python, and it run's by Node API, which works fine on Windows system but when I'm deploying it on Linux I'm facing this issue:-I have given both AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess access role to I'm user. Still I don't know how to get things going.Python code:-Node Code used to run Python script:-I have Python version 2.7 installed on my Ubuntu, pip version 10.0.1.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
107,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",True,1,33,True,2,67,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,True,3,100,False
108,43735666,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""",True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
109,41348880,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
110,55099418,"""I'm trying to integrate AWS Rekognition into my Rails app. After the user uploads his avatar via Active Storage, Rekognition should show some info about it.However, I get the errorHow can I get the image file property into AWS Rekognition?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
111,51156207,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
112,35602395,"""I want to do some Analytics on the image hosted on the cloud using IBM Watson Visual recognition. Currently I am downloading the image and storing it locally and then give it to the Watson visual Recognition service.I dont want to download the image locally.I am using JAVA""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
113,42375271,"""Can we use Microsoft Emotion API in our Android Apps, considering the fact that it's still in its 'Preview' mode, can we create our own customized app using the code of EMOTION API to recognize the moods of users in our own app?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
114,50782221,"""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
115,48892816,"""I'm trying to find a way to crop/obtain offsets of Movie titles via Google Cloud Vision API.Here's an example image:I've tried to use FACE_DETECTION, LOGO_DETECTON, and event LABEL_DETECTION but I can't seem to get a result for it.Any ideas?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
116,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",True,2,100,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
117,51611493,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
118,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",False,0,0,True,2,33,True,6,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
119,49388341,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
120,47641619,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
121,51991401,"""I'm trying to recognize faces in a video stream using thesebut I couldn't find any help to implement  PutMedia operation using Python. I'm  using Ubuntu 16.04 and Python 3.6. Any hint please so I can implement it using Python.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
122,44519968,"""I am accessing google vision api using requests.post method in python (jupyter notebook)in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""response i get is:please help""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
123,50673749,"""I have scanned PDFs (image based) of bank statements.Google vision API is able to detect the text pretty accurately but it returns blocks of text and I need line by line text (bank transactions).Any idea how to go about it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
124,52270452,"""I am facing issue while implementing Amazon Rekognition. The error I am getting is:AWSRekognition class, createStreamProcessor API call always through the following error:AWSKinesisRecorder class API submitAllRecords API call always through the following error:Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.Any help appreciated?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
125,45160721,"""I am trying to do a simple face detect call using AWS Android SDK Reckognition 2.4.4.  Can someone point what is going wrong?I am getting the following errorHere is the code""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
126,43425391,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""",False,0,0,True,1,50,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
127,47392166,"""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
128,50452142,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
129,50536717,"""What is the best and simple way to search and get images list from S3 by using English keywords. Or do I have to use the Rekognition to store all the image metadatas into database?My development is using Php.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
130,47557888,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",True,1,100,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
131,42431787,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
132,55585979,"""I am new to Google Vision, and I want create code to receive an asynchronous response. For example, create a JSON file to response and later load the JSON file and continue with de recognizer.I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.This is how I save the response to a JSON file:This is how I try to read and use the JSON file:but it does not work, it says""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
133,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",False,0,0,False,0,0,True,3,100,False,0,0,True,1,33,True,1,33,True,1,33,False,0,0,False
134,37515812,"""I am using the google cloud vision api to analyze pictures. Is there a list of all the possible responses for the labelAnnotations method?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
135,49936444,"""I am unable to save the image inbut can save in the same rekognition directory. How do I save a snapshot from camera to static folder?My OpenCV code is running in post method of.In:""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
136,38147675,"""I have integrated google vision in my project as shown in below post:Everything looks fine except the camera view brightness . The camera view here is very dark when comparing with my actual android camera app.Please let me know if i can increase the brightness of the camera and turn on any low light settings. Thanks .Pictures :,""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
137,52647919,"""I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text. For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR. Is it possible?Also is there any other way to not get jumbled up text when we have 3 columns of text in image?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
138,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",True,2,67,True,3,100,True,2,67,False,0,0,True,1,33,True,1,33,False,0,0,False,0,0,False
139,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",False,0,0,True,1,20,True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
140,48266531,"""I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
141,47849128,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
142,44996852,"""I'm trying to integrate the Google Vision API for face detection.But at creation of object FaceDetector, through FaceDetector.Builder the exception works and the application takes off.The activity in which FaceDetector is created:AndroidManifest.xml:dependencies:Full Error Log:p.s: When creating a FaceDetector object in an empty application, there were no errors or exceptions.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
143,50164690,"""If you try Google Vision API with follwoing demo-image.jpgshown in, you will get a record with empty description and score of 0.7024 in   . Why!?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
144,46151909,"""I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers.1) how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
145,48607548,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
146,44532633,"""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
147,52536703,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
148,52746720,"""In Azure Cognitive Image processing the returned json have a ""caption"" field which summarizes the content of the image. However, I didn't find anything similar in AWS.In Amazon Rekognition for image processing how do I get the caption for an image?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
149,49840929,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
150,55369637,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
151,51444352,"""I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the errorAny idea of what I can do in this situation? I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allowsand it still doesn't work.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
152,49615202,"""When trying out google cloud vision with the drag and drop, the last tab has raw JSON.  What parameter do we need to pass to get that data?I'm currently doing DOCUMENT_TEXT_DETECTION but it only gives data at the level of words and not of individual characters.Edit: I modified this codeand changed the feature ...and the printing to ...I'm only seeing textAnnotations in the output.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
153,46095355,"""Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer.and you can take this image as refHere is my code""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
154,56224197,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
155,46244980,"""I'm using the ""TEXT_DETECTION"" option from the Google Cloud Vision API to OCR some images.The bounding box around individual characters is sometimes accurate and sometimes not, often within the same image.Is this a normal side-effect of a probabilistic nature of the vision algorithm, a bug in the Vision API, or of course an issue with how I'm interpreting the response?Here's the portion of the response specific to the letter ""a"" from which I'm extracting the bounding box.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
156,47532783,"""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.the result of vertices""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
157,45296021,"""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
158,43494736,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
159,50035928,"""I am working on a project where I need to take a picture of a surface using my phone and then analyze the surface for defects and marks.I want to take the image and then send it to the cloud for analysis.Does AWS-Rekognition provide such a service to analyze the defects I want to study?Or Would I need to write a custom code using opencv or something?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
160,45775358,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
161,55135638,"""I  am trying to use AWS's face recognition from streaming Kinesis, but got stuck on create stream processor step with error:I have configuration with:IAM PolicyThe Role has 2 attached policies, AmazonRekognitionServiceRole and above custom policy.Code for testing (by Golang):""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
162,47571678,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
163,52343909,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
164,43687962,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",False,0,0,False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
165,49974994,"""I prepare some solution to detect text of the image, now I am getting bounding box of text, symbol, and language property.Is there any way to getting table structure of documents using Google Vision API?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
166,56266707,"""I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in.The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with. I don't really care so much about the line breaks, as much as paragraphs.How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
167,55937801,"""I am creating a collection using the amazon rekognition create collection api call .Does each person need only one image for him to be classified well?Or do we need to give multiple images per class(person) as done in facenet or other deep learning implementations to extract features ?I have already added all the images(multiple images per person) and it shows me it has detected someone well enough.But can the collection cluster similar featured images to form one person ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
168,37796918,"""I am new to Google Cloud Vision API. I am doing OCR on images primarily for bills and receipts.For a few images it is working fine, but when I try some other images it gives me this error:This is my code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
169,54560404,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
170,48381832,"""I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.Here the code and verisons I am using.build.gradleManifestCodeButalways returning false. I checked in OPPO(5.1.1) and Moto(6.0)TIA""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
171,49123683,"""how can i use google cloud vision with python django rest api? My task is that i have a picture,i have to find similer picture from an another picture.is there any other solution to do this task?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
172,56285264,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""",True,2,100,True,1,50,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
173,45559285,"""I have integrated Google Cloud Vision API in my java application for text recognition from complex formatted documents. One of my colleague suggested to use ""Tesseract API"".Can anyone please give difference between these two API's.And which is better in terms of accuracy or have any advantage over other.TIA""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
174,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",True,2,100,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
175,41166264,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False
176,55929206,"""Gettingwhen trying to detect text in a part (numpy array rectangle out) of an opencv image. When I try to convert the resulting base64 string online it works without issuesI foundon the topic but why would I need to do that?Any ideas what I could be doing wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
177,40136543,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
178,38211578,"""I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage. After that, I want to analyze the photo using Google Cloud Vision APIs.Uploading works fine.To analyze the photo, I'm specifying it usingThe problem is that I get the following errorDo you have any suggestion w.r.t. what permissions I need to set?Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
179,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",False,0,0,False,0,0,False,0,0,False,0,0,True,3,100,True,2,67,False,0,0,False,0,0,False
180,43520997,"""I have implemented microsoft face api, with the help of the library:The script is working perfectly on my local machine and I can identify the faces and store them on microsoft db. But when I upload it to my server, it is showing the below error and api call won't process:I have then installed the corresponding files with composer and place the folder with the 'Net' having URL.php..but then it shows the following error.Let me know if you need any further information...Thanks...""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
181,54061460,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
182,52326351,"""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
183,51479671,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
184,54218010,"""I  want to use the Microsoft Emotion API but program it to detect an emotion it currently does not. Is it possible to do this?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
185,35748095,"""Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ? For unfamiliar ppl via some learning/look-alike mechanism ?Thanks.""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
186,41803160,"""I am looking for a Google Cloud API that can do both face recognition and identification. My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.Is there any Google Cloud API that can do face recognition?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
187,48448138,"""do I understand the Documentation right and it's not possible to use the Amazon Rekognition service with ""external"" links without priorly save the images on Amazon S3 or another local server?Why simple web hosted images are not supported?Thanksffp""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
188,42536697,"""Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?As best I can tell I've included everything necessary as build.gradle hasHas anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
189,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",False,0,0,True,2,67,False,0,0,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False
190,45720763,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""",True,2,100,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
191,51609428,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",False,0,0,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
192,49167306,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
193,49249692,"""I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3. I am getting the following error:Any ideas what I need to change here.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
194,49126860,"""I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.Stacktrace:""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
195,50734416,"""I am using a lambda function ofAnd I am using this docwhere for comparison I am using thisAnd I am getting this error""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
196,50702342,"""I was trying to create a code using python that uses Watson Visual RecognitionI was wondering if you can send the image URL instead of a local image path to classify.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
197,54897189,"""All of Amazon documentation on their Video Rekognition API are examples of videos that are stored in S3 bucket. Is there anyone out there who have tried using the API without storing the videos in S3 i.e. on local machine or GCS?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
198,55599305,"""I have consumed the google cloud vision api to recognize a document with a table, but sometimes the image will be a little rotated, im triyng to get the value using theof the key i want, but how do i get it if it's not on the same.I was thinking of making a 'line' above and below theand finding if the point is between that, but i dont know how to do it.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
199,50722472,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""",True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
200,52440902,"""I'm using the ""DOCUMENT_TEXT_DETECTION"" option from the Google Cloud Vision API.It seems that it's returning correct text value, but incorrect coordinates bounding box.Why this problem occurred?Thank you.raw picturedraw bounding box picturereturningappendixdraw bounding box words and overall""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
201,48115069,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""",False,0,0,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
202,50894208,"""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
203,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False
204,56193287,"""I need to develop a face recognition system in using Angular with Azure Face API. However, the documentation for Azure Face API is in C#. Could anyone help me rewrite it to typescript?is the guildline for face recognition in Azure Face API""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
205,55447535,"""I want to implement the google cloud vision textDetection using a google cloud vision.I have install the composer from google cloud vision to the thirdparty vendor in codeignier.What my setup in construct is :and my function to call the OCR is :But before process the text detection i have run into an error :Which is this line :$imageAnnotator = new ImageAnnotatorClient([What could possible cause the error ? From the construct above i already include or require_once the Path to the Class.Is there something that i have missed in here ?Thank You""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
206,44093669,"""I'm new in Google API.Recently, I use the Google Vision API but I met the following problem:And I tried the solution about ""Create the service accout"" to generate the service json key and invoke it in py.script,it will work first in almost 3~4 url, but it will error in next url.This is my detect code:And I invoke the ""detect.py"" in another py.script:""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
207,42188322,"""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
208,49231034,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",False,0,0,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
209,49912384,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
210,48733647,"""I am working a Xamarin.Forms App that uses the Azure Face API. With this API you retrieve a JSON response (See Below).I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.This is how I set the JSON data to a string.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
211,51561234,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
212,41397369,"""I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response:I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
213,55047291,"""I am developing an Android app to detect text from the PDF file.First, I tried to use Google Cloud Vision API.But it required to OAuth 2.0.So I changed from it to Firebase ML Kit.But when I run 'fromFilePath' method, NPE occurred.It looks like the Firebase ML kit doesn't support PDF file, right?Is there any good solution?Is it impossible to recognize text from the PDF file using Firebase ML kit?I tried to test more file formats: JPG, TIFFAll is same, just input file is changed.JPG works fine, but TIFF has the same problem.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
214,49881417,"""in Watson Studio I am writing code in a Jupyter Notebook to use a Watson Visual Recognition custom model.It works ok with external images.I haven't been able yet to refer to an image I have uploaded to the Assets of my project. The url of the asset gets to a full page not the image only:Thank you""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
215,47749413,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
216,46189464,"""I have question on Watson Visual recognition Service of bluemix?Is 50 Images a minimum requirement to recognize a face of a person?What would happen if we train with less than 50 images? What would be the consistency of the output in terms of facial recognition?Requirement is, Retrieve the employee id of an employee by his facial(visual) recognition.Is it achievable with Watson visual recognition Service?In real time, it may be little hard to have 50 images of an employee or a person.?Thanks,Priyanka""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
217,55894824,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
218,54425585,"""I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
219,44184869,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""",True,2,100,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
220,56016618,"""I am implementing google vision API to convert pdf to text. I am at the end everything works fine but getting an error at the endI have usedOutput file is showing Output files:But after that getting.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
221,56051643,"""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
222,40281166,"""Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.This is how I am fetching the credentials, from the examples that the documentation provided. Is there something wrong?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
223,53486685,"""**help me this a simple script to implement google vision API in python .i installed all requirements i need but still see that error**AttributeError: module 'google.cloud.vision' has no attribute 'Client'""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
224,49752955,"""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but thereturns null only, And My 2nd Question is how to do brightness option...""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
225,48077116,"""I use Google Cloud Vision Document OCR API. The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
226,46693204,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
227,46760602,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
228,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",True,1,50,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
229,48445901,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False
230,38048320,"""I am working on the python example for Cloud Vision API from.I have already setup the project and activated the service account with its key. I have also called theand entered my credentials.Here is my code (as derived from the python example of Vision API text detection):This is the error message I am getting:I want to be able to use my own project for the example and not the default (google.com:cloudsdktool).""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
231,41126381,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
232,51646568,"""The release of Google Cloud Vision API had been on July 24, but what else was there later?There was no problem when using Document Text Detection of Vision API on July 25, but when I did the same thing on Augst 1, it became a error response ""Bad image data"".Not all images causes this error.  Several images causes ""Bad image data"" response.Using the not Document Text Detection but Text Detection API, I can get the correct response.Does anyone else have the same problem?My code is following.Response is following.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
233,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",True,1,17,False,0,0,True,6,100,False,0,0,False,0,0,True,6,100,False,0,0,False,0,0,False
234,54439669,"""I'm doing a system in C # that needs to parse an image of a keyboard returning the position of the characters in it.I tried to use IBM Watson but it does not return the position of the classifications, after that I tried to use Google Cloud Vision because in the site demo it returns the positions of the characters in JSON format, however I had problems with GOOGLE_APPLICATION_CREDENTIALS (look).I would like to know if there is any other alternative, preferably free or with a lot of free access, to do this kind of reading of the image and return the position of the characters?I do not need OCR I want to return the position of the character in the image""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
235,44543092,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
236,42420031,"""Testing thecall of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented atIs this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.This seems to be the same follow-up question asked here:Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.Code at the moment.  Have tried it with smartCropping set to both True and False.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
237,43277815,"""We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.Account is ""Pay as you go Standard"".Thank you.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
238,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",True,2,100,True,1,50,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
239,50553795,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
240,53043003,"""I want to compare two photos. When I connected to AWS I try to connect to:But have this error:Full code for getting information about photos:What is wrong? What I did wrong in this code?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
241,53117918,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False
242,36125830,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
243,39551502,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
244,52308804,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
245,54469189,"""When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?Heres my request:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
246,54979768,"""I try to use google cloud video intelligence demo on their site:and it works perfectly fine with their predefine demos to choose. When i try to use my own location the video loads forever.Even if I just download their sample video (which worked) and upload it in my bucket.I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
247,54048657,"""Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file?This is the code that I am using the print out the operations for a tensorflow model and the output.Theandare the inputs and the outputs. Thetakes a tensor of shapeand theoutputs a tensor of shape. If I am detecting just a single object, how do I get the bounding boxes from thetensor.Where am I going wrong? Any suggestions are welcome.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
248,54365930,"""i'm studying the Azure Custom Vision service for object detection, but I would like also to extrapolate text information within a tagged image zone.Is it possible with Custom Vision?If not, is it in the service roadmap?Thank you""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
249,52659767,"""I have to try AWS Rekognition API's. And am new for PHP. So, Now am using PHP code for SandBox.Now, I got following Error,Note: my sandbox test cases URL:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
250,52397039,"""I have a large set of difficult words to recognize through    .The image are all of lowercase latin alphabet without diatrics, but despite using languagehints, I end up with results likein a lot of cases.So how to force Google cloud vision to use a specific set of letters and not just hint ?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
251,55568129,"""I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly. How to use this API more appropriately so that I can extract MICR accurately.""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
252,44167057,"""After taking a picture using the Google Vision Library () I grab the orientation from Exif data usingThis is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:However ExifInterface defines 2 as.Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
253,54306129,"""I'm trying the Rekognition API from aws in my new Android app in kotlin, but when I try to create the client, my app crash.I put the json file in the raw folder.This is my code:""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
254,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",True,4,100,False,0,0,True,3,75,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
255,52365814,"""Is possible to modify the maximum detections that Amazon Rekognition textDetection has? It only detects the first 50 occurences, but we need more (at least 60).If not, do you have any idea of how to make a workaround?Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
256,53259815,"""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,How can I get that JSON from annotation structure? Is it possible?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
257,51973564,"""I am creating an app in app inventor for that I need to detection emotions. So I used API (Google Vision API) to make my work easier. But got sucked in the screen when I access the url.Here is the screen shot""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
258,53865532,"""As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces. But its giving me error :I tried earlier using S3 bucket images and it worked properly. But right now I am trying to send the images without using S3 bucket.I am using a Lambda function, and I referred toMy code (edited version) :And the error that I am getting in the Cloudwatch is :Where am I going wrong?, as thethat I am passing is of theformat.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
259,42984821,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False
260,44832036,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",False,0,0,False,0,0,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
261,46061561,"""I am trying to do translate a document with google translate from the package google.cloudI already did:and the result was:then I called the package in Spyder (Python 3.5):I obtained this error:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
262,41711704,"""Is it possible to detect faces using Camera2 with Google Vision API only ? I could not find a way to integrate it.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
263,49471062,"""how am I able to extract just the ""roll"" from this list which I got from Amazon Kinesis/Rekognition using Python 2.7?{u'FaceSearchResponse': [{u'DetectedFace': {u'BoundingBox': {u'Width': 0.10875, u'Top': 0.08555555, u'Left': 0.775, u'Height': 0.19333333}, u'Confidence': 99.82224, u'Pose': {u'Yaw': 39.53371, u'Roll': 10.791267, u'Pitch': -1.0082194}, u'Quality': {u'Sharpness': 99.93052, u'Brightness': 44.374504}, u'Landmarks': [{u'Y': 0.17006741, u'X': 0.81887186, u'Type': u'eyeLeft'}, {u'Y': 0.18348174, u'X': 0.8479081, u'Type': u'eyeRight'}, {u'Y': 0.21523575, u'X': 0.8444541, u'Type': u'nose'}, {u'Y': 0.2389706, u'X': 0.81935763, u'Type': u'mouthLeft'}, {u'Y': 0.2415149, u'X': 0.83268094, u'Type': u'mouthRight'}]}, u'MatchedFaces': []}], u'StreamProcessorInformation': {u'Status': u'RUNNING'}, u'InputInformation': {u'KinesisVideo': {u'ServerTimestamp': 1521934266.557, u'FrameOffsetInSeconds': 0.035999998450279236, u'StreamArn': u'arn:aws:kinesisvideo:us-east-1:086906171606:stream/AmazonRekognitionVS/1520802835146', u'FragmentNumber': u'91343852333181789275940108114159018792280348730', u'ProducerTimestamp': 1521934266.294}}}""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
264,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",True,3,100,True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
265,52805384,"""I am receiving this error message when trying to upload to an AWS Lambda.  This is from the- exampleIn particular it says IAM is not authorized to perform iam:ListRoles nor iam:ListPolicies.I checked my IAM user's AWS Lambda ListFunctions in the AWS policy simulator which says it is working , although I do not know if this is relevant to my problem.thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
266,43534783,"""Can someone tryand see if it works?Here's the.All I get when I try running it is a black screen (after fixing). I don't get any errors in the logs either.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
267,44794681,"""I am working on Google Vision Api and I know how to send a request by using any image uri below.I want to use send request by using Php Client Library. When I look at the sample codes on Google Vision Documentation, it shows only sample for Local images of Remote images that work on Google Cloud Storage.As you can see my json request, I need to use Php Client library with remote url but I couldn't.Thank you.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
268,43910100,"""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
269,43844506,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",False,0,0,False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
270,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",True,1,25,True,2,50,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False
271,48145425,"""I am Deploying Google cloud vision Ocr in My angular2 webapp.but i am getting many of the errors when i add this code in my webapp code.please help me to sort out this.When I run this code ,it gives me this output:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
272,51145859,"""When an image gets captured it defaults to left orientation. So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
273,53709061,"""In the Amazon Rekognition Developer Guide  there is a tutorial  ""Create the AWS Toolkit for Eclipse Lambda Project""It tells me to resolve namespace issues in Eclipse  by doing this :However I am not given an option  to choose  the latest Amazon Rekognition archive.Is there another  way to load this archive  or  force the install of this archive?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
274,54307715,"""I am trying to deploy my Geo-Django app to Zappa1st I gotThen I followed thisand added the belowI set these environment variables in my AWS Lambda console:and in my (Django) app's settings file, I set:Now I am getting the errorHow can I fix this ?Below is my zappa_settings.json""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
275,55921253,"""i am using react-native-camera module to take pictures but i want to set the camera brightness via a slider, does this module support brightness settings like the one in samsung's native camera appmy current configurations are:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
276,48709133,"""I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
277,45849380,"""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
278,51778072,"""I am using the OCR functionality of mobile google vision. I used the sample example where there is a box surrounding the text being detected.I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text. How can  I improve the performance so that the box is more realtime when the camera is moving ?Thank you""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
279,36944481,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""",True,1,50,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
280,54351470,"""I'm trying to get bounding box from an image in Rekognition, i get the label but i get:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
281,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",True,1,20,False,0,0,True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
282,50785134,"""I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
283,55514176,"""I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
284,45563012,"""I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
285,33482245,"""I am trying to develop a face tracking app using the Google Vision API ()This is my manifest:This is my code:This is the error in Logcat:Why does this happen (...on an Xperia Z3 compact 5.1)?UPDATE:I spotted a new error. I think it might be the reason why my code is not working.How can I resolve this problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
286,50780962,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
287,55252358,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
288,49207943,"""I am using Google cloud vision OCR to detect text. The displayed text is always 1. detected text, 2. each of the detected words. I only want to display the detected text.I am using the code fromwhere I set the type to Text Detectioninmethod.I also modified themethod to:This is my gradle:The detected text ofthat was displayed is:But I want it to only displayHow can I do it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
289,55950028,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
290,54131993,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
291,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",True,1,33,True,2,67,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False
292,40512241,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
293,41562347,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""",False,0,0,True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
294,45376533,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
295,47430886,"""That is my code which I'm running on terminal, I tried but don't know why is that error coming. What changes should I do??""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
296,51027161,"""I am trying to use Microsoft Computer Vision to make a handwriting analysis of an image, but I'm not getting a 202 response, and I'm not able to find the ""Content-Location"" header on my responde, I try it on PHP, Javascript, Python and Curl, using Microsoft Examples, but I couldn't find the header in any of my tests.That's my response:I also try to dump the response, but I also wasn't able to find the header.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
297,41388926,"""I'd like to try, but I don't see a full example of the syntax for using the HTTP API. Assuming I have two images, how would I call this API from Python to retrieve a similarity score?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
298,52414541,"""I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example atunder theheader. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile errorThe below is my code:I get error at the line below:Any hints please?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
299,40156561,"""I'm trying to call the Google Vision REST API from a Xamarin.Forms app. I have the following code:-But this is returning me aerror message:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
300,38280779,"""I'm working on an OCR android application using Google Cloud Vision APIFor testing I've used the sample application which is provided by GoogleI've tested it for type ""LABEL_DETECTION"" and it works fineI've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""I've tested it using this image and it return ""nothing"" result[ocr_image]Appreciate if anyone knows what is the issueThanks in advance""",True,2,100,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
301,44362421,"""We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:a) What is the maximum size of the image?b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day?c) The customer has about 2 million images. How can we upload the images faster?d) Is there a separate plan available for bulk volumes?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
302,50829647,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,33,False,0,0,False
303,53938721,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""",False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False
304,55617828,"""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
305,54015677,"""I am using the following code for Rekognition.It is taking almost 1 min to finish the face detection between the 2 images.Is this normal? I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrongThank you""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
306,45366479,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
307,42631231,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
308,47924385,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""",True,1,50,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False
309,47779841,"""I am using Amazon rekognition to compare faces in php (AWS SDK for phpthe documentation.The code belowI don't know php. How can I get the confidence of the picture?I try some things but I cant get it.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
310,49293605,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
311,46580171,"""By that I mean:I can opt-out of using the images for training, orMy users can delete their images (if they are used for training by default).My reading of the Google Vision terms indicate that they are completely non-compliant.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
312,47885650,"""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
313,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",True,2,33,False,0,0,True,6,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,17,False
314,50417917,"""I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS). Is internet connection required to use Amazon rekognition?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
315,40278271,"""I need to store the output  of IBM Watson visual recognition service in ibm cloud or any other database, which can be accessed for further use.Can anyone please help me out? thank you.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
316,45727079,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
317,47671289,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
318,37718233,"""i have created a method - (IBAction)clickToGenerateEmotion:(id)senderThanks in advance!!!""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
319,49589780,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
320,49657006,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
321,52534025,"""I am trying to implement AWS Rekognition. Be default API allows only one image.I want to upload multiple image for face recognition. Is it possible?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
322,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",True,2,100,False,0,0,True,1,50,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False
323,52274071,"""I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.Points to be noted:-All data should be in kept inS3only.I can't change my cloud storage toGCSnow.I can't download files fromS3and upload toGCSmanually.The numberof files that are incoming per day is more than 1000 and less than100,000.Even if I could automate downloading and uploading of the pdf, thiswould serve as a bottleneck for the entire project, since I would have to dealwith concurrency issues and memory management.Is there any workaround to make this API work with S3 URI? I am in need of your help.Thank You""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
324,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
325,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False
326,50739245,"""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case . ATM using this""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
327,38869448,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
328,46090578,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
329,51935317,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
330,49527406,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
331,53464278,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",False,0,0,True,2,100,True,1,50,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
332,54996104,"""We are doingand we do have a requirement to validate text and graph in images.I triedbut it is not giving a consistent result.I came to know about, but don't know how to set it up with Eclipse Java.Can someone tell me,Is it paid?How to set it up with the existing selenium maven project.Any other accurate alternative for validating images?Thanks,Nilesh""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
333,56326059,"""I want to extract text from the image and using google vision API but getting an error ""str object has no attribute batch annotate images"".The error showing to me is the followingI will be good if you could provide me with some source link where I could learn more about the GOOGLE API""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
334,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",True,1,25,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
335,53658135,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
336,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",True,1,33,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,True,1,33,False
337,53613704,"""I'm triying to multiple language text detection with google cloud vision. But I have a problem. If I send the request text detection api endpoint this url;and this body;I'm getting the this error code;}What is a problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
338,48608334,"""I am trying to develop android face recognition app. Starting from-how can I get camera preview FPS value?I understand that I can set requested FPS onwith, but I wantactualFPS value updated every second or every new frame. Is it possible to get or calculate it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
339,40372942,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
340,48529675,"""I want to create a barcode scanner that uses full screen.I used google's vision API samples that can be foundThis is my result:I want to make a preview of the camera to the full height, how can I do it?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
341,41903995,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False
342,44465669,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
343,56406581,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""",False,0,0,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
344,50893836,"""I am new to microsoft face API. Is it possible to use it in linux environment?I could not find any documentation about it in their website.Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
345,49256697,"""I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :.But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.I design this boundary in my app and i want read Qr Code from only from boundary:-""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
346,43302771,"""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
347,52475518,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
348,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",True,9,100,True,1,11,True,2,22,False,0,0,True,1,11,False,0,0,False,0,0,False,0,0,False
349,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",True,2,67,True,3,100,True,2,67,False,0,0,True,1,33,True,1,33,False,0,0,False,0,0,False
350,44856326,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
351,55500377,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
352,55514812,"""I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.see below errorAnd my code is""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
353,44910023,"""I am using below code to call google cloud vision api. not able to find out how can I set response timeout for the request in case I do not get response within a set timeout.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
354,43124732,"""Im trying to install thepython package, and I encounter the foloowing meesage whenn the installation fails:what could cause this? I installed other packages without problems.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
355,40258634,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
356,43759806,"""I am trying to use the google Vision API.I did the following steps:Enabled the Google Vision APICreated the service account idSetted the enviorement varialbe 'GOOGLE_APPLICATION_CREDENTIALS' with the json pathdownloaded the API with composerwrote the following code:$image = $vision->image(file_get_contents($path), ['FACE_DETECTION']);$result = $vision->annotate($image);When I lunch this code I obtain the following error:Somebody can help me to resolve this problem?Thanks a lot!!!""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
357,53081398,"""Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?Apparently there is new functionality released, described here:Improved detection models are now available for the following features:Logo DetectionText Detection (OCR)Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.We'll support both the current model and the new model the next 90 days. After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.This is what my code looks like now:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
358,52149997,"""My application run normal until now.But suddenly one service account (used for google cloud vision) create many Computer Engine instances(about 32 instances).I checked log and see this line.My billing increase from 10 to 200$/day :(Anyone help me?""",False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False
359,54345710,"""I'm facing an issue in the Azure Face API. It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False
360,44096947,"""Android Dev withI see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Androidwe use thedifferent from. Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
361,49669981,"""I am trying to use google cloud vision OCR API to read text from image.var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
362,47838580,"""The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte? How can I get a bitmap in ruby?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
363,35532645,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
364,45139378,"""I try to analyze multiple images with AWS RekognitionIn main function in a loop:Rekognition Class:But one api call takes about 2 second at least, and I would like to make more parallel calls if possible""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
365,56391486,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",False,0,0,True,2,100,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
366,53744481,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:Note :img_stris a byte arrayFirst error: sendall() argument 1 must be string or buffer, not dictReason in my understanding: { ""Bytes"" : image } is a Json and NOT a stringMy Solution: Make the json a string ( not sure whether I can concatenate img_str ( a byte array )Now error: Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)QuestionHow do I concatenate a byte array (img_str) with strings without losing the array ?Can i convertimagevariable to string WITHOUTgetting the can't decode byte 0xffexception ? orCan we do something else to overcome this issue ?Thanks in advance guys !!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
367,51103236,"""Not sure what the issues is as my code just stopped working overnight, but the text detection on Google Vision is either returning nil or returning words that are non-existent on the subject.Here's my request function:Part of my analyze results function:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
368,38984901,"""I have been testing the Microsoft Computer Vision API with some pictures I took and it is not able to properly identify what I am uploading. Is there a way I could teach it what I am uploading is?My tests have been using.A sample image:It should include ""bottle"" in the tags at the very least.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
369,36655630,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",False,0,0,True,1,100,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False
370,39905841,"""I would like to get the data from Google cloud vision API and see the input can be given in the base64 and image uri format.But base64 appears to be too long and to upload the image as uri it take some extra time.Please let me know if anyone knows of any other work around for this.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
371,51389440,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
372,54847537,"""Is it possibile restrict the search of Google vision api filtering by specific countries or geographic zones ? By language ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
373,50715542,"""Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.I am getting error in below line. And tried to open this url is not working:Below is my design code.Below is my code which worte in button click for display in labelI used below example url:I created service key account also in google for json file.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
374,53701338,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
375,52219324,"""I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.How can I convert it to human-readable text with utf-8 format?Thanks all of your answer, may be I post my code is more easily for all of you.Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success.try:    code = requests.post(''+GOOGLE_API_KEY, data=params,headers=headers)except requests.exceptions.ConnectionError:    print('Request error')Thank you""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
376,48704050,"""I got ""undefined"" while trying to Parse this JSON file:This file is th result from a XMLHttpRequest from the google vision API and This is what i'm doing to print ""description"" field:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
377,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",False,0,0,False,0,0,True,5,100,True,1,20,False,0,0,False,0,0,False,0,0,False,0,0,False
378,56184573,"""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the sample photos i put in the faces folder. thank you !""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
379,55766327,"""I have videos stored at amazon s3 cloud and i need to analyze it using google vision cloud and nodejs. Please help me.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
380,44119233,"""I'm trying to run thebut I'm getting this error:Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;These are the characteristics of my project:1-POM.xml file2- Class3- Google Cloud SDKI used the command: gcloud auth application-default loginCan anybody help me?Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
381,55683585,"""I am trying to access image rekognition service of AWS using google-App-script, for this, I am trying to generate an AWS signature for API call, but response showing an error message.when I am trying with postman using authorization (AWS signature) it is working fine.below is my google app script code in which I am calling the main functionI am not able to figure out what I am doing wrong in the code, or it is the wrong method of generating the signature, please help.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
382,50210568,"""I want to build a deep learning chatbot application which accepts image as input. I have built a lambda function integrating AWS rekognition that accepts image.Now, i want to extend this lambda function, and connect it to Amazon Lex bot , where user can upload the image for analysis.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
383,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",True,2,100,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
384,43586009,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""",True,1,50,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
385,48896074,"""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file usingand now the file shows strange characters.shows(after sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
386,44556228,"""I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:I getting a bad request error. Need help with this. The format of my json object (request) is as follows:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
387,32558923,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""",False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
388,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",True,4,100,False,0,0,True,2,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
389,43070014,"""Iam doing a project with nodejs and aws.I am using WebRTC and taking photo.After i am taking photos base64 data and posting nodejs and i am putting it my aws console and i am using it for detectfaces but its giving error.But i am adding photo from my aws console manually detect faces not giving a error.My codes here : MY WEBCAM JS : this is giving a base64 for me.and i am trying post with POSTMAN CHROME EXTENSION to my nodejs i can put it well but i can't  using a detect faces.My nodejs :MY ERROR :How can i do this please help me i could not fint anything.Thanks for help.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
390,46509380,"""In the above code if I provide the source image as david.jpg it will work with no errors,but if I store image name in a variable and use json_encode and send it as source image.It throws a big error.What am I doing wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
391,47427567,"""I am trying to testwhich uses Google's Vision API. In, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
392,55376350,"""I am facing an issue with Azure Computer Vision API. If I send a request with contentType = application/json and image URL in JSON request body things work fine but on sending a binary image(base 64 encoded) with contentType = application/octet-stream it gives me ImageFormatInvalid in the respContent-Type: multipart/form-data and asking input as binary image data""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
393,43660043,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
394,54672488,"""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
395,49631726,"""I getas I am trying to pass an image to. I make an HTTP request to an, download the image and pass the buffer to the aws function.Here is the code for it:I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?Here is the trace for this:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
396,54132185,"""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
397,53239821,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
398,55464809,"""I have try to implement the google cloud vision with API ImageAnnotator using a codeigniter PHP.I have install the require google cloud vision using a composer to my third party directory in codeigniter.This is the code looks like in my controller :I got the error :I dont understand why this error occur :does not existBecause when i opened the link the file is there.And this error :is a line code :$middleware = ApplicationDefaultCredentials::getMiddleware($scopes);What is the proper way to use the google cloud vision ImageAnnotatorClient in codeigniter PHP ?Is there a problem with the authentication to google cloud api ?Thank You""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
399,55043291,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""",False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
400,52383178,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
401,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",False,0,0,False,0,0,True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,20,False
402,45790003,"""What is the daily limit on the number of images that could be processed using Watson Visual Recognition. Free plan on the doc shows 250. Can we upload more on a Standard plan ??""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
403,47744054,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
404,55382899,"""Is their any API from which we can get the number of person trained in a particularin?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
405,56122848,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
406,55466156,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
407,47570243,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
408,55170430,"""I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!Some idea about What can I do to check that the person is a person and not is a photo of a another person?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
409,53790848,"""I am using google vision library for OCR application (according to) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?Thanks every one""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
410,54685737,"""We are using Microsoft's Azure Face API for over 12 months now. But our low level API Android users (19) starting to get this error:javax.net.ssl.SSLException: hostname in certificate didn't match:  != <.cognitiveservices.azure.com> OR <.cognitiveservices.azure.com>It is working on level api 21+""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
411,56175138,"""Wrong image recognition on Azure Custom Vision Service.I have a doubt. I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
412,41998868,"""I am trying to compare faces using AWS Rekognitionthrough Python boto3, as instructed in the AWS documentation.My API call is:But everytime I run this program,I get the following error:I have specified the secret, key, region, and threshold properly. How can I clear off this error and make the request call work?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
413,42222036,"""I'm currently using node.js to create a post-upload API, to upload an image, which is processed by the Watson Visual Recognition Service. This returns a JSON, which is currently logged to the console.Is there a way to send this JSON back to the user, after the process is done?I'm a total newbie to Node.js, so I really appreciate your help.This is my code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
414,39735249,"""It is great to get face detection using Google vision API for Android but there is no where I can find how they did that and how it is compared to other state of the art face detection techniques like Deep-learning etc. Did they publish their research somewhere?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
415,53798619,"""I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:And here are the relevant parts of my API request route:Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:This is what is showing in the terminal after the base64 conversion of the foregoing:It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data.I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
416,51186135,"""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it. My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
417,50808403,"""I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
418,54722619,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
419,43875720,"""I am working on running few examples on Microsoft Face API, I would like to know if there is a possibility that i can send the request to my local cloudlet to fetch and retrieve information instead of sending the request to Microsoft?Thanks!""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
420,52003791,"""Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me.My MainActivity Class}My activity_main.xml file""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
421,39161387,"""Using Google Visioni want to take photo to detect face, but photos are rotated. I had tried to rotate it using my legacy codebutalways return 0 so image keep wrong rotation""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
422,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",True,3,75,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
423,40843164,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
424,44394496,"""Does anyone know if it is possible to use Lambda to call Rekognition functions when an image is uploaded to an S3 Bucket?I am looking at integrating a Raspberry Pi device with a Pi Cam to take photos and do some face recognition.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
425,54110288,"""I have to send an request with an image to Microsoft Azure Custom Vision with the header ""Prediciton-Key"" -> myKey  and the body should be an image (binary data).First of all i tryed with postman and it works fine, but when i put it on Unity ,i get an error :401 Unauthorized.Does anyone know why? (i put the api key as string)""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
426,53003814,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
427,42406824,"""I am using PHP library for using Google Cloud Vision. The docs tells about 2 ways of authentication - 1) API and 2) Service Account.How do I use API based auth with my VisionClient?There is no document on using it. Please let me know if I am wrong.I get the below error message when running the above code.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
428,44607269,"""I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.Download and install ""GoogleCloudSDKInstaller"".gcloud auth application-default login to activate my logincredentialUsedcode to run the face detector, for the imagein my local system.cd into the folder where image existsThen run the following code:python detect_face.py ""image.jpg""But I'm getting this error:Can anyone please tell me why I'm getting this issue?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
429,50546373,"""I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
430,44276413,"""Error:Execution failed for task ':app:transformResourcesWithMergeJavaResForDebug'.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
431,43130920,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
432,48865892,"""I am currently working on a project which get the images(scandir by php) from the server() and then pass it to Microsoft Emotion API for emotion analysis.However, after I pass the concatenated url to the ajax request, I got below error:My code is like this:I tried to remove ""JSON_FORCE_OBJECT"" in json_encode while the same error shown.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
433,50860448,"""Using Google Client Library interacting with the vision library.I have a function to detect labels from an image.GoogleVision.pyI have an api to call this function.However, it does not return the result and errors with the following""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
434,52906415,"""Here is my vollyNetworking codehere is my errorHere my cloud console shows the exact amount of request so it means that my request is going to my console so what could be the problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
435,42004626,"""I use Microsoft Face API and I want to show data to final user, but how can I use foreach to atteint faceAttributes->age ?There is an example of JSON fileI tried this code but not working :Thanks !""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
436,38860919,"""I'm trying to use Google Vision service with NodeJs. However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?For example: the real text ""  renci"" but the service returns ""ogrenci""""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
437,53530035,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
438,55450247,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""",True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False
439,53585778,"""According to the, the maximum number of image files per request is 16., however, I'm finding that the maximum number of requests per minute is as high as 1800. Is there any way to submit that many requests in such a short period of time from a single machine? I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible).""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
440,39641111,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
441,43657393,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
442,42135072,"""I am working with Watson Visual Recognition and have successfully created a custom classifier.I notice the build in default classifier can return a hierarchy eg: Animals/Dog, But how to create a custom classifier return response contains ""type_hierarchy"" such as default classifier ?It may be necessary to train a custom classifier with more positive class and negative class or it is possibly due to me being on the trial version  ?!!""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
443,51556793,"""Is it possible to deploy Azure Face API trained model to IoT Edge like Custom Vision?If it is, please answer me how to do that?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
444,55897523,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
445,56213065,"""I noticed the Google Vision PDF OCR DOCUMENT_TEXT_DETECTION takes about 15 seconds to detect a single PDF page.But if I submit the same PDF page as JPG it takes less than 3seconds to detect textsI used the code provided here (C#)I noticed it takes about 15 seconds for the following line of code to say all text in PDF is detected and saved to gsBucketMy GsBucket is ""Multi-Regional Storage"" USI'm also uploading from a US locationI was wondering what else I can do to speed up the process or this is expected?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
446,54094156,"""I am using the Google vision api to extract the text from an image and I also want to store this text in a .txt file.Whenever I useI get:Withit gives me:gives me:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
447,39378862,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
448,55465835,"""I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
449,56002802,"""I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want toextracttheTotalandVATfrom the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.Following are some commercial products of receipt scanning which use the ML approach,The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
450,55268232,"""I am followingarticle trying to make my Python script read labels related to an image using. The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
451,46152610,"""I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.Does anyone have any experience in this? ThanksMy node-red flow is here""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
452,51776654,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
453,42421317,"""In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.In their documentation page we have a pricing tablein which we can see there is a free plan in which you have some limits. In order to start using this I needed to join the free trial and set a billing account.My questions are the following:When the limits are reached, am I going to be billed by Google? Or the service will be unavailable until I accept to be billed for that?As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
454,40668684,"""I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text. Here is the code:This is the error I recieve:Does anyone know what the issue is here?Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
455,42294083,"""I am using Microsoft Face Api to verify two images.I have created the api key as the picture given below.There are two api keys generated.I am confused about these two keys , whether the quota limit of 30k per month is applicable for both the keys or individually for one key is 30k per month.And one more is there any validity period of the key.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
456,56012355,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
457,48999636,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",True,3,100,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
458,39071341,"""Currently I am working on Google face detectionfrom thisI am able to detect human complete face (eye, nose and other parts) and also as per this concept I have developed one application, if you stand in front of the front face camera then it will detect your face and show some gestures.Application video link which i have developed till now :-Application play store link:-Is there any API available where we can recognize human other body parts (Chest, hand, legs and other parts of the body), because as per the Google visionit's only able to detect face of the human not other parts.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
459,56050457,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
460,33447957,"""How to generate an Barcode and convert it to Bitmap using new Google Vision API?//Implement conversion    Bitmap barcodeImage = barcodeToBitmap(barcode);// I do know this part.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
461,51004854,"""Its a small query I just want to get those records which have event name = 'newevent'how to apply this filter as in documentaion all queries are of composite keys like thisbut in my case its not composite key .""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
462,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,True,1,100,True,1,100,True,1,100,False
463,47867995,"""I am using AWS, boto3 and Pycharm to write a very simple program on python that compare one faces from one face image to another face from another face image.I written the following simple source code:However I get the following error:(Obviously in the place of xxx and yyy I am using the real keys)What is the problem and how can I fix this?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
464,42146912,"""I am having an issue getting my image to upload to the microsoft face api.I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.It posts to the server, and returns the following in the command line:What do I need to manipulate so that it works with the base64 encoding? It was posting with an image url off the internet prior to the modifications.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
465,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",False,0,0,True,1,25,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
466,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
467,49801592,"""I am trying to convert andto java. In order to do this I need the byte array from the AWS Image. However, when I call themethod it returns null instead of returning. My code is as below:The input image is only 80KB in size, not sure if size matters.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
468,52448751,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""",True,2,100,True,1,50,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
469,52446033,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
470,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",True,1,17,False,0,0,True,6,100,False,0,0,False,0,0,True,1,17,False,0,0,False,0,0,False
471,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",True,1,25,False,0,0,False,0,0,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False
472,47109354,"""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
473,54212819,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
474,55714798,"""I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:I resulting file does load into a JSON object by usingI have triedbut I only getHow can I get this to work?  I am using Python 3.7.2My code is below:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
475,53952217,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""",False,0,0,False,0,0,False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False
476,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",True,4,100,True,2,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
477,43746016,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
478,53814812,"""I am impressed with accuracy levels of Google's Vision API for OCR.Are they using tesseract as part of their suite?.Can somebody guess what tools they might be using in order to improve the OCR operation?.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
479,51116095,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""",True,3,100,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
480,54677464,"""I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.QRScanner ClassInfo ClassCurrently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
481,51605428,"""I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.Do you have any solution for this problem? I try to change color and resize the image but it still not work.for example please look : [The API can detect only 'AMATA' but not 'S']""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
482,37306516,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
483,42117805,"""My target is to use Microsoft face API cognitive service to detect the faces in a frame , then using the landmarks returned for each face I would track it using optical flow for example!.My question is about the accuracy .. is this approach would work properly , or there are some other logical constrains exists behind tracking face using its landmarks?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
484,48428894,"""Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint? The purpose is to find the best match between a specific photo and the existing photos in the repository.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
485,38363182,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
486,51876363,"""I am trying with a pdf containing images as well with google vision API but it throws the following error :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
487,48400312,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
488,55994493,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
489,46814207,"""I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.I'm gettingas output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
490,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",True,1,33,True,3,100,True,2,67,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
491,43926563,"""I am using the following code:based on:Now I need to base64 encode it for posting to Google Cloud Vision API. How to do that?Or does the Google Cloud vision API work with image urls?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
492,54904596,"""At the moment I am trying to use Image Recognition using Google Cloud Vision API in R. It works until the authorization, but this is the error I got:Does someone knows what possibly goes wrong? In the cloud console I activated Google Vision API.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
493,48384087,"""I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.As I understand it, you would useto set the correct size.andI have set asand, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.I have read various solutions on StackOverflow, but most use theinstead of the.My code thus far is (part of):Can someone help me with setting preview size?The way I fixed itwith help of Alex Cohn's answer:And I set the size of thewith:If I remember I will update this to a non-hardcoded version.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
494,49340214,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
495,49463736,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
496,41863595,"""I have some code that calls into AWS's Rekognition service. Sometimes it throws this exception:I can't find theanywhere in the documentation or code, though, so I can't write a specific handler for when that occurs. Does anyone know what library module that exception lives in?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
497,51580768,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
498,51611001,"""Its been so much of time exploring the Google vision API, I am trying to get the Vision API Response in English Language only , below is my request object to API which has language hints :Even this request object not getting correct response(multiple languages) from Vision API  ..if there is any steps is there to get response in English only please let me know, as of now response contains multiple languages like below :""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
499,40349933,"""I use Google Cloud Vision Api (Text_Detection) it is working normal but when I return answer from the Google, message style like imageI want just one text e.g ""ACADEMIC PLANNER"" so how can I remove front of Academic ""null:"" and other words?Image e.gAnd here is my code;""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
500,55868948,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False
501,51008892,"""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
502,52017315,"""I'm new to Google Vision API Client LibI'm using Vision API Client Lib for PHP to detect text in images, this is my code:All I need is using response as an array for handling, but $result returns something likes array of object/object ( sorry because I don't know much of OOP/Object)Although i convert $result to array $res, but if I use foreach loopI get thisHow do we get value (text detected) in above response for using ?.""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
503,55022748,"""I am trying to use Google Cloud Vision API.I am using the REST API in this.POSTMy request isBut the response is always only ""name"" like below:My ""gs"" location is valid.When I write the wrong path in ""gcsSource"", 404 not found error is coming.Who knows why my response is weird?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
504,54315250,"""I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-tokenI already read this:--""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
505,44816006,"""I am working with Google cloud vision API with Python()But I could not understand why the annotation result of a single image consists ofofs.Thesays:Whyreturns multiple annotations for a single image?It seems unnecessary because detection results are contained in each attributes (,, etc.).And when I try the api with my own image it returns theof length 1.So my question is:Why python's vision api client returns multiple annotations for a single image?Do I need to parse everyin the list?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
506,51443537,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
507,44304400,"""I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.My codeThe response""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
508,32260893,"""Is there a way to detect sunglasses (glasses) using the new Face API from google vision?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
509,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",True,6,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
510,51223852,"""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
511,41254591,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""",True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
512,43584965,"""Whenever I request to GoogleVision api's, this error pops up. Even cannot install/uninstall any of the packageSample Output:[sudo] password for engineer: Traceback (most recent call last):ImportError: cannot import name JSONClient""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
513,34643033,"""under my IBM Bluemix account, I have registered a Watson Visual Recognition service.My intention is to call the service from Bizagi BPMS as REST service.Bizagi brings an ""unauthorized"" error.The URL for the REST Service isThe Service URL is(x and y are the credentials from the service instance in Bluemix.When entering the Service URL directly into the browser, I can enter the authentication credentials in a popup window, but the response is ""Error 404: SRVE0190E: File not found: / """"",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
514,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",False,0,0,True,1,25,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
515,39009764,"""I'm trying to use the Microsoft Emotion API.I can use the image version without any issues but when I try to use the video version I get an empty response.It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.Here is my code. Would appreciate any help!""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
516,40503587,"""I wish to use Google Cloud Vision API to generate features from images, that I will further use to train my SVM for emotion recognition problem. Please provide a detailed procedure for how to write a script in python that can use Google Cloud Vision API to generate features that I can directly feed into SVM.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
517,44166139,"""Hello I'm trying to test out the google cloud vision api for my android app.I enabled the api and created an OAuth 2.0 client ID and I'm using the sample code from google:and here is the json resonse:I am pretty sure the api key provided is the one I am using.What could I be doing wrong?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
518,54898476,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""",True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
519,54863800,"""I have this code for Google Vision API. I have Google credentials as a path and also as enviromental variable, butreturn nothing and error appears atHere is code:And here is console with emptyand error:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
520,47287749,"""I'm using Google Vision API, yet I've noticed that it is limited for the top 10 labels, and does not return results under 70% confidence.Is there a setting or a way to receive results that are lower than the 70% threshold?""",True,1,100,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
521,48456300,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
522,54816799,"""I am usingto link Rekognition results to a DynamoDB table. It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
523,44195115,"""I have one project that integrates with Google vision APIs.I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all. For my case I need to proof that every photos uploaded must not contain any glasses.This image, it seems that the Google Vision API can not detect wearing glasses at all.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
524,48514721,"""Edited:I installedGoogle Cloud Speechby using the followning command:when I runPython Programto convertAudiointoTextusingPython3, I get an error:But If I run the same program usingPython(Python2),I get the correct output.I want to run the program usingPython3. I am usingOpenSuse Tumblebeed.So, I tried the following commands to installgoogle-cloud:But I am getting the following error by all these commands:I thinkGCCis trying to findpython.hso the process is exiting with error code 1.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
525,49711924,"""I am trying to use the google vision api in a java project in eclipse. I've looked for tutorials and looked over the steps that google provide but I am still lost. Can someone explain the steps i need to take in order to import the api?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
526,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",True,5,100,True,2,40,False,0,0,False,0,0,True,2,40,True,1,20,False,0,0,False,0,0,False
527,52818392,"""I'm using the Python SDK snippet provided byI want to return face attributes, The docssuggest that addingTo the Base URl will return age and gender attributes. It's throwing me an error, am I missing something?This is my first time using Azure Face API.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
528,52809726,"""I would like to use Google Cloud Vision service in commercial purpose.But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.Briefly my question is 2 points below.Is there any license which should be followed in order to commercial use?the condition for commercial useIf there is someone who have answer about them,it would be highly appreciated that you could give me answers.""",False,0,0,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
529,54120224,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
530,44893985,"""I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or soIf a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead. This makes it impossible for me to know whether there is a new person in front of the camera, or a different personThis was not the case a couple of weeks ago, it's just something which has started happening recentlyThe parameters that I'm sending are...... the gender and age detection are working fine, it's just the faceId that I'm having problems withIs there a limit to how many faceIds it'll assign per month or something? I can't find any reference to a limit in the documentation""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
531,55739476,"""Currently the maximum preview resolution supported by Fotoapparat library for the frame processor is set to 1280x720 but Im using Google Cloud vision API and the recommended resolution can go up to 1600x1200 as mentioned inHow can I increase the maximum preview resolution to suit my needs for firing the appropriate detection request?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
532,56160026,"""I want to know the font size of the text used in an image. Do you have that way?I used Google vision, but it only return""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
533,53737023,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
534,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",True,3,100,True,2,67,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
535,43639575,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
536,49386572,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
537,51907473,"""I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image?If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.If there is a better approach than this, please let me know.I am using following code to index an image in a collection:And the following code to see if a face exists in a collection:This is the output of search_faces_by_image:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
538,51192145,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",True,2,100,True,1,50,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
539,55886635,"""I'm using the Google Cloud Vision API with Python 3, but i'm getting the error""Cannot find reference 'Image' in types.py"" when i use:I made the correct imports and the documentation tells me to use this function to get an image. Anyone can help me?Code:Error Message:Google Cloud Vision API version: 0.36.0""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
540,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False,0,0,False,0,0,False
541,35951874,"""I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js aloneThe node.js code I'm trying to run is to access a Watson visual recognition api:I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:in my index.html before connecting my javascript file.However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
542,49944780,"""I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error. I am not sure where the issue is. I make the request as suggested in the documentation .The error I get is:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
543,49425490,"""I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this? I tried to do with an ajax request and via browser too.Thanks in advance!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
544,55292198,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
545,41127382,"""I'm trying to make a call to the Amazon Rekognition service with NodeJS. The call is going through but I receive anerror in which it says:I'm basing my code off an S3 example:The documentation states that the service only accepts PNG or JPEG images but I can't figure out what is going on.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
546,53281993,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
547,40984635,"""I am trying to use google vision api to perform OCR on my images. The Json Output to the API call returns recognized words with bounding box information.Could someone please tell me how to use this bounding box information to do layout analysis of my image?If there is a library which takes this as input and returns sentences instead of words?For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line. Is there a library which can give me this information out of the box?Image used for OCR""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
548,48923406,"""I don't find a way to use a ktor application inside an AWS lambda...That is, instead of starting an embedded server or using an external server as described in, I just need to ""execute"" the pipeline.I suppose this is more or less like the TestEngine but I am not so familiar with the ktor framework to be sureNote :I have already found examples to run one kotlin function per lambda (the best tutorial IMHO being).The problem is I dont want to manage one lambda per function (I want one microservice per lambda, the microservice being responsible for multiple tightly coupled operations)""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
549,56256801,"""I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
550,47634218,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
551,50805898,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""",True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
552,51365884,"""When I start the stream processor it fails.I create the processor and I check (list stream processors), the initial status is STOPPED, then I execute the command to start stream processor, I check again and the status returned is FAILED.I'm using: - Raspberry Pi and Camera Pi;- AWS SDK for PHP 3.x ()More info:CREATE COLLECTION = OKCREATE STREAM PROCESSOR = OKSTART STREAM PROCESSOR = FAILEDPrint Screens folder:1 - Kinesis video stream:2 - ""streamA"" (1):3 - ""streamA"" (2):4 - ""dataA"":5 - Role ARN:6 - Stream Processor Failed:What am I doing wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False
553,36416503,"""I'm getting this error:And it may have something to do with the encoding of the images, not sure.  I am sending Google a bunch ofs.Note that this is similar to, but that one doesn't help a whole lot - there's no answer.Here's my JSON:This is the first image I send it.Any thoughts on what's causing this?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
554,45395459,"""i'm kind of new to the Azure Computer Vision API but i'm interested to use this for parsing a lot of mathematical documents.I wanted to usefor evaluating the output string from the Vision API but currently only text recocgnition is supported.Does somebody know if the API is usable for this kind of scenario in any way (or will be in the future)?If this is not a good use-case for  this kind of AI API what would you recommend to use for generating a usable string expression from handwritten mathematical documents ?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
555,42391009,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""",False,0,0,True,1,50,True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
556,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",True,7,100,True,1,14,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
557,46731393,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
558,45211033,"""How do i crop an image while rendering to  browser using the bounding box returned by aws rekognition indexFaces api? below is the bounding box example""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
559,52471113,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
560,51417691,"""How to darken the area around the field to be scanned?I would like the view as in the. I have a frame around, but I do not know how to dim the area around. I use google vision.[]""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
561,49664844,"""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
562,56043593,"""I am following this tutorial:However, when I run the commandI get the following error:I have no idea what's wrong. I replaced the single quote with double quotes and double quotes with escaped double quotes (\"") so it'd work but I got that error.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
563,38748027,"""I am trying to use Google Vision API and upload an image using their API to get analysis. I am using this php code:It always show ""you did not upload image"". And here's my Swift function where I upload the image:When I do, I get:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
564,53203939,"""I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
565,38288634,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
566,40526090,"""Got a problem with project oxford vision API. The example fromworks fine and recognise text on images. But my code throws exception:Class code:Did anyone have same problem and how can i solve it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
567,39478404,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
568,46790435,"""I m calling Microsoft Face API for detecting the face in an image. While loading an image from local , I always get an error as belowBut using the same image via URL , its working fine.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
569,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",True,2,100,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
570,54961012,"""I would appreciate some guidance on the following issue.Use Case:Create a collection with known faces.Search a stored video toidentify ""known"" faces & draw a bounding box around them in the videoframeSteps taken:I'm able to create a collection and index faces I'm able to analysethe stored video and get the results of PersonMatch and FaceMatchusing getFaceSearch()I'm able to draw the bounding boxes around Persons found in the video, etc., however...Issue:The response of getFaceSearch() returns an array of FaceMatches.However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video.I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame. Here is the API document that I'm referring to.Thanks for your help on this issue!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
571,47833305,"""I'm using Google Cloud Vision API to detect logos of brands or companies, when testing everything works correctly. However, in the application that I am developing I need to upload images or logos of brands that are not so popular (new companies, new logos or new brands e).Example: it is necessary for a new company to upload logos and images and remain in the google database to be able to scan or upload an image and theAPI REST  I get the name of the  newcompany.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
572,55917241,"""I want to get text from image using Amazon rekognition api.Here is my code:However, it shows an error:How to solve the problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
573,44762298,"""I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API. However, the output I am getting from theis only providing individual characters as an output, when I require words to be grouped together.Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or orfunction in Python?I am not looking for full text extraction as my .jpg files are not visually structured in a way that thefunction satisfies.Google's Sample Code:Sample output of the off the shelf sample provided by Google:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
574,46995404,"""I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line usingaws-cli/1.11.175. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:The documentation suggestsand I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.I tried things like this:Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should theoption look like?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
575,39407269,"""So I just want to detect text or labels from an image using the google cloud vision API. But When I run this code I always get:But I don't know why...here is the full json output what I get:My test code is here:So the question is.. what is wrong with this code?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
576,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",True,2,67,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
577,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
578,48706864,"""I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. Thisis about android app and it seems that for that case the answer is positive. Here is my code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
579,54113520,"""I have built an application with the following architecture :Client : Angular 7Back-end: Spring bootThe aim is to take a screenshot of the user and compare it with a picture in a database, using AWS Rekognition API.But I keep getting a CORS errorThe front make a POST calls to the back (a REST api built with Spring) and send an imageThe Back get the image and send it to AWSRekognition API (using AWS SDK) with the target image stored in a databaseThe AWS api send back the resultThe Back Spring api send back to the front-end the resultWhen I use Postman and keep the code which calls the AWS API: OK it works fineWhen I try on my web browser and KEEP the AWS API call code : I get a CORS errorHere is the Postman screenshot :Here is the Angular code : picture.components.tsHere is the request.service.tsIf someone can help me, that's would be super great,CheersAlexis""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
580,49518796,"""I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated.}""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
581,54824911,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
582,40472437,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
583,47906157,"""I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.My source code is taken from the Google Cloud tutorial for this API and it is the following:However I get the following error:This is weird because:1) I created a new service account2) I addedto my .bash_profile (I put the json file at the Pycharm file of this project)Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.How can I fix this bug and make the program running?By the way the problem is solved if I simply addto my source code.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
584,40037830,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""",False,0,0,True,2,100,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
585,56334124,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
586,54015378,"""I try to compare the two image which is in s3.So I have completed the code by referring to the following:I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).But it makes that error.com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)I thought if I make a mistake of IAM, the app can't upload the file.I try to upload the file with same credentialsProvider, upload success.I don't think that's what happened because of permission.S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2is there any information to get s3 object with Rekognition?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
587,43814654,"""I am trying to usein my Google App Engine Python application. Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
588,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",True,5,100,True,1,20,True,1,20,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
589,53039190,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False
590,42970980,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
591,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",True,3,100,False,0,0,True,1,33,False,0,0,True,2,67,True,2,67,False,0,0,False,0,0,False
592,44862532,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
593,52509581,"""I am trying to create an app which makes use ofAmazon Rekogitionin AWS for identification of a person and retrieving the personal information for an internal storage system.I wanted to know how to connect the Amazon Rekognition part and the information stored in the database. The face detection part will be done by Amazon Rekognition, but how will my app store and retrieve the personal information after detection of face?Can anyone give me a sample code in Java for retrieving the information stored inusing?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
594,42525482,"""I want to use Amazon Rekognition for some Detection project from India, each and every time I use to connect, either check for Region error pops up or S3 metadata issue?botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectLabels operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
595,50598790,"""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
596,36976312,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
597,53799577,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
598,55947906,"""I am using the amazon rekognition API to analyse my video to find and Search faces.A one minute video has been on processing for nearly an hour now. But have not received any result .Is this normal ?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
599,53109098,"""I was trying to create an ios app for text recognition with Google Vision text recognition.I had integrated all the required pods into my project as mentioned inIn order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class.Whenever I create an instance of that class and with a variable and try to access the properties of that class it there is aRed errorindication at the line where I try to access the properties with the instance variable of the class and aGRAY ERRORindication at the top of theViewController ClassThe ERROR MESSAGEGoogle ML DocumentationIn that documentation, they have used let and I also checked with that but every time same problem.Here is the code also:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
600,45126387,"""Is there any way of using Watsons image classifying abilities to extract information from an image of a document? Rather than simply classifying an image as a, b or c?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
601,39768788,"""Is there any API to compare two images and gives me similarity score?I have tried using IBM watson and Google vision services.But they are doing sophisticated stuffs of matching similar images across net. Is there any straight forward API where I can mention URL of two images and it gives me similarity score between them""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
602,50343162,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
603,53718791,"""I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this...""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
604,50868017,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
605,53249139,"""I'm trying to integrate my project withHere is the maven dependency for you to check the client version I'm trying to integrate with:The way the documentation offers to set the API authentication credentials is the following:I'm wondering if there is a way to set the credentials explicitly in code as that is more convenient than setting environment variables in each and every environment we are running our project on.As I know for a former client versionthat was possible doing the following:But for the new client API I was not able to find the way and documentation doesn't say anything in that regards.""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
606,51257124,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
607,41972937,"""I am using trail version of google vision API ,using the rest API i am trying to get the face_detection values from postman tool but i am facing an issue showed below.can anyone help me on this.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
608,48527517,"""I am studying Amazon Rekognition API. I would to like to know if it is possible to call Amazon Rekognition API via curl?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
609,48549064,"""I'd like to use the Google Cloud Vision API with PHP.Inside mydirectory I executed the following command line:So now insideI have adirectory which has the following files/directories:,,,,,,,,,,,I'd like now to use the Google Vision API in PHP, following this tutorial:But I'm getting:I tried to add ""vendor"" when I'm defining the namespace (as my Google files are under vendor)But it didn't help.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
610,53578143,"""I am doing some thing wrong over here, while comparing two images in different S3 Bucket.Even though, I am comparing images of male and female it would give 99% confidenceor am i missing something in the declaration yetMaybe This line is causing a problemOr my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below. What else do i need to correct so that it will return the confidence within the rang specified""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
611,51646954,"""I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 & next time I scan the document, the text is coming up in Block 10. There is no inconsistency.I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text?I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
612,47466195,"""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next. I am completely stuck right now.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
613,48551659,"""I'm using google vision API in one of my PHP script.Script works well when I'm executing it through the terminal:But when I want to execute it from my browser I'm getting an error 500:I don't get why the error message suggests me to use OAuth 2, I don't need my user to log to his google account.My code is the following:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
614,42914619,"""Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?Here is the package.json that I am using:and the error throws while loading the vision api module -  at the line mention below""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
615,44373968,"""I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right.How do I use vision.detectText() when there is no cloud storage bucket?I'm thinking I can/should ignore the reference to storage.bucket() indicated onI have:the console reports:I have tried using:but the error is:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
616,54406629,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
617,51691313,"""I need help with my Ionic typescript code.This uses a Google Cloud Vision API to tag photos that you upload to Firebase Storage.My problem: (returns the following error)My ts file:This index.ts file is located at AppName/functions/src/index.ts.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
618,45942150,"""Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches () within a particular website, rather than the entire web, as is the default behavior.I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"" (for example) in the search bar.Is this possible with the Google Cloud Vision API?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
619,51036159,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",True,2,100,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
620,44041039,"""I have an issue with detecting image whether it is painting image or real picture taken. I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that.Appreciate if you can share algorithm how to detect it.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
621,44449053,"""I want to create an application android that films on the road as long as it is open, and if it detects an accident it sends a request to a database, I directly thought to google vision, but unfortunately it paid and so, I found watson's vision, how i can use it for android studio""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
622,47647693,"""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py file.The error i'm getting is as followsThanks in advance!""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
623,50545515,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
624,55927749,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
625,53117283,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",False,0,0,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
626,48473858,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""",True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
627,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",True,10,100,True,2,20,True,1,10,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
628,48625509,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
629,42162320,"""I'm using Google Vision for face detection on Android. Currently my code:The problem is thatandare not correct and even negative sometimes. I know that to get correct coordinates it should be rotated somehow, but how exactly?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
630,48022812,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
631,55619304,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
632,46838135,"""When I load my application I get this:I am trying to follow this:I have run the command:Then on the Client Libary it is saying I have to set up a Client Library? I have done this with all he correct things then it says to-:And the execute this:Where I am stuck is, where do I execute this, how do I set the environment variable?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
633,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",True,3,100,True,1,33,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
634,55708104,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False
635,40808590,"""I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:However, I get the error message below. Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here? I was able to get the Google Vision API to work locally using the same Service Account.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
636,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",False,0,0,True,5,100,True,2,40,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
637,52579907,"""I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in,,andhierarchy.However, when convertingtotext andtotext, I need to understand the DetectedBreak property.I went through.But I did not understand few of the them.Can somebody explain what do the following Breaks mean? I only understoodand.EOL_SURE_SPACEHYPHENLINE_BREAKSPACESURE_SPACEUNKNOWNCan they be replaced by either a newline char or space ?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
638,45176829,"""Since google vision has someon input image size, I want to first resize input image and then use thefunction.Here's theirthey useto open the image file. I wonder in this way, how to resize the imagein memoryand then call?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
639,56301560,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
640,50219443,"""I want to extract emails from a text which I get from the business cards using Google Vision API. How can I do that using Natural Language API?(I'm using Python)""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
641,43094048,"""I am having a base64 encoded image.How I should proceed for text detection with google cloud vision python library?My Code looks like :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
642,52549743,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
643,54989726,"""Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
644,51863232,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
645,53341184,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
646,51880350,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
647,55223343,"""how to get a text based on its boundingPoly in Google Cloud Vision API?I run below to get its textsand get response, :I just want to get ""12345678990123456"",  is there any way to get it by passing its boundingPoly in Google Cloud Vision API?please helpmany thanks in advanceDon""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
648,54008514,"""I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image.I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data. The field separator(|) is being read as ISo,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image havingcomb fields?Or Is there an option to blur or remove the pipes in between the letters before reading the text?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
649,55744464,"""Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects? Like Coca Cola brands or what ever you would like to detect?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
650,44606352,"""By using Google Cloud Vision it detects face and returns only 34 facial landmark points.Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with?""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
651,54557026,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
652,56250899,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
653,41434746,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
654,54855549,"""I am trying to create an application that read an image containing a simple Javascript code -- ex: a-- and execute this code afterwards.For that, I am trying to use Google Vision API. My main problem so far is that some special characters (,,) are not being recognized correctly.Some examples:is converted tois converted toI saw this, and it seems that I can insert some special chars to be recognized, but I do not know where.Is there a way to improve the recognition for this context?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
655,48893088,"""I'm trying to detect text in a photo taken with the Camera, but with no luck.The code I'm using is:This produces an exception with the messageWhen not encoding the bytes in base64 - it yields weird outputs, where each text detected is separated by a comma, likeorWhat might be wrong in with my example?When using references to S3 Objects even with the same photo - everything works fine.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
656,38967064,"""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
657,53882629,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""",True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
658,48834479,"""Does Google Cloud Video intelligence work with embedded video content from Vimeo or YouTube. Will it be able to create tags, see faces etc... since the content is not directly uploaded?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
659,54418688,"""I would like to specify multiple modules to install by version number.If this is my:How can I adjust the contents of the mods list to specify the version number?Edit:The utilities.py file begins:The error I get after applying the advice from @jpeg, is as follows:My pip freeze is:The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
660,48749859,"""My users need to be able to authenticate themselves using a picture.So that when they create a account on the phone a picture is selected and saved.When they log in awithshould take a picture and compare it to the saved picture.I've found a possible duplicate, but this is very old and not really relevant anymore since the introduction of ARKit and Vision..I has to be done locally sois unfortunately not option, the same goes for&.The Vision Frameworks has aclass, that can detect faces, but I don't know how one would compare that to a saved one.""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
661,37796580,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
662,56346851,"""I'm getting an error while sending Image for recognition to AWS Rekognition. This is the code which I use:And this is an error:Exception looks likeis empty, I have debugged and checked that ByteBuffer is valid and is not empty""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
663,52512050,"""I want to detect text in PDF and TIFF files with Google Cloud Vision, but from the looks of it that can only be done if you first store the file to the Google Cloud Storage. Can this also be done without storing it in the cloud?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
664,54065638,"""I have a lot of documents in offline books. It's in table format and I don't want to manually input these tables into a dynamoDB table. Can I use AWS rekognition to help me here OR I should look at some other service ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
665,45134020,"""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However, I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
666,51894464,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
667,49014560,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
668,51142340,"""I'm using the Java Google Vision API to run through a batch of images to find the Web properties and that's been working fine the last couple months.I'm now getting a channel closed and ClosedChannelException error on the request. I created a new project with just theto test and I run into the same issue.I updated my Maven dependency version to the most current and still get the issue on the original application and the new test project.It errors on the request here:This is the error message I'm getting:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
669,56379854,"""I am querying the Azure Custom Vision V3.0 Training API (see) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:u'precision': 0.9859485, u'precisionStdDeviation': 0.0, u'recall': 0.3752228, u'recallStdDeviation': 0.0}The precision and recall uncertainties,andrespectively, always seem to be 0.0. Is this user error and if not are there any plans to activate these stats?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
670,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",False,0,0,True,1,33,True,3,100,True,3,100,True,2,67,False,0,0,True,2,67,False,0,0,False
671,48075707,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
672,50765893,"""I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.Any ideas? or people that experiences similar problems ?Regards""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
673,52322574,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
674,55323321,"""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
675,48721858,"""I've been accessing the Google Cloud Vision API from a script in a Google Sheet using UrlFetch and the REST API.Until I got ""UrlFetch failed because too much upload bandwidth was used"" I didn't even know there was a quota on UrlFetch!Is the a way to access Google Cloud APIs from a Google Apps Script so I can dodge the quota?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
676,46332861,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
677,41228649,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
678,34785457,"""I'm using Android StudioMy project needs import google vision apifollowingcom/google/vision/v1a1pha1how can i do to import java files on my project?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
679,42112519,"""I'm trying to disable multipledetection at time.How to disableusing, I couldn't find any solution from official siteI have downloaded sample fromCodeEven if remove below line, I cannot detect at all.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
680,54231123,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
681,55005298,"""im trying to upload a image file to aws s3 using the following code and it gives an following error sayingrequests.expectations.HTTPError:400 Client Error: BadRequest for url:the complete error is attached as an image =the problem is mainly in upload part is the im not able to figure out the file_namewill it be rfid tag which is 0001249950 or will it be the scan variable that holds the value of rfid or some thing else""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
682,51310239,"""Is there a way to filter the label results by category type when calling the Google Vision API?I have an iOS app that will take photos of food and want to only receive or read labels in category of food from the API.Thanks.""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
683,48056251,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
684,54118524,"""I'm making a request with the google vision api that appears to have worked, I get an operation number back. The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.This is the script I ranThis returns backand when I do a lookup on the operation I get thisHowever the output folder is completely empty and I am not sure what to make of the state created.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
685,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",True,8,100,True,3,38,True,5,62,False,0,0,False,0,0,True,1,12,False,0,0,False,0,0,False
686,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",False,0,0,False,0,0,False,0,0,False,0,0,True,3,100,False,0,0,True,3,100,False,0,0,False
687,49380672,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
688,47415721,"""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But, when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
689,54788081,"""I'm trying to use the google vision api facetracker sample ()to detect and track faces without showing the preview (at least detect, tracking is not necessary).The main problem is that if I delete the ""preview"" I don't receive the callback in ""onNewItem(int faceId, Face item)""  (function in FaceTrackerActivity in the github link provided).I've been lookin through SO and I didn't find anything similar to this question, also it's not necessary that I use ""google vision api"", if you know another system that can achieve this objective it's perfect.Thanks for reading.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
690,53029413,"""I'm reading this article :and reproduced the sample application. It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx by mine.We get the exception.Here all my project's configuration:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
691,44635222,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
692,36452038,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags.  But I can't get the hashtag symbol recognized.  Any ideas?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
693,43383886,"""in google vision api label detection, can't know where object located ? any options or idea ??I have tried in sample, and then response json is does not include object position!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
694,55644244,"""Please Note that I am a NEWBIE. Under Learning ProcessRight now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.NOTE:R.drawable.changing is where ""changing"" is my image name. Now i want to replace the image with the image i uploaded on ImageView.Any help would work.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
695,55603140,"""I have a collection of profile images from customers I need to be able to pass a selfie of the person and scan it across the collection of images and pull up the customer information.Need to do the following using AWS Rekognition -Create a collection - DoneAdd Images to the collection - Whats the REST API syntax for thisWhile adding the images to the collection also tag it with the customer name.Take a selfie portrait and search across the collection and return the tag information which matches.Im using Flutter as a platform hence there is no support for AWS SDK so will need to make REST API calls.However the AWS docs don't provide much information for REST support.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
696,54752242,"""this code always return 10 records, anyone knows how to set 100?Is for use the API of Google Cloud Vision Product Search.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
697,53519748,"""I have problem with my properties from my json response body.The reponse I get from the call is from Google Vision API.The property 'description' is undefined but will show up when logged in the console sometimes this will works.This is the error:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
698,46236523,"""i am using Amazon Rekognition when i upload an image to my s3 bucket , the api resonse i get isis there any api to search for similar product (eg. search: 'striped blue t shirts' when i upload a blue striped tshirt) among images in my bucket.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
699,43404118,"""Recently,I use google cloud vision api for detecting image label ,follow,I  set up the  credentials for my application to authenticate its identity to the service and obtain authorization to perform task,then follow the API Documentation ,write code like this:I get error :why?""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
700,51934306,"""We are just thinking whether to validate some input for google vision (OCR).We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.I can't find it the documentation. Does anyone know?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
701,50767594,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
702,42702426,"""I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.I've been experimenting with the API and using PHP with no success.I've been looking through all the tutorials and always get stuck on some point.This is the closest I've came so far in phpBut then I get this error.I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.Does anyone have any experience with the google vision API that can help me out? Preferably with the android part, help me get on my way or help me get started?Thanks ahead.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
703,49335721,"""I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.The code that performs the face detection itself is not responsible for the behavior. Simply havingin the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.Edit:doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:From the DatabaseConnector class:This is where data is inserted in the database inside the Activity class:And this is where the images are fetched from the database:Just for clarity, this is the OverFilter class:mFilter is just an ArrayList of OverFilter.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
704,44629662,"""I'm trying to do face detection in a video using Google Vision API. I'm using the following code:But I'm getting the error:The ""frames"" are getting read as numpy array. But don't know how to bypass them.Can anyone please help me?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
705,49877255,"""I am trying to extract text from such images but Google Vision API does not seem to recognise majority of the text, Can someone suggest a better alternative?Results from Google OCR""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
706,55334563,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
707,51034435,"""I want to use the Firebase with the Amazon Rekognition is it possible to use?I read Class for Rekognition for Node.js it has the S3 command in the code.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
708,41713056,"""I want to make live filter in camera like Snapchat app. This app based on.I have these following code in FaceGraphic.java:I create function to take a photo in:First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with idcapture.This is my main.xml:How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
709,43041575,"""I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this?Thanks to all""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
710,37508678,"""I've been testing an application example using Google Vision API.I have the code from their github, and created the relevant project and credentials file (something Google Cloud requests) and tried to run the code.I get the following:I've tried to add the relevant SSL certificates to my JRE's cacerts, I've restarted eclipse and an admin, but I still get this.EDITI've also tried to manually set the ssl properties as follows:Any ideas to resolve this issue, please?If there's anything I should provide, please let me know.Thanks in advance.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
711,48846121,"""So I'm trying to use Google's vision api, where it takes recognizes the labeles,facial, and text detections....etcBut it unfortunately, I can't fix an error that's causing us to fall behind .SOURCE CODEIt says ""Client"" object has no attribute, ""before_request""That's where I'm stuck, and I'm not sure what to do from there.ERROR""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
712,54536079,"""Getting the below-mentioned error on calling Google Vision API""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
713,37264402,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
714,45695542,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
715,54586779,"""I am creating an API which will take a text based image(Business card) as an input and return image with improved quality by remove noise and applying some image filter. For this I want to use AWS Rekognition or any othre php library.I have read AWS Rekognition documentation but I have not found any api to just improve image quality.There are various php Libraries are available but I can not find a proper solution for this problem.Please let me know a solution where a text based image quality can be improved either using php library or AWS Rekognition .Following is sample image of Business Card of which we want to improve.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
716,55283215,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False
717,54981232,"""I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial:The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciatedBest regards,Daniel""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
718,44961470,"""These series of videos use Google Compute Engine to access Google Cloud vision API.But, I only want to use Google cloud vision API on my local computer.I found Cloud SDK is an option but I don't understand how it works.Does it link my computer to Google Compute Engine?Then, do I still have to pay for the Engine?To Sum up the question.Is there any way that I only pay for Google Cloud Vision API without paying for Google Compute Engine?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
719,49842534,"""I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that? Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
720,56031856,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
721,49187806,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
722,52968434,"""I'm planning to useservice forforgot passwordin my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action.How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not.Please advice.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
723,50513107,"""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
724,54581027,"""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
725,40225148,"""I am developing an android application which requires use of google vision service provided by Google Cloud Platform.For authentication, this uses a classGoogleCredentialclass, the code for which can be found here:I need to setGOOGLE_APPLICATION_CREDENTIALSvariable as an environment variable pointing to a json file downloaded from another link, which is not important.The question is:When I set the environment variable usingIt gives me exception:in lineAll the code can be found in the link given above.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
726,51350903,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
727,50958350,"""For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.How can I get the raw json? The .NET SDK does not offer any methods. I tried to serialize the face details without success.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
728,50554306,"""according to AWS Rekognition Terms:How can I detect text in an image it has more than 50words?Give me a clue... thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
729,46594701,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",True,2,100,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
730,48085989,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,True,1,50,False,0,0,False
731,45024118,"""I am creating an android aar in which I am using google's vision API. To check if Play Services are available or not I have added check using.For excluding this from obfuscation I have addedI am getting this error when obfuscated:Update:When I am using playservices dependency in project where this aar is also imported, then my code is working perfectly.Is there any way, to avoid adding playservices dependency in project and just use it from aar?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
732,51676317,"""Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
733,36728347,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
734,53910973,"""I'm building a React native app with serverless framework using AWS services.I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.Everything works fine except rekognition services.When I call any rekognition services it shows.But it works when I call locally usingI attach all necessary permissions to mylikeHere is my codeindex.jsHow to solve this timeout error as it doesn't show any error on?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
735,47250652,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",True,2,100,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
736,55391510,"""so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage.I want to make app which take photo only face but i get whole image instead of face when i save into my storage file . so how to crop  face and save into storage with the help google vision face detection API.**so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
737,53515301,"""I'm using android studio and cloud vision in order to detect faces features in a picture. When compiling, I get this error (About cloud vision V 1.53):My Gradles dependencies are:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
738,44594617,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False
739,51273104,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
740,40714481,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
741,51961697,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
742,52928909,"""I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
743,46764797,"""I'm testing outand noticing it's replacing underscores with spaces.Does the API provide a feature to ensure these underscores are not omitted?Code example:Here is the example_image.png that I'm using:Which produces the output:(no underscore).""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
744,48623615,"""Task:Convert Google Vision API response to JSONProblem:The return value from the API call is not in a JSON formatPython FunctionGoogle online JSONGoogle Response (List)Type:Tried:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
745,46921518,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
746,51991109,"""In the above activity, I have to scan two different barcode and assigned their values to the each. But I am not able to do it. I have tried many logical approaches but none of them worked.activity_scan_qr.xmlScanQR.java""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
747,48806569,"""We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.To get actual idea I have attached image.In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.Note: We have tried two SDK""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
748,48140339,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False
749,51959287,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""",True,2,100,False,0,0,True,1,50,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
750,48521816,"""I want to integrateAmazon Rekognitionfor the Face Recognition.I have created bucket and IAM user. I am trying to hit""RekognitionService.ListCollections""for the testing in POSTMAN but getting error as follows;My request header is as follows;Can anyone please guide me how to test AWS apis in POSTMAN ?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
751,56155219,"""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
752,42761695,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
753,51803569,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""",False,0,0,False,0,0,True,3,100,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False
754,40893623,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
755,40949801,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,33,False,0,0,False
756,52877398,"""The problem I am facing is my application needs to support older devices ( at least API 14 ) so I am using google play service maps 7.3.0.However, I am unable to use newer play service APIs such as Google Vision because my google play service version is low.The problem is my users are unable to update their google play services app so I cannot simply upgrade my library.""",True,1,50,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
757,49590288,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
758,46718939,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
759,44411988,"""I'm trying to use google vision in google api, however I have the following problem:I'm trying to run the api's own sample code, below:I'm using the following dependencies:Has anyone ever had this problem?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
760,35121089,"""I am integrating Google vision API into my existing android application.  the app does recognises the QR codes but i need to implement the UI feature where the user is shown a graphic outline over the bar code .""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
761,53844994,"""I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).I had check below option.1.The palette library attempts to extract the following six color profiles.2. Get color of a particular pixelif I break bitmap in small size then find out color and save in list.Then there is time taken and OutOfMemoryError.Please suggest any library  to find color of images.EditPick from gallaryGet Color code from Pixel""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
762,55298114,"""I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
763,47368685,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
764,49114316,"""So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
765,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",False,0,0,False,0,0,True,6,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
766,45680183,"""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log (), how do I get this""id""received and use it in""Get"" to return the results?The API name is:""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
767,51918848,"""I have two AWS lambda functions which are processing all images uploaded to an S3 bucket(One is for creating thumbnail and another is for image moderation[rekognition]).While I am doing it, I found it invalid to add event notifications with overlapping prefix and suffix.For example, let's assume that I want to set the two event notification like the below.If it is not available I think there must be a kind of pattern commonly used in this case(for instance, making a proxy lambda to call the two lambdas passing the same event notification.)What is the best way to handle the case?""",True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
768,45372938,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
769,55929639,"""I've written node.js application that identifies and pulls a face out of an image using AWS Rekognition which gives me a rectangular bounding box:I want to tightly crop out the face itself so that I can merge it with a snazzy background like the might do in a theme park.Can anyone suggest a node package or web-service?thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
770,43598191,"""I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""Anyone knows how I could fix it?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
771,39616361,"""I am getting an error while querying Google Vision API:I have passed a pdf file which contains images and then extracted image usingto createlistAnd passed the list created above to vision service :I have also tried removing the base64 encoding of image bytearray, but still get the same error listed on the top.The bytearray length is ""774800""Is there something which I am missing because when I multipart an image to the servlet and pass the bytearray obtained from the inputstream it works fine.I am running the application on Tomcat V8dependecies used :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
772,42663690,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
773,46733224,"""I have uploaded images using php on S3, now i want to compare/match given image in my S3 collection, I googled but not getting answer, if it is possible to use AWS face rekognition using PHP to search in S3 collection.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
774,39982559,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
775,55940804,"""This document -recommends indexing 5 faces of a person straight-on.But indexFaces takes 1 image at a time.After indexing first image, when indexing the second image, how do I tell rekognition that it belongs to the same person?How do I tell rekognition that these 5 images belong to the same person?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
776,50815200,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""",True,2,100,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
777,52705012,"""I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
778,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",False,0,0,True,3,100,True,1,33,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False
779,42735068,"""I am using the google vision API and it can return the color scheme of a picture like this:The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.I am trying to create something like in the first picture. But I have no clue how to do that, so far I just have a listview that gives an overview shown like here.Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself? Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.Thank you!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
780,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",False,0,0,True,3,100,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
781,53102455,"""I try to use AWS recognition with Java SDK.I have the code below but it throws exception:I've create a new IAM with permissionsAmazonS3FullAccessandAmazonRekognitionFullAccess.When I run the execute method it throws:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
782,38580989,"""I would like to classify images by calling Watson Visual Recognition APIs.So, I set my end point as(a) End Point :"""";(b) Captured Request Message(c) Captured Response Message""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
783,47478044,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
784,55263570,"""I am trying to run a test on theto see how it fares to the client side.I am hoping toJSON with a base64 encoded image and get image text and barcodes returned.I have created aproject and API key per the tutorial at (), but am getting an 401 error when trying to make requests.The request is written in Polymer 2.x as follows:......How do I resolve this authentication error?It is an account admin issue? Improperly formatted code?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
785,44915072,"""I have an IOS app where users can upload images.I want to run all these images through Google's Vision API.Could someone please let me know how realistic this idea is?Let's say that I want to run 1000 images through their API.How much would this cost me?In the question title, I used the word scalable because I'm worried that using this service would be really expensive.This is mainly a question about how much it costs to get Google to scan each an image.Thanks!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
786,55408126,"""I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
787,55113529,"""Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong? Hoping to figure this out as it is a very interesting concept. I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.Thanks for the help!Im getting this error:Here is my code which I tried""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
788,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",True,2,100,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
789,50685445,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",True,1,100,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
790,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",True,2,100,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
791,51317429,"""i want to set CCL option in google vision api..but api document is not support this infomationI found that Google Image Search provides the following URL.tbs=sur:fcI wonder if this is also available in the GOOGLE VISION API. If possible, I want to know the URL that contains the method.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
792,54769503,"""I'm trying to analyse my images using(Azure Cognitive Service)But the issue is my Image is stored in Blob container withPrivate accesswhich means without a SAS token it will not able to access. So when I tried to call the Computer Vision API with myimage URL + SAS.It's givingbad requestYou can easily repro this issue intoo""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
793,45836819,"""I want to integrate the Amazon rekognition, compare face api in my ionic project. Can anyone suggest any sample or way to that.I am trying on simple JS but getting an authorization error.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
794,48262231,"""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After inclusion of these lines in the root-context its gives page not found.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
795,51563897,"""I'm trying to use Google Cloud Vision in a NodeJS app. Following the:Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
796,51744886,"""I try OCR image by Google Vision API:Result text return from API:Result text i get from website of google:From website of google :It can get correct value [], but from API , it only return []Why result of google vision api difference with ocr from website?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
797,46483447,"""I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
798,54133439,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
799,55063440,"""In my application, I am uploading image to get emotion response from Google Cloud Vision API. But while sending the captured image to cloud, I am getting error asErrorThis is the code i have written to send image to URL,i am not getting response from Google Cloud Vision API ,when i paste that url in web browser i am getting error as 404 file not found""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
800,56040881,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
801,49890108,"""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
802,38534147,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
803,50498421,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""",False,0,0,True,1,100,True,1,100,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
804,49067894,"""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
805,54574681,"""I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
806,47754119,"""Code :I'm trying to extract text from an image using the google vision api. I need to send a batch of images - things were working fine but now there is this error:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
807,50181484,"""I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.My bucket's region is us-east-1 and I have configured the same in my code.what am I doing wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
808,49915680,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
809,47920981,"""I'm trying to make an API call to Google Cloud Vision API using an API key from Golang. But I'm getting a.The apiKey/apiKeyOption part of the code below is mine.What is the right way to make this call?  Is it possible at all?...""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
810,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",True,1,50,True,2,100,True,1,50,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
811,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",False,0,0,False,0,0,True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False
812,50764331,"""I'm using the Google Cloud Vision API to detect landmarks, webEntities and other things from a given image (check the docs), I am specifically using the images:annotate endpoint, and I want to specify the language, I want the returned results to be in English.Is there a way I can achieve that?""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
813,55537358,"""I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.I am getting a bad image error using the b64content.  However, if I use the non base64 content, my call to the Vision API succeeds:Does blob.download_as_string() return a byte string that is already base64 encoded?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
814,41959043,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
815,53739849,"""AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name. Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False
816,34925092,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
817,46962077,"""how do I have to ""translate"" the following curl command into a valid php curl function?It seems that I'm doing something wrong and I can't figure out the problem:I tried several variations and the Watson Visual Recognition API error is now:before it was:Thank you for your help!""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
818,38480586,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
819,42041693,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
820,43631861,"""I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.The error says""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
821,50500341,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
822,51009080,"""I am getting data in this get_item . how can I get this data in scan query where EventName='newevent'  and 'RekognitionId': {'S': match['Face']['FaceId']""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
823,37742610,"""I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.In the main.js file in server folder I run:But that causes an error in the terminal:My site does not load so I run:But then I get an error I'n can't find anything about in the internet:I've been been trying to solve this issue for 2 days now, without luck. Any suggestions?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
824,53756545,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
825,50866887,"""I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.return base64 of image that has captured.Below is my code so far:Thanks in advance!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
826,46078769,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""",False,0,0,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
827,54644237,"""I have dumped response from google vision api for several images in text file. Now I need to do some manipulations, but response dumped is in string format.So, how can I convert str to AnnotateImageResponse or JSON in python?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
828,40450515,"""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
829,42657315,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""",True,3,100,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
830,55019870,"""I'm trying to detect handwritten dates using the Google Vision API. Do you know if it is possible to force it to detect dates (DD/MM/YYYY), or at least numbers only to increase reliablity?The function I use, takes an Image as np.array as input:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
831,44373059,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""",False,0,0,False,0,0,True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,True,1,50,False
832,50199393,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
833,51590523,"""The question has beenand the commercial solution from BlinkID is working well for me.I am trying to extend the application to recognise more type of document. For the moment I am using Google Cloud Vision with aalgorithm to (heuristic) detect the keyword (,,etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short () and not always visible etc.Having no background in computer vision, I'm looking for a way to improve the current solution or a better one.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
834,49648719,"""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl asandboth failed with error:I am using Windows 7 64""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
835,50273951,"""Can you please help with list of video formats and codecs that are supported by Google Cloud Video Intelligence API""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
836,52468352,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
837,52154222,"""I am trying to get the details of the image using google vision api.everything works fine,but when i use this line of codethen it doesn't find the image,but if i use like thisthen this works fine and it classifies the image.If you see i have also used the same method to plot the image in,and for plotting image this datapath works fine, then why it is not working for this""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
838,35519689,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
839,55086411,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
840,41440605,"""I have a problem when running the php script.I am detecting faces through google vision api.when running it in the terminal:I am seeing the output without having any problem.But when I am running it from the web browser, I am having this error:I tried to write in the terminal:and then reopen the webpage in the browser, I still have the same error.Any idea?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
841,36729360,"""Is there anyway I can analyse URL's using Google Cloud Vision. I know how to analyse images that I store locally, but I can't seem to analyse jpg's that exist on the internet:Is there anyway I can analyse a URL and get an answer as to whether there are any logo's in them?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
842,40671175,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
843,43069723,"""We are using Google Vision API to extract text from image. Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.Can someone explain why we are getting that error and how we can rectify it?""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
844,53827914,"""I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.I want to detect faces after every 1 second in the video that means run CF.face.detect once/second on video frame.Please tell how to do itThanks in advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
845,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",True,2,100,True,2,100,True,1,50,False,0,0,True,1,50,True,1,50,False,0,0,False,0,0,False
846,44025456,"""Is there any API for Google translate app like feature for website in which user upload an image and start selecting image textual area and he/she get the text?I found google vision API but its not just what i want. I want that image should be showing on website and user is selecting textual part of the image with mouse and user is getting the same text in some variable so that he/she can print the text wherever required.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
847,36120746,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""",True,1,50,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
848,54082512,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
849,45300613,"""I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient. As I try to initialize it, I get the error:I have tried everything but I can not find my error. Can you help me?Thank you!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
850,39712648,"""I am using google vision api to recognise  text from image. The image in Japanese language.But response is not in Japanese language it is in English. Can any body tell me how to change english to Japanese.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
851,51747822,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
852,44615959,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
853,52541577,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",False,0,0,True,2,100,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
854,52471647,"""currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?or maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.i have tried getboundingbox(), but i dont know what to do nextthis is my code for the text detector""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
855,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",True,2,100,True,2,100,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
856,51977903,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
857,43599556,"""Building a simple AWS Rekognition demo with React, usingGettingerror.GitHub repo:GitHub Issue:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
858,44981942,"""I want to use Microsoft Azure emotional API to analyse the local video, but how to upload a video to Microsoft service using python by sending a ""POST"" request with application/octet-stream content type together with the data read from a video file.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
859,42713068,"""I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
860,55301066,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
861,43748559,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
862,48375623,"""What is the syntax for calling the watson visual recognition api in python?I've looked around a lot but have not been able to find a proper syntax for the call of the api. What parameters have to be defined within the call of the api?Thanks for the help.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
863,54554676,"""Using Raspberry pi got the live video streaming using Kinesis Video Streaming Parser library and want to process stream to Kinesis Video Rekognition for detecting persons.Set the required details of ARN, got the video stream an set to Frame Viewer. Then trying to integrate Kinesis Video Stream with Rekognition.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
864,45696336,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
865,55251217,"""I have a directory with images, and I'd like to query the Google Vision API for each and store the aggregate output in one tibble.I tried what seemed like an easy solution: ifworks, then all I need is:Only to get:I found this answer, which involves writing a function from scratch, but that seems untidy and overkill, no?I also foundthat aimed to address this, but it did not get merged.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
866,47281901,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""",False,0,0,False,0,0,True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False
867,51926971,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False
868,35885801,"""I was trying to use Google vision API v1 by Alloy AppceleratorI create a request HTTPClient and call APIBut i have get response text from google :}And there is my code use HTTP request by AlloyThanks for your help""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
869,53543793,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""",False,0,0,False,0,0,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
870,51399511,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
871,55029634,"""So, I intend to perform a face to face verification using Azure Face Api. In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database. My question is: can store faces in a faceList in a way that I can use thepersistedFaceIdfor the faces coming from my database instead of performing a detect on the same picture just to get it's Id?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
872,48924950,"""I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library.I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.I used twoin thein:After switching tab or changing screen orientation everything works fine but before the first tab stays black.I also found the following error in the logfile which i don't know how i can resolve this.What can i do that the view doesn't stay black and shows a working cameraview?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
873,52684763,"""I found this project on GitHub that is an application with the Google Cloud Vision API:.I've edited a few things to avoid compatibility errors, and now it perfectly works.There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.This is the entire code of MainActibity.java:.Go to the line 169, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".Is there a way to change the feature type?""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False
874,52848759,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
875,54236946,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
876,52252580,"""I'm fairly new to AWS and for the past week, been following all the helpful documentation on the site.I am currently stuck on bring unable to pull the External Image Id data from a Reko collection after a 'search face by image', I just need to be able to put that data into a variable or to print it, does anybody know how I could do that?Basically, this is my code:ifname== ""main"":at the end of it, I receive:What I need is the External Image Id instead of the FaceId.Thanks!""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
877,51712998,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",True,1,50,False,0,0,True,1,50,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False
878,51117724,"""When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error.These are my routes (The uploadresume/post was having this error.):These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server.This is the view where I have my form for uploading the file:This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
879,54794136,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False
880,46289477,"""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False
881,48135978,"""I am a programmer with minimal understanding of. I have a large set ofimages from which I need to extract text through. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size.I have attached a sample image.This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.I request someexperts to help me out.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
882,40020225,"""This question has been asked before () but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.So I would need to extract some text from a photo to perform some analysis on it, for an Android App. I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...Do you know if there is any API that would allow this kind of feature for standalone apps ?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
883,37935601,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""",True,1,33,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
884,47176260,"""I am building an face recognition php application with web camara Using Amazon Rekognition API.i did basic face matches using the API from below documents.Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches. I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.Is there any way to overcome this issue? i want to detect whether the captured image was captured real or from captured from another image?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
885,51354969,"""I need a count of all thecarsincluded in a image with the Google Cloud Vision API in Python. I take only the labels of the image right now.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
886,53671813,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
887,46848590,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
888,50237770,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
889,44517510,"""I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
890,51493545,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
891,45981451,"""I got a problem when using the Google Vision API.I'm looping the process, to analyze several pictures, but when I print the results, all is coming in a block after 5 minutes of process, but I wanted to know if it's possible to start the program and make it print the results after each picture analyzis ?Here's my code for the algorithm :I'm looping this with a foreach(Image file in Directory).Any ideas to help me ?Thanks a lot !""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
892,50657314,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
893,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False
894,39029331,"""i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will workin atleast oneof these languages. (the webpage is being hosted at google appengine)what i found:1:google cloud vision API , i also found their project on githubthe code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)2:i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all.3:i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ? can someone help me ?thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
895,37985715,"""I want to useto generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ?EDIT 1Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.I must have overlooked something i guess?Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
896,50580902,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
897,56068100,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
898,49181313,"""***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********could you please help with an issue I am having?I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
899,44740363,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
900,49868678,"""I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
901,44845273,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""",True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
902,56175881,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
903,52046473,"""The aws docs for this are really confusing. Following the steps from here, I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
904,54587451,"""It would be very much help full if someone might walk me through this, i Have tried going through event structure but was not that helpful !I am still not abel to understand Many of u attended the question its grate full, but yet im not able to understand you, I'am completly new to aws.I went through the event structure, it says you will find the configuration id over here  ""configurationId"":""ID found in the bucket notification configuration"" but i could notand I'am still clue less about-----------------------event------------------error----------------------------xxxxxxxxx------------------------""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
905,56032884,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
906,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",True,5,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
907,45654972,"""I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).The problem is that I have to train at least 700 images but I have just something like 30.I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games. Do you know a generator like this? Or do you have a good idea for me?Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
908,43758896,"""I want to useusing Postman or any other rest api tool.So as per documentation I convert my image to base64-encoded-image-content using Java 8 encoding as below :Now in postman I choose Post option and then use the URL asAnd then in the body part I choose raw option and paste the below contentBut I am getting an error :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
909,56099062,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",True,2,100,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
910,35651277,"""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
911,50490782,"""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler. To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get. For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
912,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",False,0,0,False,0,0,True,5,100,False,0,0,True,1,20,False,0,0,False,0,0,False,0,0,False
913,48690047,"""I am testing Google Cloud Vision with Java in Eclipse.I have copied the java code fromWhere can I download the .JAR's from so that everything compiles properly?Google themselves say (see the link above)However, I am not using Maven (nor Gradle or SBT which are their other suggestions).So I thought to open a New Maven project in Eclipse, which would then download all the JAR's automatically, and then copy them across to my project.So I did ""new Maven Project"" in Eclipse and then when it said ""enter a group id for the artifact"" I entered the details from Google as I pasted above, but it did not download anything.Any ideas how I can get the JARs so that the code will compile?The pom.xml file which was auto-generated by Eclipse is""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
914,50907959,"""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly: null""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
915,55037756,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
916,47946770,"""I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.I import the Google Cloud vision client library as followand then I callHowever I get an errorTypeError: vision.detectText is not a functionInitially I usedfrom this examplebut I got the exact same error. I then read that textDetection has been replaced by detectText but no more successThanks in advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
917,54916175,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
918,51767916,"""I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none"". But when I try the same image with Google the vision API it gives me a good response.Is it possible to integrate tika with Google vision API? If yes then how using python?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
919,48868820,"""I'm using Rekognition for face authentication.When I register a user I havetheir user idmultiple photos of that userHow can I associate/label all those photos with that id when indexing/adding their faces to a collection?When I search by face in a collection I want to be able to get back their id.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
920,53954761,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
921,53736963,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.When I trigger the API from my local machine - I get a good responseWhen I trigger the API from AWS console      - I get failed responseProblemAfter much digging, I found that the 'boto3' (AWS python library) is of version:1.9.62   - on my local machine1.8.9    - on AWS consoleQuestionCan I upgrade the 'boto3' library version on the AWS lambda console ?? If so, how ?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
922,42245229,"""i am working on parse server with AWS , i am using microsoft face API  '' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger"" when i test the code there for the Parse.Cloud.httpRequest() all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text) not as JSON object but as char array , that means i get array of thousands chars.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
923,39865311,"""I'm trying to use the Microsoft Face Api and I have stuck with this code for several hours now:What am I doing wrong here?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
924,46630157,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
925,51189901,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
926,49890225,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""",True,1,50,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
927,56052040,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
928,38890861,"""I do have a Barcode Decoding Applcation :I have Used Zbar library and Google Vision API for ScanningNow what i want is while scanning the Barcode if user taps on a button at appbar for turning on Torch (Flash) then it should be turn on and off vice-versa.But the Problem is the camera is already on with its all parameters so when user taps the button to turn on torch we need to interrupt the ongoing Camera parameters and I don't want to do that,I am searching the other way to get torch on without changing the existing Camera Parameters..Below is The Camera Activity of ZBar And Google Vision this both use some other Camera Classes for Camera Preview.And GoogleScanner""",False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
929,42229158,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
930,50642047,"""I am calling the google vision OCR api from a spring boot maven project to extract text from an image.The call on the line below(i.e below line 80) does not return a response.It gets stuck for a very long time after which it throws the below exception.The console has the below logs printed. Kindly help me  resolve the issue.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
931,51811837,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False
932,51959009,"""I'm getting below error when I execute Rekognition using Image or Video (s3 source).My as code Below.When I'm uploading into S3, Getting an object from S3 and creating a collection in Rekognition All Its works fine but I can't execute searchFacesByImage (Source: S3) and startFaceSearch in Laravel PHP.Also tired after bucket policy setup in S3 like below.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
933,45812258,"""I'd like to know if is there a possibility to change the format of result returned bi Watson Visual Recognition API.For example:Instead of having this:Get this:So the sum of classes results would be 1 (100%)""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
934,54017318,"""Currently I am usingto match photo during login (capturing using webcam while login and compared with already uploaded photo).Issue I am facing is people can use photos from mobile or some photos to share account.Is there any full proof way to verify face?Thanks,""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
935,52048829,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
936,43129395,"""I have gone through all the documentation provided for the running Django Application on app engine.I have a Django Application where I am using Vision and Storage clients and my app name is pvd.I have been constantly getting below errors in error logs.Below is my app.yamlBelow is my requirement.txtFor deploying I am using:What am I doing wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
937,47104465,"""Hello guys i try to implement a registration functionality using asp.net identity.One of my required properties is the new user's photo  which  i pass as an argument in a  action method where finally  i callmethod of azure face api.I grab the photo using ain my view and then in account controller of identity in register action method i use ato read it.The problem is thatneeds as first argument a stream object or a string(imagePath) but in my case i cant figure it out how i can give a stream object or the image pathdoesn't retrieve the image path but to get the image i need this type of object.And then even if i try to passas argument i get null by break pointing thecommand lineTo be specificAccount Controllerand myFaceRecognitionControllerAs you can see i just check if validation api find any face and just return true to complete the registrationany thoughts on how i can overcome this??""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
938,56362468,"""I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:This is the part of the image with the issue:As you can see, there are overlapping rectangles on the 'M' and the 'a'.The result is something like this:   '''MMaay 10, 2019'''Why this is happening?Is there a way to fix it?I tried to change the image format from bmp to png. The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
939,54513958,"""I am trying to use the Microsoft Custom Vision. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
940,43578360,"""I'm planning to use Google Vision for document recognition.For example, I will upload driver license and I should get all text data and verify that it is driver license and not the cover of a magazine.The question is: does Google Vision has API for deletion of uploaded images?Does Google Vision fit my case if I have some security requirements?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
941,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",False,0,0,True,6,100,True,1,17,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
942,51133277,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
943,50716375,"""I use google vision api to detect the ocr.However, I got no ocr text.On the other hand, I uploaded the image toand got.Is it normal?How should I correct my code?Thank you very much.Here is my code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
944,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,True,1,50,False
945,45245748,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
946,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",True,4,100,True,3,75,True,4,100,False,0,0,True,1,25,False,0,0,False,0,0,False,0,0,False
947,50358189,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
948,47182750,"""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst.""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
949,47157038,"""I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error.""",True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
950,45341208,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
951,56008341,"""How do I decide when to use Amazon Textract vs Amazon Rekognition'smethod?My usecase is click picture from mobile and convert image data into text and store into AWS RDS.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
952,51216224,"""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally? As a third party or globally? How will it work on Google when I deploy?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
953,49450500,"""This is my first question here so I'll try to be as relevant as possible.I am interested in using Cloud Vision to process some documents, as I need OCR capabilities. I also happen to need bar code reading, which I currently have implemented using ZXing.I stumbled upon the BARCODE blocktype in the OCR () but I did not manage to produce such a block, even with an image containingonlya bar code.Hence the question: is the feature implemented, and if so, how can we get it to work ? Thank you for your time !Note:I have seen those related questions:But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.Edit:I have tried for example with:I also tried with""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
954,56010701,"""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
955,37741299,"""Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
956,50415121,"""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes.""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
957,44258407,"""Following error message is shown when I implement these codes...Console Messsage: AmazonWebServiceClient: {cognito-identity, us-west-2} was not found in region metadata, trying to construct an endpoint using the standard pattern for this region: 'cognito-identity.us-west-2.amazonaws.com'.CognitoCachingCredentialsProvider: Loading credentials from SharedPreferencesCognitoCachingCredentialsProvider: No valid credentials found in SharedPreferencesError Message:""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
958,41167490,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errorsI found a similar question asked(except for the change in the third question) but no answers.I tried deleting thefile to fix the last error, but no luck""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
959,54367776,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
960,44362759,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
961,51209404,"""I already integrated the AWSRekognitionthem into my project. and I make sure I'm  connected to AWS, by this codeand the respnse isbut when I write this line it give me an error""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
962,41560252,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
963,42992782,"""i am currently working on a project which will use the device's camera function to take a photo of a ingredient list and then it will display text in the image.I have used google cloud vision API to display the text.However i want to be able to highlight the text that was displayed on the image itself.Something like the google translate application where the user takes a picture and the screen display a box to let the user to draw out which of the words they wanted to translate.May i know which API do i need to use in order to create this function?As requested: This is what i had done so far, this is to display the text in images to a console.log()""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
964,49932603,"""I am trying to use the @google-cloud/vision package to send requests to the Google Vision API. Is there a way to do multiple detections without doing something like:Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
965,49665196,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
966,56341410,"""I'm trying to run the Google Api Vision sample code but I'm getting this error:These are the dependencies that imported into my project.Code that I'm using. Which is provided google Vision API from:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
967,49842698,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
968,47542341,"""Relevant Code:However,When I run this, I get an error:How do I get my image to be the right format for detect_text? The documentation says it has to be base64 encoding.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
969,41488436,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""",True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
970,54410031,"""I am followingto use Google Vision API, but even configuring the authentication credentials I get the following error:My code in Visual Studio 2017:What can I do to fix this? Do I have to create a trial account to use the Google Cloud API?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
971,53417035,"""When I read an image with text, Google Vision inserts line breaks in the middle of the sentence. How can I do to avoid this. Here's an example of the image text and Google Vision return:Text in the image:Google Vision Return:Thank you,""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
972,45427109,"""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
973,55072291,"""I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.I tried some generic OCR solutions such as:Amazon RekognitionGoogle VisionMicrosoft Computer VisionTeserract3.0 / 4.0 (experimental)None of these provide accurate (sometimes not at all MRZ recognition)I also tried some other tools specialized in MRZ OCR:BlinkIDfrom MicroBlink (which is very good but doesn't have a cloud solution)Accurascan(provides cloud solution but less accurate than BlinkID)Abbyy(too slow, 10~ seconds per request)Can you recommend me a good cloud solution for MRZ OCR of documents?""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
974,55127193,"""I'm trying to use volley to call Azure Computer Vision REST API using POST request to upload image to be analysed.Here's the API documentation:The input is passed within the POST body, and I wanted to send the raw image binary using.I convert the image to raw binary usingandI have no success so far in sending the raw binary image data, it's giving error 400.Here's the code:There are only few resources in the internet about Volley POST request using, so I am a little bit lost here...Any help would be appreciated!""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
975,51294008,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""",False,0,0,False,0,0,True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
976,45518029,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",False,0,0,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
977,38227082,"""Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.Personal experience with Italian, French, English, Chinese, Spanish works. What about the ones like Hindi, Urdu etc?Thanks and appreciate your help!Suman""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
978,44419153,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""",True,1,50,False,0,0,True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
979,47563346,"""i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.so can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images? here is a simple code of how to detect faces""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
980,47439799,"""I am trying to enter to the API from Microsoft FACE API. I created an account on Azure and created the service that they provide me the keys.The point is that I am trying to get access  to the API and it's all time a 401 errorAll time returns the same error:There is no almost documentation for the API in IOS and then it's just objective-c, no swift. Can someone figure out why is this returning all time the 401 error???Edit:As well I tried  let client =but this one returns all time 404 error, resource was not found.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
981,51588864,"""According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
982,51972479,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
983,45742530,"""I am working with Android, and have created a standard barcode reader, but I want to detect what type/format of barcode is read i.e.,,.... etc.Is there anyway to return the type?Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
984,56217832,"""I am using Google Vision API for ""TEXT_DETECTION"". Input for it is png image.(scanned document).When I am running this API , its causing numbering issue.Numbers for first two paragraphs are not visible.Tried with same code.It should extract exact numbering from original scanned document.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
985,35660357,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",True,1,50,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
986,54733517,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
987,49717845,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
988,45792942,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""",False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
989,44225909,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
990,45481935,"""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
991,53493720,"""I am usingfor a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
992,49296976,"""I cannot find my build.gradle project file to add dependency only my android studio contains 1 module build.gradle but cannot find the other one to add dependency.i need to add the google vision dependency in my project but i am not getting where to write the dependency codelikecan please anyone help me out regarding this problem""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
993,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",True,3,100,True,2,67,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
994,54224197,"""how can I add languageHints to my google cloud vision python code.FromI know that it is supported but I do not know how to implement it into the code.I think I have to do it like this:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
995,52103546,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
996,48683242,"""ClassCastException while parsing Google Vision API ResponseREQUESTRESPONSEParsing code snippet:Exception at line :  for (AnnotateImageResponse res : responses) {Exception in thread ""main"" java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to com.google.api.services.vision.v1.model.AnnotateImageResponseDependency : google-api-services-vision-v1-rev370-1.23.0.jarHow to handle this ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
997,54881537,"""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ? Using,""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
998,51219570,"""Google cloud vision full document detection outputs words with no space in between.Appreciate the help.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
999,42842441,"""I have images of receipts and I want to store the text in the images separately. Is it possible to detect text from images using Amazon Rekognition?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1000,54803618,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",False,0,0,True,1,50,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False
1001,56143548,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",True,2,100,True,1,50,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1002,42123633,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1003,41400421,"""I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1004,48286385,"""I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face:The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1005,52335053,"""On the Microsoft Custom Vision Portal I only have the Precision and Recall results, how can I generate the  True positive (TP), False positive (FP), True negative (TN)  and False negative (FN) from the same images that the service used to corss validate itself to then get the accurancy?tks""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1006,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",True,1,33,True,1,33,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1007,44792573,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
1008,51711390,"""I want to avoid all the spoofing and other violence related to face recognition security application. which provider would you prefer to use from Amazon Rekognition or Azure Cognitive Face?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1009,40493285,"""Is there a way to get Watson Visual Recognition to return the location of the classified content (car, tree, etc.) when using image classification? This capability exists in the face recognition service and would be invaluable in general image classification.The currenthas no information on this topic.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1010,56219425,"""The google cloud vision api works fine on one pdfbut returns absolutely nothing on the other pdf. I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.I'm using the code given in their examples section by uploading these files in a google cloud bucket.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1011,37984641,"""I have the following IBM Watson Visual Recognition Python SDK for creating a simple classifier:The response with the new classifier ID and its status is as follows:The training status shows failed.print(json.dumps(visual_recognition.list_classifiers(), indent=4))What is the cause of this?""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1012,19902877,"""I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:This is meant to send filejack.jpgup to the API. And afterwards printJSONresponse. Here is the documentation of the API to identify the face.Below is my code, I'm using Python 2.7.4I can see that everything looks fine, but my console gets this output:What is wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1013,49543773,"""Is there an API to see how many calls you've made this billing cycle to the Google Cloud Vision API?I would like to add this information to a UI so user's know when they're about to make queries that will be charged.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1014,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,True,2,67,False,0,0,False,0,0,False
1015,41186458,"""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful.""",False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1016,49955157,"""I am trying to print out results from an Amazon Rekognition call, but it returns the error:I put the index[0], I don't really see why it will happen to out of index range.Can any one help please?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1017,46966332,"""Im going to use Google Cloud Vision API.In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.So the scheme looks like this:Phone photo(Local Storage)--download--> GC Storage --get link-->Send request with this link to GC Vision API --get JSON--> work with JSONSo the question is.What for I need to storage image in cloud? Only for a link? Can I send the image direct to the Vision API without GC Storage?So the scheme:Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1018,49762291,"""I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:an example of the code being used is:The last line throws the error. However, if I submit:There is no error and faces are matched. Does anybody have ideas as to the reason for this error?A full trace for this error:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1019,51320588,"""I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1020,52143046,"""I am really new to python and i am using Google vision API to detect some text from image,In the API output there two thing which are of interest to me text and text.vertices both of them i have stored in two variables p & c  however i dont know how to create a single dictionary which i can then use to do some post processing(sorting,dicing etc.) on that data below is code i am using, could anyone please suggest any solutionsHowever this creates seperate lines for each c & p i want everyting into a single dictionary so that i then sort it in ascending/descending""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1021,50907956,"""Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1022,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",False,0,0,True,1,17,True,6,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1023,36540684,"""I want to implement google-cloud-vision API for OCR on my Project.But due to compliance issues, I need to know does google-cloud-vision stores the uploaded image? if yes what is the privacy policy for that?Does anyone have any information regarding this?Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1024,52825897,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""",False,0,0,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1025,54656996,"""I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?Note: There is no problem with the database because data gets retrieved when passing a string as childThis app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfoWORKING(When String is passed as a child)  class name=AnimalInfoQRScanner class: gets QR code and pass it to AnimalInfo class using intentI want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1026,55164321,"""I am trying to run the code in. I have run the following terminal command correctly:However I am getting the following credential errors:""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1027,48672879,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1028,53475106,"""I want to try face masking likeSnapchatfeature on my Android app. I've tried this face masking using Google vision, but I can't find how to record this face masking.Is there any solution to record face masking?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1029,53457755,"""Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image. But when i try it gives me result in matching them with english:Request code is as below:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1030,55997760,"""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1031,54813301,"""So,I take an image from a canvas using p5js and i want to send it to the Azure Custom Vision Service(the code bellow).Is p5 image even the same as the normal js image(like when you take a capture from a video) ?My problem is when i send a form as a json like :But it gaves me ""401 Acces Denied"".Is it even possible to send an image created using p5 via http request?""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1032,53643788,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1033,36289389,"""I am having trouble understanding the concept of  API discovery  as used in Google products/services.  Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1034,55568798,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1035,52477690,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1036,42227768,"""i am using IBM watson visual recognition api when i add an imahe to collection but i receive follwoing error all the timehere is my code""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1037,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",True,2,100,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False
1038,51025861,"""Right now, my AWS account has the following policies:AmazonEC2FullAccessAmazonSQSFullAccessAmazonS3FullAccessAmazonAPIGatewayInvokeFullAccessCloudWatchFullAccessAmazonKinesisFullAccessAmazonRekognitionFullAccessAmazonKinesisVideoStreamsFullAccessAmazonKinesisFirehoseFullAccessAmazonSNSFullAccessIn order to setup an  application load balancer  with auto scaling group, target group, subnets in a VPC, what are the other policies that I would require?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1039,45306016,"""I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.specifically this code where the batch client.batchAnnotateImages occurs:I am being presented with the following Stack Trace / Error:Here are my POM Dependencies :I have tried excluding guava and including multiple versions of the API.The code shown is the sample code from the google vision client implementation.any ideas ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1040,39252746,"""I am trying to use the Watson Visual Recognition API as an OCR component, however while it is doing a good job on the computerized text, I want to expand it more to recognize ""Nicely-handwritten"" text.Is it possible to use the custom classifiers to train the API? and if yes and someone did try it already, is it effective?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1041,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",True,1,25,True,1,25,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1042,44203114,"""In azure face api we create a person and add face to person and we can identify same person.My question is  when we add face to person does azure upload and save image on azure or just it stores mathematical attributes of person'a face.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1043,45505722,"""I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1044,55920132,"""I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)AWS HTTP error: Client error: POSTresulted in a 400 Bad Request response:{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.) - {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'I am using the following PHP script.I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.I am not sure whether it is the right way of solving this problem.Please Help""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1045,36602892,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""",False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False
1046,55296808,"""I have this image.I am using Azure Computer Vision API - v2.0,combination of Recognize Text API(POSt) and Get Recognize Text Operation Result(GET) as mentioned in. to detect text characters in the image.Currently it is able to detect all the characters except letter I and II.Can someone help?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1047,41766196,"""Face Tracker app based on. By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.This is the code for CameraSourcePreview that google vision provide:I call camera source with this method:Face Tracker front camera still too dark compare with default phone camera app.How to brighten front camera in face tracker google vision? Is it related with surface view?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1048,38753678,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1049,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",True,2,100,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1050,54640539,"""I am trying to send an image file to Google Vision just after I saved the file using a promisifiedbut I am getting an error saying that the file or directory is not found.I have added a function to get the file size to confim that the file exists before I send the request to Vision and the file is already created.I've also called a pathname to an existing file and Google Vision just worked fine.So I guess that it's just a matter of setting a time between saving the file and sending request to Google Vision.Here's my code:Here's what I get in console:How to fix this? Should I just add a setTimeout? or is there any other better solution to overcome this error?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1051,50265754,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",True,1,33,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1052,49168443,"""I ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url. The reference from the API () says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".I ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).It works fine if I use the url method, but I need to upload the image.This is the code I ve been using:My Bitmap works fine by the way.This is the error that the logcat gives me:So, finally, what do I have to do to send the proper image format that the api requires?Thank you in advance.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1053,35740396,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1054,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",True,4,40,False,0,0,True,10,100,False,0,0,False,0,0,True,1,10,False,0,0,False,0,0,False
1055,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",False,0,0,True,3,100,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1056,38804790,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1057,52393200,"""i am trying to create a app which makes use of amazon rekogition in aws for identification of a person and retrieving the personal information for an internal storage system.i wanted to know how to connect the amazon rekognition partand the information stored in the database.The face detection  part will be done by amazon rekognition but how will store and retrieve the personal information after detection of face""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1058,51045843,"""I am following the tutorial to extract text from images at:But I do not wish to translate the text, I wish to detect and save the text.The tutorial implements 3 functions:I just wish to detect text and save the text but I could not remove the translation portion of the code below:After that, I just wish to save the detectec text to a file, as the code below shows:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1059,41872763,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1060,49470158,"""I am following a tutorial () that utilizes AWS rekognition on a raspberry pi to make a robot. I am having trouble figuring out how to create logs to see where the code is hanging up.Here is the lambda function I need to debug:This lambda is called upon by the bot in AWS. Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.Here is the logs in AWS console:Please and Thank You for any help,David""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1061,47324768,"""I have been searching for a while, but have not found any information on whether there are limits on calls to Amazon Rekognition service.Does anyone know the numbers, or any source where I can look?What I want to know is if they limit the number of calls allowed per minute or second. I'm looking for information on the the paid (not free) service tier.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1062,50700241,"""It's possible to read Identity Cards information like name, address birthDate using Google Vision API?In the documentation, I fount something but I don't know how to use it.I checked also the google samples (), but I didn't find anything related to Identity Cards scanning.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1063,50741636,"""I'm trying to send pictures to the aws rekognition from my webcam to detect the activity of the person sitting in front of it using python.To do so I take a picture every 5 seconds and I send it to the aws.But when I do so it seems that he's always sending back information about the first frame that I sentHere is the result i get from that call:Knowing during the time of the call the image has changed a lot and should (at least I think) show me other results.I'm new to aws so I might be missing a point also it's my first post on stackoverflow so I might have forget to write something""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
1064,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",True,4,100,False,0,0,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1065,56399486,"""When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means.I've only seen this happen consistently for one image (so far).Example of a normal WebEntity which has description, entityId, and score:Actual WebEntity that I get:How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1066,47780934,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,True,2,100,True,1,50,False
1067,49635343,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1068,56313325,"""I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links. Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1069,52984076,"""I am working on augmented reality where i have to Recognise chinese & japanese textI used google vision and tessarct but still i am not getting text  when camera on text .""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1070,37152708,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""",False,0,0,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1071,55570088,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1072,50733603,"""my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.then I want to call this function one by one for all images""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1073,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False,0,0,False
1074,51102444,"""i'm playing around withGoogle Cloud Vision APIand have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (), you can output more than one type and I want to do the same.After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.""type"" is a ENUM and I listed the documentation under:I need help to implement more than one ""type""..Any ideas?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1075,51815006,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1076,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",True,5,100,True,1,20,True,1,20,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1077,48019749,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1078,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",False,0,0,True,2,40,True,5,100,False,0,0,True,1,20,False,0,0,False,0,0,False,0,0,False
1079,38292746,"""Good afternoon!Tell me how to overlay an image (ImageView), anchoring the to the eyes?I want to track using Google Vision API.Maybe there is an example of how to put a hat on head?Added. (12 Jule 2016)Use the following code:not anchoring (blue square) strictly to the eye, you can see it in the screenshots with different distance from the photos:Please tell me why this happens?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1080,34310579,"""I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below:Here's the error I receive:I want my code to look something like this:However, when I do this, I get the following error:How do I pass my code if a request doesn't apply to it?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1081,36405717,"""I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1082,42345567,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1083,52081400,"""The application only able to translate the image to text with using res/drawable image but i need to create an application that able to capture a photo and display it on ImageView and let the google vision api to translate the image to text. Can anyone tell me how to do it or other way to do it.}""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1084,55024058,"""I am developing an application that uses theGoogle vision APIand I have a question about the colors properties.Is the color shown in the properties with the highest percentage is the dominant color? And how that works ?Because the color with highest percentage is not accurate.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1085,56403969,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""",True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1086,49711906,"""I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:But it gives me this response:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1087,54839576,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1088,53918980,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1089,53275683,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1090,53963357,"""What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?Ex.:The json response shows the four coordinates of the bounding boxes in a clockwise disposition. However, I haven't found the unit. I assume that it is pixels, but it's not written anywhere...The API is available here:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1091,47537811,"""I am trying to build an android application (in android studio platform) which extracts different text languages from image using google cloud vision, but I have a problem in starting.I don't know how to use google cloud files. Which files do I need to create or download and how to direct my API to extract multiple languages?I got the API and this source code :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1092,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",False,0,0,True,2,50,False,0,0,True,2,50,True,3,75,True,2,50,True,4,100,True,4,100,False
1093,50699149,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1094,36977715,"""With the IBM Watson Visual Recognition API, how do you know the Classifier ID when we create the train? How do you do multiple classifying?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1095,43882577,"""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result. It is not getting populated.""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1096,56388041,"""The following script, allows to add to a collection only a single image at a time.How can I add the whole S3 bucket into a collection?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1097,48582519,"""I am IAM user and trying to hit APIRekognitionService.CreateCOllectionfor the testing in POSTMAN, but getting thisMy header request isThough it works with sameAccess Key IdandSecret KeyusingCLI(Command Line Interface).Can anyone help me to solve this problem.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1098,41923595,"""I'm trying to post data to Google Vision API. Butt i always get 400 response code - bad request. I have no ideas already.I have read and tried to use those links:And i came up to this:Here is my data to post:And here is my post:Here is data from console (THE CONTENT IS BLANK FOR EXAMPLE( not to post wole base64 )):And here is the response:Where is my mistake here?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1099,56026129,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""",False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
1100,55158595,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1101,48037316,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1102,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",True,1,50,True,2,100,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1103,56014699,"""I am using google vision api for detect text in image.Now, I want to know font size of text in image. How do it calculate ?or Do google vision api support?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1104,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",True,1,50,True,1,50,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False
1105,53269405,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
1106,53591219,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1107,33687536,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",True,1,100,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1108,49652382,"""Being locked on .NET 2.0, i can't use the GOOGLE VISION C# API which is only available since.NET 4.0.So I wanted to use this API with web request like this :The problem is that i permanently have a 400 return from google and after a lot of searching i can't find a solution.Can you give me a way to proceed to solve this problem please ?Thank you very much.(Sorry for bad english...)""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1109,44085992,"""How to get square crop hint from Google Cloud Vision?I'm setting the aspect ratio to 1 but the resulting hint is never an exact square.Below is the modified example fromchecking sample image from.The result is (zeroes are not printed):""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1110,38336213,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""",False,0,0,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1111,54295015,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1112,40957513,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1113,55120982,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1114,47831050,"""I want to create a collection of faces from 1500 face images and thenthis collection with one reference face image. The final goal is to find which face from the collection is the most similar one to the reference face image.So I want to retrieve one number for similarity for each pair of images (reference image and one face from the collection) each time.So does this amount to 1500faces x 1similarity_metadata = 1500metadata or the similarity attribute is counted as one metadata for any number of face images?In other words, does my request amount to 1500 metadata or 1 metadata for the 1500 faces?I am using the free version and AWS specifies that:So I am asking this because I do not want to exceed the limit of 1000 face metadata each month.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1115,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",True,1,25,False,0,0,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1116,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",False,0,0,True,2,50,True,4,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1117,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",False,0,0,True,2,100,True,2,100,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False
1118,51746269,"""I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module. But I always get the following error. What am I doing wrong?I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.My service authentication informations (just test data):My Node.js code to create the VisualRecognizionV3 object:I will appreciate your help!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1119,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,True,2,67,False,0,0,False,0,0,False
1120,52323135,"""I have a template like thisand wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?Thks""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1121,49918950,"""I've readbut it doesn't help at all.is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.Hence, I went check the documentation of posting requests directly:,and here are minimum codes to make the failure:I went to my console and see there are indeed request errors for, but I don't know what happened. Is it because the wrong format of sentin?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1122,51866993,"""We've got a pretty extensive BI system built on/. We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using. Here is the relevantif it helps:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1123,48188923,"""I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. I have tested an image in size:Google cloud vision api reports an image size ofThe strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)If you use e.g. an image size 5152 x 2890 the values from the API are correct.This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".I appreciate any idea how to fix this.Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1124,43852275,"""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1125,46281898,"""I am usingin a project. My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result. Is this possible?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1126,40189866,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1127,13617087,"""Im using an API to CURL a submitted user image to a remote server, the response is in JSON and I'm not sure how to parse it.Here is my php CURL fileand here is a sample of the JSON data that comes back (Generic)Code:Where the json string says ""name"" I need my script to only print the username if the number (after the : is higher then a threshold (lets say .70 for now).How do I do this? I've worked with XML api's before and returning the data was simple with aCode:type thing.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1128,53675809,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,True,1,100,False
1129,49444278,"""I'm having a problem withusing Amazonaws-cpp-sdk. I'm gettingsegmentationfault in following program.So, how to provide an image file from my local system to amazon aws-cpp-sdk?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1130,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",False,0,0,True,3,100,False,0,0,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False
1131,36570132,"""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1132,53224704,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False
1133,55956173,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1134,51187280,"""I am building anofflineionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it takespicture as an inputandreturns a string.this returns an output asHow can we access the python script or how to make it as a service ?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1135,51737440,"""I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well.Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.I am using the regular sample code to detect it:here is a sample image:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1136,50275116,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1137,51630466,"""i want to use Aws rekognition service to detect text in image with java script..Kindly tell me procedure to do it.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1138,41501511,"""I have extracted tags from the given image using Clarifai and Google Vision APIs. Similar thing I want to achieve for videos.Can anyone suggest, if there are any APIs available to do so.Thanks!""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1139,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",True,1,33,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,33,False
1140,47872907,"""Get different result for text detection from .Net code and demo app for the same imageandthis is my code:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1141,53105446,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
1142,41527687,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1143,42733482,"""I am using Microsoft Emotion API using python + requestsUsing the following code I am always getting a 400 error -and the input""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1144,51242545,"""i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.i have the next structure :when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService i have java.lang.NullPointerException: null       how right to push data to constructur ScanDirs?ImageService.class:the ScanDirs.class look like:}""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1145,46046085,"""I'm developer but new in Android and I need to know how can I use camera inside a Activity Layout.I know I need to use Surface View to insert camera inside an activity and currently my app is reading QR Codes with Google Vision using default camera (a button opens camera, the user takes the photo and my app perceive the activity result).But I really need to implementation that function inside app with real-time scanner.Someone can direct me?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1146,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",True,1,100,True,1,100,True,1,100,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False
1147,49562890,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""",False,0,0,False,0,0,True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1148,46516766,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""",False,0,0,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1149,56006562,"""I need to create a Driving license scanner, but I don't know how to started.The process that I need:1 - Open Camera2 - Detect Driving license 3 - Autofocus, autocrop and take the picture4 - Extract and OCR data from the driving licenseI already create an application where I crop and use google vision to OCR. My problem is detect, autofocus and autocrop only the detected document.Reference apps:Regula:BlinkID:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1150,45508302,"""I ve faced such a problem:In the documentations AWS have Rekognition SDK of Unity3DBy downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).Through the nuget I can t get SDK in Unity3D, because it s in conflict with Core.Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.SDK Rekognition for Unity3D exists?Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?Thank you.""",False,0,0,False,0,0,False,0,0,False,0,0,True,2,100,False,0,0,True,2,100,False,0,0,False
1151,53737055,"""I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.Here is an example of the output:And to draw the boxes on the image I do this:However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1152,43793934,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1153,42871797,"""I have an android app which on local WiFi and doesn't require an internet connection so I am looking for a solution which can scan QR codes without requiring Google Play Services as no internet connection is there so I do not want to update Google Play Services. Currently, I am doing it by using Google Vision API but somehow(if possible) I want to remove this dependency.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1154,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",False,0,0,False,0,0,False,0,0,False,0,0,True,3,100,False,0,0,True,3,100,False,0,0,False
1155,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",True,2,100,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1156,44240464,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1157,56178717,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1158,37900554,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1159,49732664,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1160,54757580,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1161,35672845,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1162,36774015,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",False,0,0,True,2,100,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False
1163,54881611,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""",False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1164,47155510,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1165,50714894,"""Background information:I'm trying to create a PoC for Google Cloud Vision API using their.What I have done:Create a simple console apps with the following code for Vision API.Problem:The linegets stuck for a long time before ultimately throwing the error.My code sits behind a corporate proxy, but I have validated that it is not blocking internet access because I can callfrom the same apps and get its JSON response just fine.Any suggestions?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1166,52539984,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1167,39797164,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
1168,45624819,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""",True,1,50,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1169,45274013,"""I have a number of non-JPEG image files that I want to process using Google Cloud Vision, but the API only accepts certain formats (see questionand answer).I can use PIL or some such to convert a TIFF to JPEG to be uploaded, but I'd like to avoid a temp file if possible.So, in python, how do I convert a TIFF in-memory for upload to GCV? numpy array, base64, string..?""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1170,54451771,"""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.can someone please explain what im doing wrong and how to prevent this error  ?""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1171,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",False,0,0,False,0,0,True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,False
1172,39925488,"""I new the Android code side and I have a project.I want text detection from  mage using google vision api, but I cannot.I search the internet but I can't find enough information and I know I should use Json(AsynTack).Just want this not face detection,logo detection.How can I do this, can you suggest anything?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1173,56176858,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",True,1,100,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
1174,54089791,"""I am using Google Vision via Rest API v1 with featureand  API is returning correct result:what I want to deserialize via Json.Net to Google vision object (fromnuget), but when I run thisit will return me empty resultsWhen I try to deserialize it to, then I am getting this error message..I cannot use nuget package directly for detection, because I need to call it via rest API""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1175,51361560,"""I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. ().It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1176,55666678,"""I created a collection using boto3 with following code:which is appearing as created when I fetch it using the following code in python boto3:But when I try to fetch the same collection using Javascript SDK ""aws-sdk"" in nodeJs using following code I get empty results:RESPONSE JS:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1177,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",False,0,0,True,2,67,True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1178,53799908,"""I am using  this Java  Lambda  code provided by  AWS   to detect labels in a  video:When  checking the  results in cloudwatch  I receive an error:I  am using the root   account and have  set  up   AWS  as described in the  "" Create the AWS Toolkit for Eclipse Lambda Project""  Rekognition developer-guide.Is the  problem  with the code  or something    else ?""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1179,54039956,"""I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1180,35757052,"""I'm using Google Vision API's sample to make barcode reader in a webview of existing project.this is my error message:This is build.gradle file:when I created BarcodeCaptureActivity.java,I get tons of errors...BarcodeCaptureActivity.java:why can't android studio recognize import in java?I'm stuck here for 2 days...please help me!!!!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1181,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",True,1,33,False,0,0,True,3,100,True,1,33,False,0,0,True,2,67,False,0,0,False,0,0,False
1182,54802917,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1183,50423259,"""I'm facing this issue today. Unable to access Microsoft FACE Api endpoint:Anyone facing the same issue today?Error:""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1184,49182513,"""I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings. But when I attempt to use the client, it doesn't work.The.The code I use:The exception I'm being thrown:I'm guessing there has to be something going on that i'm not aware of""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1185,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",True,3,100,True,2,67,True,1,33,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1186,56093211,"""I have encountered several cases that a call to object_localization, on Google Vision API, returns empty, whereas a cell to label_detection with the same image returns many object with hight level of certainty.Example:This image from Google Street ViewThe API can identify the billboard with high certaintyBut then I try it with object localization and... nothingWhen I try it with other images I get a partial localization, like in this image:I do get both labels and object localization annotations:Labels:Object localization annotations:So, it appears that there's no fundamental problem with my API configuration, but is there anything I can do different to make it work?""",True,3,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1187,49510330,"""I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.Detailed error:""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1188,55982785,"""I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1189,50174426,"""How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc . ) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt . Thanks""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1190,51319671,"""This is my gem file. But when i use bundle install i see this output.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1191,49466041,"""I am getting the following error when trying to access my s3 bucket with aws rekognition:My hunch is it has something to do with the region.Here is the code:And here is my config file:I have given almost all the permissions to the user I can think of. Also the region for the s3 bucket appears to be in a place that can work with rekognition. What can I do?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1192,44572133,"""I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition. I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.While trying to add an inline policy to the role I use and get stuck at the ARN field. I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail.I always get this error:What is the correct ARN that I have to enter?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1193,35952647,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1194,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",False,0,0,True,1,50,True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,True,1,50,False
1195,49099718,"""I'm having some trouble getting JSON response from Microsoft custom vision API (Optical Character Recognition API) when using Android Volley request.I have used this approach with other API's without any problems, but for this API I cant get it to work.I'm getting this response back:When using postman I'm not getting any errors.So hope you can see what I'm doing wrong.Let me know if you want me to elaborate on anything.Thanks!""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1196,49522705,"""I am trying to use google vision API to detect text from camera preview. However, I want the detected text to be within a specific region/rectangle in the camera review.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1197,38634409,"""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info.""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1198,49589030,"""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.""",False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False
1199,46287956,"""I have created a collection in was CLI like so:I would like to rename that collection to another string. I can't find how to do that. Any suggestion?""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1200,52163842,"""I m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get' What I m doing wrong? Thank you so much.All other services e.g. Polly and Rekognition are working well.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1201,54444114,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1202,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",True,4,100,True,1,25,False,0,0,False,0,0,False,0,0,False,0,0,True,1,25,False,0,0,False
1203,54725524,"""i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.i tried adding Bitmap.createBitmap();but it can only accept int values. but my values has decimal so it's a float.this is my code for face detection :""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1204,50258562,"""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.""",True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1205,50630045,"""I need to recognize image with Google Vision API. Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes. I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1206,51543592,"""Im experimenting a way to make the samples of microsoft cognitive services face api for real time and i cant find a way, i have tried mixing up codes from fotoapparat so that it would make a rectangle on faces detected and then from identificationtask from microsoft cognitive example like this but it wont seem to work.how will i do this?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1207,41824724,"""I am trying to use Amazon Rekognition Service with Node.js,I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.jsThe code is as below/* This operation detects faces in an image stored in an AWS S3 bucket.     */I coudn't get true data, this is the response I get:[Object] is written. Normally it should give a response like below.How can I get the face data?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1208,49819964,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""",False,0,0,True,2,100,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1209,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",True,1,50,True,1,50,True,2,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1210,38417738,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1211,55555575,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1212,44613751,"""I'm using the following, to start working on Google Cloud Vision platform. I tried the following steps:1) Created the project2) Created the bucket3) I also ran the following code:4) Ran each code given in the tutorial line-by-lineWhen I'm running this part of the code, it's throwing the following error.Error:I'm configuring the pipeline as following:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1213,51189021,"""I am running a simple code that utilizes google cloud vision api but it keeps on throwing this error. I have tries upgrading my packages shown here:but the error persists.PS:am using a virtual environment(virtualenv)""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1214,41299413,"""I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.But, I got the error message from Google Cloud Vision.This is my javascript code fragment.And my PHP script is below.I guess image processing must be wrong. But I have no idea what should I do about this. Would you give me an advice?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1215,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",True,3,100,True,1,33,False,0,0,False,0,0,False,0,0,True,1,33,False,0,0,False,0,0,False
1216,49416747,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""",False,0,0,False,0,0,True,2,100,False,0,0,False,0,0,True,1,50,False,0,0,False,0,0,False
1217,47000735,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1218,54700930,"""I am looking for a Google Vision API TypeScript definition file, but did not find any.Do they exist or do I have to create my own?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1219,50874265,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",True,2,100,False,0,0,True,1,50,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1220,44467350,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""",True,1,100,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1221,47296283,"""I'm trying to authenticate google vision api with electron and it seems like the google vision api can't find the required libraries so it gives me these errors.I added the node module usingand this is how I'm trying to initialize it""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1222,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",True,2,100,False,0,0,True,2,100,False,0,0,True,2,100,False,0,0,True,1,50,True,1,50,False
1223,50168647,"""I am relatively new to Python and trying to implement a Multiprocessing module for my for loop.I have an array of Image url's stored in img_urls which I need to download and apply some Google vision.This is my runAll() methodI am getting this as the warning when I run it and python crashes""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False
1224,50160997,"""I'm working on a project in which I need to analyze an image using Google's Vision API and post the response to a Dynamodb table.I have successfully implemented the Vision API, but not able to convert its response into Python Dictionary.Here's what I have tried:Here's the Vision api implementation:And Here's theFunction:Now, It doesn't return any error but theis not posted in Database table because it's not the correct form of the object, the problem here is thetype of response returns from Google's API.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1225,46287595,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""",True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1226,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",True,1,33,False,0,0,True,2,67,False,0,0,True,3,100,False,0,0,False,0,0,False,0,0,False
1227,53564632,"""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.""",False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False
1228,45190421,"""I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1229,53123056,"""I need to return the message sent by Rekognition to SNS but I get this error in  CloudWatch:Code:And is this the correct way of implementing Rekognition stored video in AWS Lambda with python I didn't find any examples on it.Update:The steps my app needs to take are:In the frontend, the user triggers a lambda function with API gateway which sends a file to s3When the file arrives trigger the same lambda function to apply video recognition and send jobId to SNSwhen SNS receives a message trigger the same lambda function to get the label data and return the data back to the user with API gateway""",True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1230,40856101,"""I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it.In my local text, text detection works properly.using local path :So, I immigrate that code to my server. But, when I run OCR function, error message appeared.Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.So. I want to need help. Thank you.My programming language is .net using visual studio 2012 + IIS 6.0""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1231,37786967,"""I am trying to create a Watson Visual Recognition Create Classifier using v3 of the rest API following the documentationwhich states:However, using a ""positive"" zip file of 48MB containing 594 images (max size of an image is 144Kb) and a ""negative"" zip file of  16MB containing 218 images (max size of an image is 114Kb) but I keep getting the error:In response to:I've kept trying reducing the file size by deleting images within the zips and re-trying but I'm well below the stated limits.Anyone got any idea?Thanks""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1232,52119949,"""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1233,50182544,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1234,56257028,"""I'm working on Pdf to text file conversion using google cloud vision api.I got an initial code help through there side, image to text conversion working fine with json key which  i got through registration and activation,here is a code which i got for pdf to text conversionthis function required  3 parameters string gcsSourceUri,string gcsDestinationBucketName, string gcsDestinationPrefixNameI dont understand which value should i set for those 3 params.I never worked on third party api before so its little bit confusing for me""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1235,53381742,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""",True,1,100,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False
1236,48548552,"""I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False
1237,51702859,"""I'm getting an errorswhen trying to run a codecamera is working, tested it in an other application.what are the reasons of those errors?""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1238,37690111,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",False,0,0,True,1,100,False,0,0,True,1,100,True,1,100,False,0,0,False,0,0,False,0,0,False
1239,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",True,4,100,False,0,0,True,2,50,False,0,0,True,1,25,False,0,0,False,0,0,False,0,0,False
1240,43409733,"""I using Google Vision OCR to get text from image with PHP language.When i used in a long time. I get error below:Please show me reason and how to fix it.Thank all so much.""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1241,50571761,"""I'm trying to read in an image of morse code (dots and dashes) using Google Vision OCR but it's not picking up the symbols very well. Is there a way to make Google Vision pick up the dots and dashes better or is there a different OCR product that can do a better job of recognizing symbols?Currently I am using the nodejs example as provided by Google Cloud at the moment:""",False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,True
1242,45455138,"""we are using google vision ocr for gathering text from receipts.In some cases the receipt have some text written in vertical  , like vat information and some other.The question is that  google vision read efficiently only the text in the main orientation (horizontal by example) and discards all the text written in the same receipt in vertical orientation instead in horizontal.Is there a parameter to set up for tell google vision to acquire also the text in vertical orientation?I have put online an example with an image with text in two orientations .Text recognized from g-vision :Horizontal text lineText I've expected to be recognized:Horizontal text lineVertical text line""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1243,39212656,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""",False,0,0,False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
1244,50018491,"""I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve. I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks.""",False,0,0,True,1,100,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False,0,0,False
