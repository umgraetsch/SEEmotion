,QuestionID,Question Text,Emotion Label,Form,AbsoluteRelevance,inip,endp
0,45033467,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""",Anticipation,ready,1,723,727
0,45033467,"""I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.Tagging solution won't work as they are at very high level.AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me.One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?""",Disgust,does not work,1,436,448
0,49619897,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""",Anticipation,result,2,33,38
1,49619897,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""",Anticipation,result,2,501,506
0,49619897,"""Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.Above is the python function that I want to execute in a. Here is the DAG:But, the work step is throwing an exception. Here is the log :This exception is due to this, which accroding to the descriptionhides another exception, still strange because I'm not doing any insertion.What am I doing wrong? Maybe there is a problem withused in the. Or, dataFrame is not the way to go in order to handle query results.PS: result of""",Anticipation,results,1,489,495
0,55388663,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""",Disgust,does not work,1,180,192
0,55388663,"""I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:I would like to know is there any other way to get text. Thanks a lot.""",Joy,would like,1,213,222
0,47919331,"""With AWS Rekognition I was able to get faces detected in a mp4 video with the following nodejs,And was able to get the results with the following cli,and outputs the faces in the following json format,How to extract those faces as images and dump them in an s3 bucket?Thanks""",Anticipation,results,1,120,126
0,41895608,"""I am trying to implementMicrosoft Face API in C#using code available on GitHub.I followed all the steps given in :I have some errors like:1-The name """" does not exist in the namespace.2-The type '' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built.3-The tag '' does not exist in XML namespace ''. Line 8 Position 10.In Solution Explorer, """" appears: that means no user controls libraries are loaded.""",Joy,like,1,134,137
0,46676980,"""I am working on a lambda function that needs to access,andservices from AWS.I gaveandpermissions via theand thepolicies respectively and it worked fineThe thing is that I could not access myinstance insidebecause it's inside a VPCI changed my lambda network configurations so it would be able to access the VPC, and theconnection worked as expected, but then the connection tostopped working, whenever I invokefor example it just hangs.Am I missing some permission?""",Anticipation,expected,1,341,348
0,46676980,"""I am working on a lambda function that needs to access,andservices from AWS.I gaveandpermissions via theand thepolicies respectively and it worked fineThe thing is that I could not access myinstance insidebecause it's inside a VPCI changed my lambda network configurations so it would be able to access the VPC, and theconnection worked as expected, but then the connection tostopped working, whenever I invokefor example it just hangs.Am I missing some permission?""",Fear,missing,1,442,448
0,56094441,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""",Joy,is,1,552,553
0,56094441,"""I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following.The string:The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No.I am defining patterns for each of these fields using regular expression library (re) in Python. For example:Output:However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age.How do I solve this problem?""",Joy,good,1,555,558
0,54683853,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",Anticipation,would be appreciated,1,440,459
0,54683853,"""On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request.Someone had answered this question in Python but not sure how to translate it in PHP.Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.######### Safe Search would look as follows""",Joy,would be appreciated,1,440,459
0,54122545,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""",Joy,;),1,348,349
0,54122545,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""",Joy,progress,1,375,382
0,54122545,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""",Anticipation,progress,1,375,382
0,54122545,"""Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.I went through the documentation but did't find any clue.""",Trust,label,1,171,175
0,50133223,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""",Joy,would like,1,69,78
0,50133223,"""We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?""",Trust,support,1,105,111
0,52419957,"""Through my IBM Cloud account, I have registered a Watson Visual Recognition service.Then I tried using their API to upload an image '' to my bucket associated with this service and then get some analysis on that image.I have tried the code available atTill the API and bucket validation, code works fine but throws errorI can see that this file is available in my bucket.Code:My current directory for IBM cloud notebook is""",Trust,associated,1,149,158
0,38811303,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""",Anticipation,in,1,572,573
0,38811303,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""",Sadness,have been wrecking,1,3,20
0,38811303,"""I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).I have managed to- save image after taking a picture with a camera- show it as a bitmap image on the appand I am trying to upload the file to the Watson API, but I keep getting this errorI would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be""",Trust,have managed,1,286,297
0,54612783,"""I am currently working on an Android app where I want to integrate the Google Cloud Vision API and do Facial Recognition on images.The code I implemented so far to make this work:The Layout XML File:The Log message:I basically don t know why the app crashes there with a null object reference and would appreciate any hints and feedback, thanks!""",Sadness,crashes,1,251,257
0,55685353,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""",Anger,conflict,1,135,142
0,55685353,"""I'm trying to evaluate Google vision endpoint. my pom is configured like belowThere are no other google dependency added. I see below conflict within the vision dependency itself.When I run the code I'm getting below error.I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.""",Sadness,conflict,1,135,142
0,48670839,"""I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string.I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?Thanks :)""",Joy,:),1,521,522
0,51336137,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""",Joy,like,1,267,270
0,51336137,"""I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.}But I can't figure out how to start the stream processor? Do I have to simply write the main method and callfunction? Or do I have to do something else: like the guide mentioned something as?""",Trust,guide,1,276,280
0,45260779,"""say that I have images and I want to generate labels for them in Spanish - does the Google Cloud Vision API allow to select which language to return the labels in?""",Trust,labels,2,47,52
1,45260779,"""say that I have images and I want to generate labels for them in Spanish - does the Google Cloud Vision API allow to select which language to return the labels in?""",Trust,labels,2,154,159
0,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",Joy,comfortable,1,205,215
0,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",Joy,to accomplish,1,558,570
0,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",Joy,would be appreciated,1,1288,1315
0,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",Joy,greatly would be appreciated,1,1297,1315
0,49575082,"""I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.The problem I have is the producer. I found that AWS has a Java Producer library available (). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.Any help, or direction would be greatly appreciated.""",Anticipation,would be appreciated,1,1288,1315
0,49520402,"""I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.""",Trust,am associating,1,329,342
0,40921512,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""",Anticipation,expected,1,153,160
0,40921512,"""I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" onI noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site?here is my code:""",Disgust,accurate,1,172,179
0,47982254,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""",Sadness,Invalid,1,279,285
0,47982254,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""",Sadness,Unable,1,312,317
0,47982254,"""I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:Some extra info from the visual recognition node's result:""",Anticipation,result,1,981,986
0,52829583,"""I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.It's happening because I have only English characters in google vision word component as follows.As I can't include characters from all the languages, I am thinking to include the inverse of above. Something likefor example.So where can I findALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION?Trial and error, keep adding the special characters I find is one option.But that would be my last option.""",Sadness,fails,1,163,167
0,50805054,"""I am trying to perform OCR on PDF files using google cloud vision, to do some basic testing i am using the sample code provided in google documentation in the below link, but the line of code provided below is throwing unicode decode error given below, can anyone please help me fix this, i did an extensive search and tried different approaches but i am unable to fix it.Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte""",Sadness,unable,1,356,361
0,50805054,"""I am trying to perform OCR on PDF files using google cloud vision, to do some basic testing i am using the sample code provided in google documentation in the below link, but the line of code provided below is throwing unicode decode error given below, can anyone please help me fix this, i did an extensive search and tried different approaches but i am unable to fix it.Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte""",Sadness,invalid,1,453,459
0,41285556,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",Joy,:),1,326,327
0,41285556,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",Joy,(:,1,439,440
0,41285556,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",Anticipation,result,1,252,257
0,41285556,"""I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.However the response result doesn't include information of correct image orientation. (document:)Is there anyway to not only get recognized text but also get theorientation?Could Google support it similar to (: getRollAngle)""",Trust,Could support,1,404,423
0,55330723,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",Joy,:),2,153,154
1,55330723,"""I have a question for azure custom vision. I have a custom vision project for object detection. And I use the python SDK to create the project (see that:). But I found something wrong in the process of uploading. For example, there is a picture that has 3 persons in this picture. So I tag 3 same class  person  in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture.I tried to use python SDK (see that:) to upload my picture and tag information.The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?""",Joy,:),2,628,629
0,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",Joy,successfully,2,89,100
1,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",Joy,successfully,2,433,444
0,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",Joy,like,2,389,392
1,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",Joy,like,2,605,608
0,49558215,"""I am trying to invoke IndexFaces API but getting an error :I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to  xternalImageId'.Here is the snapshot from the S3 of my uploaded file :If I get rid of folder structure and directly dump the file , like :then the IndexFaces API is passing it successfully .Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :Above code internally calls :""",Anticipation,to invoke,1,13,21
0,46516494,"""I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error thatwas inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue.Here is my code:""",Trust,fact,1,438,441
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Anticipation,instructions,1,284,295
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Anticipation,in,1,595,596
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Anticipation,results,1,367,373
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Trust,instructions,1,284,295
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Trust,CREDENTIALS,1,442,452
0,50190527,"""I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""I have checked the older stack overflow questions and the links provided in answers are not active anymore.Thanks in advance for your help.""",Fear,missing,1,388,394
0,54634576,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",Disgust,doesn't work,1,412,423
0,54634576,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",Fear,commands,1,377,384
0,54634576,"""To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever.I've cloned downwith code samples straight from Google. I've built the project usingand it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all.First I get anmessage in the log stating:After that follows:This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so).I got my environment variablepointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers.""",Trust,credentials,1,746,756
0,55665919,"""I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error. I have no idea what I am doing wrong.I tried to make this request via Postman and everything is working fine there but in c# it just won't work.Can you help me please?The C# request body looks like this:Whereas the Postman request body looks like this:""",Disgust,won't work,1,310,319
0,54353716,"""I'm testing IBM's Watson Visual Recognition using Node-RED, I've trained it to identify some elements in the image, but I wonder if it's possible to get the exact position of these elements.""",Anticipation,I wonder,1,121,128
0,55276425,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""",Trust,labels,2,97,102
1,55276425,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""",Trust,labels,2,776,781
0,55276425,"""I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...Here is a link to the documentation :I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this...Here is more or less what my code is:Anyone got an idea how to getmore resultsin the $labels array?""",Joy,so good,1,124,130
0,52126752,"""I am trying to call google cloud vision api from xamarin C# android application code.I have set environment variable but still I was not able to call api.So I decided to call it by passing credential json file but now I am getting error deserializing JSON credential datahere is my code""",Trust,decided,1,160,166
0,52126752,"""I am trying to call google cloud vision api from xamarin C# android application code.I have set environment variable but still I was not able to call api.So I decided to call it by passing credential json file but now I am getting error deserializing JSON credential datahere is my code""",Trust,credential,2,190,199
1,52126752,"""I am trying to call google cloud vision api from xamarin C# android application code.I have set environment variable but still I was not able to call api.So I decided to call it by passing credential json file but now I am getting error deserializing JSON credential datahere is my code""",Trust,credential,2,257,266
0,56216376,"""We are trying to extract the text from an image using google-cloud-vision API:In this code, we need to make the API read the image through the 'cv2' function only, instead of using the 'io' function:Any suggestion will be helpful""",Trust,helpful,1,223,229
0,52848248,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""",Anticipation,result,1,374,379
0,52848248,"""I am using google vision API and trying to get the text from the captured image.I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.here is my main activity xml file.The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.""",Anticipation,results,1,473,479
0,47361521,"""I'm using Python to make a query to Google's Vision API to obtain labels from an image, but I'm not able to set a timeout in case I don't receive a response within a given time.I'm using the following code based onof CallOptions.This is my code:I have tried passing directly the arguments into the call to Google without success, like this:""",Trust,labels,1,67,72
0,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Trust,do label,1,149,156
0,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Trust,credentials,3,96,106
1,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Trust,credentials,3,341,351
2,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Trust,credentials,3,519,529
0,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Anticipation,attempts,1,321,328
0,45720587,"""I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentialsThis worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.""",Disgust,isn't working,1,434,446
0,51195006,"""I am following the following Google Cloud Vision quickstart:This is using the API Explorer, and I getI have created a bucket named vision2018, and checked Share Publicly for the file.My portion of the request related to the file is:The response I get is:What do I need to specify in order to access files in my GCP storage?Alternatively, I read other Stack Overflows that talk about GOOGLE_APPLICATION_CREDENTIALS, Simple API Key, and ""Create Service account key and download the key in JSON format"", ...  but these seem to be giving commands in the shell, which this quickstart doesn't even open.Is there initial setup assumed prior to the quickstart?I am not ready to call the api from code""",Fear,commands,1,535,542
0,51195006,"""I am following the following Google Cloud Vision quickstart:This is using the API Explorer, and I getI have created a bucket named vision2018, and checked Share Publicly for the file.My portion of the request related to the file is:The response I get is:What do I need to specify in order to access files in my GCP storage?Alternatively, I read other Stack Overflows that talk about GOOGLE_APPLICATION_CREDENTIALS, Simple API Key, and ""Create Service account key and download the key in JSON format"", ...  but these seem to be giving commands in the shell, which this quickstart doesn't even open.Is there initial setup assumed prior to the quickstart?I am not ready to call the api from code""",Surprise,ready,1,662,666
0,51195006,"""I am following the following Google Cloud Vision quickstart:This is using the API Explorer, and I getI have created a bucket named vision2018, and checked Share Publicly for the file.My portion of the request related to the file is:The response I get is:What do I need to specify in order to access files in my GCP storage?Alternatively, I read other Stack Overflows that talk about GOOGLE_APPLICATION_CREDENTIALS, Simple API Key, and ""Create Service account key and download the key in JSON format"", ...  but these seem to be giving commands in the shell, which this quickstart doesn't even open.Is there initial setup assumed prior to the quickstart?I am not ready to call the api from code""",Trust,CREDENTIALS,1,403,413
0,48840806,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",Sadness,unable,2,411,416
1,48840806,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",Sadness,unable,2,714,719
0,48840806,"""I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:The output after I click the ""Process"" button in the app is as follows:In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.Please help me with displaying a rectangle first. Later I will adjust it to display around the face.""",Joy,successfully,1,643,654
0,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",Trust,labels,5,52,57
1,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",Trust,labels,5,132,137
2,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",Trust,labels,5,212,217
3,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",Trust,labels,5,295,300
4,45313874,"""I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?I believe they used the same labels the following project:I thought of two possible methods of getting these labels:Sending thousands of different images to the API and recording the returned labels (I would automate this)Going through all the Google Open Image data (which I linked above), and recording the labels.I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).Thanks a lot for any help!""",Trust,labels,5,412,417
0,46380748,"""I started using Google Vision API recently and have confronted a problem.Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.My text detection code is following:The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. My question is, how do I set the hints, so the detection always has the direction?""",Anger,have confronted,1,48,62
0,40103531,"""I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?""",Surprise,result,1,230,235
0,51381522,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",Anticipation,Progress,1,206,213
0,51381522,"""I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:I searched for few days and I know that infunction I need to call.will triggerand there I will get percent of upload as parameter (i) and increment. Something like this:But I don't know how to get percentage of uploaded image in callCloudVision function.Please help""",Joy,Progress,1,206,213
0,37908660,"""I have a similar question toand, neither of which have an accepted solution.I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.suggests using, but (having tried and failed) I'm not sure how to integrate it into my app.Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from mywhich I want to use to be able to toggle the flashlight:""",Sadness,failed,1,224,229
0,47089134,"""I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this?""",Disgust,accurate,1,349,356
0,51817443,"""My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?""",Sadness,Is destroyed,1,358,386
0,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,18,20
1,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,148,150
2,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,223,225
3,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,393,395
4,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,467,469
5,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gem,6,510,512
0,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gems,2,341,344
1,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Joy,gems,2,547,550
0,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Anticipation,can prevent,1,560,572
0,52451363,"""I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then runon the Rails project that is using the gem:My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?""",Anticipation,expectation,1,230,240
0,43628002,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""",Trust,CREDENTIALS,1,176,186
0,43628002,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""",Trust,credentials,1,230,240
0,43628002,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""",Trust,authenticated,1,296,308
0,43628002,"""I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at.I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message""OSError: Project was not passed and could not be determined from the environment"".What change do I need to make in order to run this example?Thanks,Andi""",Sadness,unfortunately,1,395,407
0,40087567,"""I am trying to test out google cloud vision api by followingon using cloud vision api.Step 1:Generating JSON Requests by typing the following command in the terminalThe above command generates request.json file.Step 2:Using Curl to Send Generated RequestsOutput in Terminal (following step 2)Notice that the output in the terminal (see below) showsand.Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?The below is the out put in TerminalBelow is the JSON request generated in request.json fileBelow is theinThe below is the text inside cloudVisionInputFile""",Sadness,unable,1,430,435
0,44826567,"""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!""",Anticipation,in,1,247,248
0,44826567,"""I'm working with the Microsoft Azure face API and I want to get only the glasses response.heres my code:and it returns a list like this:I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses"" Thanks for any help in advance!""",Joy,would return,1,177,193
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Anticipation,results,2,144,150
1,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Anticipation,results,2,427,433
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Anticipation,are expected,1,330,341
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Anticipation,training,1,995,1002
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Trust,certain,2,198,204
1,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Trust,certain,2,618,624
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Trust,label,1,230,234
0,47614963,"""I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,My program is designed in a way that, when objectis detected with accuracy greater than certain value, then next action will be dispatched.There are 3 clusters of object types. For instance,goes to container,goes to containerandgoes to container.When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?""",Trust,respected,1,915,923
0,43410910,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""",Disgust,does not work,1,323,335
0,43410910,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""",Joy,love,1,9,12
0,43410910,"""for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the grouphere is my codewhat should happen is a HTTP verb status OK 200 should be return back , all I get isI look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.thanks""",Trust,love,1,9,12
0,55463500,"""I am trying to set a React-native for detect text using amazon rekognition API.My guide is this tutoriali have configured the connection with AWS using awsmobile and amplify and in both cases i had the same error: API rekognition does not exist.My user has the corrects permissions and my modules and sdk are with the last version.My connection API.js is the next:Thank you!!""",Trust,guide,1,83,87
0,45546546,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""",Joy,successfully,1,175,186
0,45546546,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""",Joy,'d like,1,43,49
0,45546546,"""I have an ImageAnalyses Controller where I'd like to execute some code just afterImageAnalysisis instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below.My controller:Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error.I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?""",Anticipation,results,1,553,559
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Trust,enabled,1,115,121
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Trust,CREDENTIALS,1,308,318
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Anticipation,I am wondering,1,475,488
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Fear,concerned,1,278,286
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Joy,are looking,1,416,426
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Joy,good,1,428,431
0,46739009,"""I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.I am wondering why I am getting exceptionHere is the code of method where exception is thrown:P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.""",Sadness,concerned,1,278,286
0,52276444,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""",Trust,are associated,2,136,158
1,52276444,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""",Trust,are associated,2,209,231
0,52276444,"""On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""But when I use the Pythonmy images that are uniquely associated with the last trained iteration are not deleted.Do I need to do something else or this is not working?""",Disgust,is not working,1,320,333
0,50515317,"""The similar question has been asked here:. However, it has not been answered yet.Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.Added at 8th June 2018Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in() function to use the Facial Mask / Filter feature with the camera.""",Joy,like,1,410,413
0,55151128,"""I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was:Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string""",Sadness,Unfortunately,1,333,345
0,36974179,"""I been trying to solve this error but I can't find what seems to be wrong.I am usingwith. Here is my code:When I run the script I get:The thing is that when I put the exact same Key on theeverything works fine. So I am pretty sure it is not the key.The error must be on my code, but I can't find it.Any tip in the right direction will be appreciated,Thanks""",Joy,will be appreciated,1,331,349
0,45468418,"""I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text). I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better.Thank you.""",Joy,achieve,1,268,274
0,44812417,"""I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it? This way I can extract only the name.OBS: I open to using any kind o service that facilitates this""",Joy,kind,1,575,578
0,55095799,"""I am using this piece of code to fetch the data from vision API for matching pages, but I always got only 10 results, whereas in google vision official website the dataset is large than the same.""",Anticipation,results,1,110,116
0,55095799,"""I am using this piece of code to fetch the data from vision API for matching pages, but I always got only 10 results, whereas in google vision official website the dataset is large than the same.""",Trust,official,1,144,151
0,35737830,"""How can this be completed with the Google Vision-API please?send image to vision-apirequest: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]receive the labels in particular the one I'm interest in is a ""clock""receive the boundingPoly so that I know the exact location of the clock within the imagehaving received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR libraryCurrently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.""",Trust,LABEL,1,112,116
0,35737830,"""How can this be completed with the Google Vision-API please?send image to vision-apirequest: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]receive the labels in particular the one I'm interest in is a ""clock""receive the boundingPoly so that I know the exact location of the clock within the imagehaving received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR libraryCurrently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.""",Trust,labels,1,160,165
0,35737830,"""How can this be completed with the Google Vision-API please?send image to vision-apirequest: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]receive the labels in particular the one I'm interest in is a ""clock""receive the boundingPoly so that I know the exact location of the clock within the imagehaving received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR libraryCurrently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.""",Trust,LABELS,1,498,503
0,35737830,"""How can this be completed with the Google Vision-API please?send image to vision-apirequest: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]receive the labels in particular the one I'm interest in is a ""clock""receive the boundingPoly so that I know the exact location of the clock within the imagehaving received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR libraryCurrently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.""",Trust,supports,1,471,478
0,56407381,"""I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application. But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent).My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly.I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform.""",Trust,enabling,1,67,74
0,47308379,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",Fear,missing,2,514,520
1,47308379,"""I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into. Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?Here are my Unity settings:Am I missing something else? Any ideas? Thanks!""",Fear,missing,2,676,682
0,53056817,"""Currently using the google cloud vision api for pulling text from images of documents.Current situation- the API works great, and returns tons of data including the bounding boxes of where the words are located.Desired outcome- to query only the words pulled from the image and not all the meta data about where the bounding boxes and vertices of the words are (it's like 99% of the response and comes out to be about 250k which is a huge waste when all I want are just the words)""",Anger,waste,1,440,444
0,53056817,"""Currently using the google cloud vision api for pulling text from images of documents.Current situation- the API works great, and returns tons of data including the bounding boxes of where the words are located.Desired outcome- to query only the words pulled from the image and not all the meta data about where the bounding boxes and vertices of the words are (it's like 99% of the response and comes out to be about 250k which is a huge waste when all I want are just the words)""",Anger,huge waste,1,435,444
0,42122978,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""",Trust,certain,1,151,157
0,42122978,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""",Trust,decided,1,179,185
0,42122978,"""So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.""",Surprise,'ve discovered,1,10,32
0,54466934,"""I have a folder with 100+ images. I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.Using following code:I am usingto choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..Usedbut getting below errorfound one post but that is in python  unable to replicate it in R""",Sadness,unable,1,466,471
0,33515465,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""",Anticipation,expected,1,633,640
0,33515465,"""I am trying to overlay awith the Earth Demo-Renderer over the.I use this to make the background transparent:This is the layout file of my activity:It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.I also get this error when trying to preview the layout xml in Android Studio:UPDATE 1:I removed this line (), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.According to, it should work as I expected it to. What do I have to do?UPDATE 2:(usingtutorial)OK - Adding the linerenders the 3d scene on top of my camera preview, but I still have a problem.Neithernoronly clear the background of the scene.I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)""",Sadness,:(,1,678,679
0,54605435,"""I want to use Google Cloud Vision API for image recognition, Everything installed fine in my yii2 framework.I'm getting authentication error like :How to point my key.json file to GOOGLE_APPLICATION_CREDENTIALS environment variable In yii2 framework.Thanks""",Joy,like,1,142,145
0,54605435,"""I want to use Google Cloud Vision API for image recognition, Everything installed fine in my yii2 framework.I'm getting authentication error like :How to point my key.json file to GOOGLE_APPLICATION_CREDENTIALS environment variable In yii2 framework.Thanks""",Trust,CREDENTIALS,1,200,210
0,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",Joy,could be remedied,1,501,517
0,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",Joy,happy,1,787,791
0,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",Joy,Extremely happy,1,777,791
0,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",Anticipation,could be remedied,1,501,517
0,54790692,"""So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appendsat the end of the filename. So let's say I'm processing a file calledthat's 8 pages long, the output will not be(even though I specified that), but rather be.Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appendingto outputs. Extremely happy for any pointers!""",Trust,could be remedied,1,501,517
0,26010360,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",Anticipation,result,1,405,410
0,26010360,"""I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. seefor similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.Hoping for help :)""",Joy,:),1,574,575
0,40346408,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",Anticipation,training,2,6,13
1,40346408,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",Anticipation,training,2,512,519
0,40346408,"""I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.I've collected 400 pictures of something - e.g. dogs.Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:Multiple dogsA dog with another animalA dog with a personA partially obscured dogA dog wearing glassesAlso, would dogs on a white background make for better training samples?Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?""",Trust,certain,1,44,50
0,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Disgust,inappropriate,3,171,183
1,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Disgust,inappropriate,3,443,455
2,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Disgust,inappropriate,3,664,676
0,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Joy,successfully,1,726,737
0,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Joy,could accomplish,1,318,333
0,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Anticipation,could prevent,1,231,243
0,54876804,"""I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process.I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?""",Trust,the best,1,535,542
0,43735666,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""",Trust,CREDENTIALS,1,311,321
0,43735666,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""",Trust,credentials,1,582,592
0,43735666,"""I am trying to make one simple application in Xamrin android using.What I did is,InstalledGoogle cloud vision v1,Google.Apis.Auth.OAuth2;Newtonsoft.Json;in my xamarin.android project from NuGet manager.I createdAPI Key,Service Account,OAuth 2.0 client IDsfrom google cloud console.I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) and linked those Json file (both service account andOAuth 2.0 client IDs json) tried both.Then I just copied code from google vision API documentaion.everytime i try to compile the program it throws exceptionI need help to set default credentials, I tried many links from googledocumentation but no luck.has anyone got any idea how to handle this exception.""",Sadness,console,1,274,280
0,41348880,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""",Trust,labels,2,80,85
1,41348880,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""",Trust,labels,2,593,598
0,41348880,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""",Trust,Guide,1,241,245
0,41348880,"""So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section.I wanted to know if anyone knows how to integrate AWS Rekognition inSwift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I already downloaded theand theand added them into my project. Also I have imported both of them in myand initialized my AWS Credentials.Also I've tried to initialize Rekognition and build a Request:Thanks a lot!""",Trust,Credentials,1,725,735
0,51156207,"""I am using Rekognition'sandto analise a video with some people. The result is a json file, something like this:Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?I couldnt find anything in the docs, nor in php/java SDKs""",Anticipation,result,1,69,74
0,50782221,"""How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?3 examples:shows a complete headshows a cut off head where the full ""face"" is visibleshows a cut off head where also the face is cut offTo narrow down the question, the following cases should be detected:there is a completely visible head in the imagethere is a partly visible head in the image where parts of the head are outside the image boundsThe following is out of scope for this question:heads that are spatially or scenically inside the image bounds but fully or partly covered by other objectsthere are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to itthe effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveatsI have checked the documentation but it doesn't say anything about head crop-off detection.I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.There was aabout this question.""",Anticipation,is expected,1,1130,1150
0,48892816,"""I'm trying to find a way to crop/obtain offsets of Movie titles via Google Cloud Vision API.Here's an example image:I've tried to use FACE_DETECTION, LOGO_DETECTON, and event LABEL_DETECTION but I can't seem to get a result for it.Any ideas?""",Anticipation,result,1,218,223
0,48892816,"""I'm trying to find a way to crop/obtain offsets of Movie titles via Google Cloud Vision API.Here's an example image:I've tried to use FACE_DETECTION, LOGO_DETECTON, and event LABEL_DETECTION but I can't seem to get a result for it.Any ideas?""",Trust,LABEL,1,176,180
0,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Anticipation,results,2,772,778
1,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Anticipation,results,2,913,919
0,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Trust,landmarks,2,96,104
1,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Trust,landmarks,2,176,184
0,47413657,"""Here's some research I have done so far:- I have used Google Vision API to detect various face landmarks.Here's the reference:Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link:I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it:For Android here's the reference:There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.For instance:1)2)There is possibility might have some see thisquestion as duplicateof this:But the thread is almost 1.6 years old with no right answers to it.I have gone through this article:It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK.As per my research no good enough material is around which helps to full fill the desired results likeMSQRD face filters.One more Github repository around which has same implementation but it doesn't gives much information about same.Now my question is:Image attached for more reference:ThanksHarry""",Joy,to achieve,1,749,758
0,51611493,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",Anticipation,promise,3,34,40
1,51611493,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",Anticipation,promise,3,295,301
2,51611493,"""I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:This will output the full text to the console. If I put the above code into a function and return the variabledetectionsI get onlyundefinedback. I assume the cause of the problem is that a promise is async.How can I returndetectionsin a route and wait for the promise to resolve so that I can return it via res.send?This is the function:This is the route:Thank you.""",Anticipation,promise,3,366,372
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,result,3,58,63
1,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,result,3,467,472
2,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,result,3,568,573
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,is ordered,1,71,80
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,likely,1,372,377
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Anticipation,more likely,1,367,377
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Joy,to accomplish,1,338,350
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Joy,are,1,379,381
0,40837023,"""I'm using Microsoft Computer Vision to read receipts.The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.I would to prefer if the whole list of products is one region and that each line is a product.Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.Bellow is the image of the receipt and the result from the computer vision.""",Joy,good,1,393,396
0,49388341,"""I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.""",Disgust,isn't working,1,394,406
0,47641619,"""I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object.Any clue? I have to readfullTextAnnotation.textkey. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]This is the execution output:""",Disgust,are not working,1,232,246
0,52270452,"""I am facing issue while implementing Amazon Rekognition. The error I am getting is:AWSRekognition class, createStreamProcessor API call always through the following error:AWSKinesisRecorder class API submitAllRecords API call always through the following error:Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.Any help appreciated?""",Joy,appreciated,1,387,397
0,43425391,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""",Sadness,Invalid,2,151,157
1,43425391,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""",Sadness,Invalid,2,231,237
0,43425391,"""I am trying to refer a local jpg file for using in Azure Emotion API.To do this, I refer my file through ""file:///"" like below.But the response says ""Invalid image URL."" How could I fix it?{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}Whole code looks like below.""",Joy,like,1,117,120
0,47392166,"""I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image.Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.""",Anticipation,ready,1,284,288
0,50452142,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""",Trust,support,2,85,91
1,50452142,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""",Trust,support,2,265,271
0,50452142,"""I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.Thanks in advance.""",Anticipation,in,1,553,554
0,50536717,"""What is the best and simple way to search and get images list from S3 by using English keywords. Or do I have to use the Rekognition to store all the image metadatas into database?My development is using Php.""",Trust,the best,1,9,16
0,47557888,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",Anticipation,results,1,436,442
0,47557888,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",Joy,'d like,1,369,375
0,47557888,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",Sadness,lack of,1,246,252
0,47557888,"""I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times.Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?""",Trust,the best,1,427,434
0,42431787,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",Trust,is enabled,1,181,190
0,42431787,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",Trust,enabling,1,447,454
0,42431787,"""Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:Example request:Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.Calls made from thealso show a permissions error message. Calls from thedo work however.Edit:The error is also happening on the. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page:Edit:Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1.HTTP request:HTTP response:""",Anticipation,resulting,1,1045,1053
0,55585979,"""I am new to Google Vision, and I want create code to receive an asynchronous response. For example, create a JSON file to response and later load the JSON file and continue with de recognizer.I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.This is how I save the response to a JSON file:This is how I try to read and use the JSON file:but it does not work, it says""",Disgust,does not work,1,417,429
0,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Anticipation,results,3,123,129
1,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Anticipation,results,3,800,806
2,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Anticipation,results,3,931,937
0,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Anger,disastrous,1,157,166
0,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Fear,command,1,760,766
0,45979638,"""The goal is to make an app which can recognize egg markings, for example. I tried bothand theon the following images. The results from both OCR engines are disastrous.0-DE-460423-ES08234-25591CroppedI manually cropped the images with Photoshop.0-DE-460423-ES08234-25591ThresholdedI color-selected the text on both eggs manually with Photoshop and removed the background.0-DE-460423-ES08234-25591Removing the circular warp?I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.My questionsAm I heading in the right direction?Are my preprocessing steps correct?What would be the approach to automate these steps in, say, OpenCV?Extra infoThe command I used to get the tesseract OCR results:The tesseract version:Platform:Edit 1I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.""",Sadness,disastrous,1,157,166
0,49936444,"""I am unable to save the image inbut can save in the same rekognition directory. How do I save a snapshot from camera to static folder?My OpenCV code is running in post method of.In:""",Sadness,unable,1,6,11
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Joy,:),1,163,164
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Joy,grateful,1,1194,1201
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Joy,very grateful,1,1189,1201
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Anticipation,am worried,1,978,992
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Anticipation,results,1,934,940
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Trust,support,1,82,88
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Trust,credentials,1,279,289
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Fear,am worried,1,978,992
0,54195144,"""I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial:), spring app/jHipster throws a bunch of errors at startup which begins with:As far as I can tell it is related to credentials and Beanwhich is located in main application class.Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.application.properties looks like that:Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.I would be very grateful for even small hint about what I can do to resolve this issue.""",Sadness,depressing,1,1132,1141
0,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Anticipation,result,2,144,149
1,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Anticipation,result,2,451,456
0,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Anticipation,postResult,1,197,206
0,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Anticipation,results,1,402,408
0,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Anticipation,would be appreciated,1,620,639
0,53794638,"""I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,And I converted the $result to an array using,I tried to print the array $postResult using,and it printed something similar to,I wanted to print only the value 'Name'. So I used,But it throws an error as,Undefined index: Aws\ResultdataI also tried using the foreach loop, but it results in the same errorHere is the output for $result,I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.Any help would be appreciated!""",Joy,would be appreciated,1,620,639
0,48266531,"""I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API.""",Anticipation,results,1,123,129
0,48266531,"""I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.This is the function:I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.I've been checking the official documentation but didn't really find out about the format of the response of the API.""",Trust,official,1,463,470
0,47849128,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",Anticipation,result,2,484,489
1,47849128,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",Anticipation,result,2,599,604
0,47849128,"""I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify methodCode:Before same code returning below result. Please refer below image:Result ""One"" class in Custom classifiers.But now same code is returning different result:""",Anticipation,Result,1,517,522
0,46151909,"""I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers.1) how to get the category list of the object detection part.2) how long does it take to process an image to get object labels in it without considering the data transmission time.Is there anyone has any ideas?""",Trust,labels,1,237,242
0,48607548,"""I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.In order to get more responses I added the fieldin. After adding thefield it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI?My python code for taking image and features:""",Sadness,unable,1,356,361
0,44532633,"""Every time I run the commandI get this error.I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)I am able to see on my user account that it is calling Rekognition.It seems like the issue is somewhere with my S3 bucket but I haven't found out what.""",Trust,labels,1,70,75
0,52536703,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",Anticipation,in,1,584,585
0,52536703,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",Disgust,is not working,1,267,280
0,52536703,"""I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.""",Joy,like,1,160,163
0,49840929,"""I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android (see:). How can I use this with?I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags.From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports.I found the. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".I hope someone has a tip or a starting point.""",Joy,:),1,99,100
0,55369637,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""",Joy,Would like,1,459,468
0,55369637,"""I am developing this system using Google Vision API and Google Cloud storage.When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1.example :filename.pdf** is translated tofilename.pdf.jsonoutput-1-to-1.jsonI want to remove thejsonoutput-1-to-1and make the file becomefilename.pdf.jsonHere is my code.I manage to list down the file but Would like to have without jsonoutput-1-to-1""",Trust,manage,1,426,431
0,51444352,"""I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the errorAny idea of what I can do in this situation? I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allowsand it still doesn't work.""",Disgust,doesn't work,1,506,517
0,56224197,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""",Trust,team,1,318,321
0,56224197,"""I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer.""",Trust,support,1,310,316
0,46244980,"""I'm using the ""TEXT_DETECTION"" option from the Google Cloud Vision API to OCR some images.The bounding box around individual characters is sometimes accurate and sometimes not, often within the same image.Is this a normal side-effect of a probabilistic nature of the vision algorithm, a bug in the Vision API, or of course an issue with how I'm interpreting the response?Here's the portion of the response specific to the letter ""a"" from which I'm extracting the bounding box.""",Trust,accurate,1,150,157
0,47532783,"""I am using google vision API to detect the face and crop the image accordingly.this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.the result of vertices""",Anticipation,result,1,180,185
0,45296021,"""I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?I have successfully integrated my web camera and able to stream video over URL usingAny library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.""",Joy,successfully,1,173,184
0,43494736,"""Thestates that formethod, theandcan takeor. I want to use thewhich can beWhen I pass theencoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error sayingDid anyone manage to use theusing base64 encoded images (not)? or any JavaScript examples usingparam would help.""",Trust,manage,1,225,230
0,45775358,"""I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one?It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct.""",Joy,would like,1,385,394
0,47571678,"""Helo everyone,I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:Create Collection name ""DATABASE""Index faces from individual pictures and store them in collection ""DATABASE"".run index faces on target image and store all faces in a separate collection ""toBeDetected"".Use SearchFaces API call to identify all the faces from the target images against Database collection.however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.RekognitionCollectionCreateHelperAddFacesToRekognitionCollectionMatchAllFacesInCollectionDetectMultipleFaceHelper}Please help. Thank you!""",Sadness,invalid,1,500,506
0,52343909,"""Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.Background:I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.""",Trust,supports,1,249,256
0,43687962,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",Anticipation,result,1,102,107
0,43687962,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",Anticipation,expected,1,117,124
0,43687962,"""I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while theurl correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.""",Surprise,baffles,1,132,138
0,49974994,"""I prepare some solution to detect text of the image, now I am getting bounding box of text, symbol, and language property.Is there any way to getting table structure of documents using Google Vision API?""",Anticipation,prepare,1,3,9
0,54560404,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",Anticipation,'m expecting,1,913,924
0,54560404,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",Joy,is,1,331,332
0,54560404,"""I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts.For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.So, my questions are:1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.""",Joy,good,1,338,341
0,56285264,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""",Trust,label,2,52,56
1,56285264,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""",Trust,label,2,96,100
0,56285264,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""",Joy,awesome,1,83,89
0,56285264,"""I tried sample of Google Vision API (PHP)I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?P/S: Sorry for my bad English :(""",Sadness,:(,1,210,211
0,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",Joy,Excel,1,92,96
0,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",Joy,excel,1,386,390
0,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",Trust,Excel,1,92,96
0,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",Trust,excel,1,386,390
0,52857016,"""I have a bunch of images similar to this:And I need to extract the data and store it in an Excel sheet. I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?""",Anticipation,ordering,1,260,267
0,41166264,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""",Disgust,will not work,1,113,125
0,41166264,"""I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message:.I've tried multiple images (), but none work.My code is:Am I encoding it correctly? Theare quite vague.I've found SO questions about 'No Image Content', but none about invalid format.Any ideas? Thanks!""",Sadness,invalid,1,318,324
0,40136543,"""IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's""",Sadness,unable,1,102,107
0,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Sadness,Invalid,1,209,215
0,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Sadness,Unable,1,240,245
0,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Sadness,INVALID,1,302,308
0,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Fear,command,2,53,59
1,35823073,"""I was working with google Vision API.When I curl in command line it gives me status 200 OK with the following command:But when I use it with PHP, I get an return message:{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }I was following this example:""",Fear,command,2,111,117
0,54061460,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""",Fear,missing,2,248,254
1,54061460,"""I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing.In the console, all I can see isLogginggivesAm I missing something?""",Fear,missing,2,305,311
0,52326351,"""rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help""",Disgust,is not working,1,73,86
0,51479671,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""",Anticipation,in,1,665,666
0,51479671,"""Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.Below is all the code (apologies for the messy code - still learning). Further below that is the output from the tests I'm running in Lambda.Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).Thanks in advance!!!Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:""",Trust,to label,1,56,63
0,35748095,"""Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ? For unfamiliar ppl via some learning/look-alike mechanism ?Thanks.""",Surprise,unfamiliar,1,231,240
0,41803160,"""I am looking for a Google Cloud API that can do both face recognition and identification. My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.Is there any Google Cloud API that can do face recognition?""",Trust,will support,1,144,155
0,48448138,"""do I understand the Documentation right and it's not possible to use the Amazon Rekognition service with ""external"" links without priorly save the images on Amazon S3 or another local server?Why simple web hosted images are not supported?Thanksffp""",Disgust,are not supported,1,221,237
0,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Surprise,randomly,1,85,92
0,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Surprise,unpredictable,1,212,224
0,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Surprise,very unpredictable,1,207,224
0,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Joy,gem,2,34,36
1,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Joy,gem,2,245,247
0,44783626,"""I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.""",Sadness,fail,1,94,97
0,45720763,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""",Trust,accurate,1,678,685
0,45720763,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""",Trust,extremely accurate,1,668,685
0,45720763,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""",Anticipation,results,1,331,337
0,45720763,"""I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url:Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It'sexactlywhat I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.""",Surprise,have not been inspiring,1,361,383
0,51609428,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",Joy,'d like,1,2,8
0,51609428,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",Joy,would like,2,97,106
1,51609428,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",Joy,would like,2,460,469
0,51609428,"""I'd like to know whether we can useAmazon S3andMicrosoft Face APItogether. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something calledPersonGrouporLargePersonGroupwhich are the list of known people. This needs to be initialized at the start and has a capacity of1,000,000, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key.Any suggestions?""",Anticipation,preliminary,1,168,178
0,49167306,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""",Anticipation,would be appreciated,1,586,605
0,49167306,"""I am using Google'sfrom, andfrom.They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.So I have a forward proxy xx.xx.xx.xx, and all requests likeget forwarded to. I tested this with some curl requests and they way work correctly.I have changed the endpoint as follows:However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.""",Joy,would be appreciated,1,586,605
0,49249692,"""I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.Here is my relevant code:This code worked with Python2.7 but is failing with Python3. I am getting the following error:Any ideas what I need to change here.""",Sadness,is failing,1,152,161
0,49126860,"""I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.Stacktrace:""",Joy,will be appreciated,1,97,122
0,49126860,"""I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.Stacktrace:""",Joy,highly will be appreciated,1,105,122
0,50722472,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""",Anticipation,In,1,357,358
0,50722472,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""",Anticipation,I am wondering,1,268,281
0,50722472,"""I have aofFace. I usedto findEyes Landmark. Now I want totheso that theEyeswill be in focus andit accordigly.What will be the easiest possible way to do this than usingI tried this but not getting proper resultI read the documentation and findaccept these parameters:I am wondering how should I implementor is there any way so I can convertinto??Thank You In Advance""",Trust,Landmark,1,35,42
0,48115069,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""",Joy,successfully,1,57,68
0,48115069,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""",Joy,great help,1,200,209
0,48115069,"""I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!The image that I am using is thisDetailed Error:""",Surprise,suddenly,1,108,115
0,50894208,"""I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream.I calculate frame timestamp accordignly:by adding theandfield values together and get timestamps with defference 1 second.For instance:I use demo app for video streaming from Java Producer SDKTotal duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.""",Anticipation,results,1,213,219
0,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",Disgust,helpful,1,424,430
0,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",Disgust,really helpful,1,417,430
0,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",Joy,would like,2,42,51
1,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",Joy,would like,2,227,236
0,56126281,"""I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)Viaand Tensorboard I already looked visually at the network layers and printed the names of the network as described. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.""",Anticipation,training,1,617,624
0,44093669,"""I'm new in Google API.Recently, I use the Google Vision API but I met the following problem:And I tried the solution about ""Create the service accout"" to generate the service json key and invoke it in py.script,it will work first in almost 3~4 url, but it will error in next url.This is my detect code:And I invoke the ""detect.py"" in another py.script:""",Anticipation,invoke,2,189,194
1,44093669,"""I'm new in Google API.Recently, I use the Google Vision API but I met the following problem:And I tried the solution about ""Create the service accout"" to generate the service json key and invoke it in py.script,it will work first in almost 3~4 url, but it will error in next url.This is my detect code:And I invoke the ""detect.py"" in another py.script:""",Anticipation,invoke,2,309,314
0,42188322,"""mention AgeRange in the response of detect_faces.But, using the Python SDK (boto3), I cannot see it in the response.Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?""",Fear,missing,1,122,128
0,49231034,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",Anticipation,in,1,306,307
0,49231034,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",Anticipation,results,1,188,194
0,49231034,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",Anticipation,result,1,269,274
0,49231034,"""I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...And based from thisit uses an sdk from microsoft to get results. However, I want to build an android app.How am I be able to get the tag result and its prediction? Thank you in advance.""",Anticipation,prediction,1,284,293
0,49912384,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""",Joy,excellent,1,567,575
0,49912384,"""I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.""",Joy,like,1,435,438
0,48733647,"""I am working a Xamarin.Forms App that uses the Azure Face API. With this API you retrieve a JSON response (See Below).I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.This is how I set the JSON data to a string.""",Joy,would like,1,284,293
0,51561234,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""",Sadness,crash,2,373,377
1,51561234,"""there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.I am initializing the AWS ""stack"" with no problems also in App DelegateI get a crash in my ViewController :The crash shows this:From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...Any ideas?""",Sadness,crash,2,405,409
0,55047291,"""I am developing an Android app to detect text from the PDF file.First, I tried to use Google Cloud Vision API.But it required to OAuth 2.0.So I changed from it to Firebase ML Kit.But when I run 'fromFilePath' method, NPE occurred.It looks like the Firebase ML kit doesn't support PDF file, right?Is there any good solution?Is it impossible to recognize text from the PDF file using Firebase ML kit?I tried to test more file formats: JPG, TIFFAll is same, just input file is changed.JPG works fine, but TIFF has the same problem.""",Disgust,doesn't support,1,265,279
0,47749413,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",Trust,labels,1,285,290
0,47749413,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",Trust,label,1,322,326
0,47749413,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",Trust,accurate,1,61,68
0,47749413,"""I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing.""",Trust,very accurate,1,56,68
0,55894824,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",Fear,missing,1,579,585
0,55894824,"""I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:Can I make calls to the analysis API using an image from memory? If so, how?EDIT:This is essentially what I'm trying to do:Thanks""",Trust,'s managing,1,110,120
0,44184869,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""",Trust,enabled,1,75,81
0,44184869,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""",Trust,instructions,1,183,194
0,44184869,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""",Anticipation,instructions,1,183,194
0,44184869,"""I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.I then copied the code fromand followed the README instructions.However, when I try to runI get this error message:Why is it pointing toand not to myproject?If II can see the correct project and service account. Baffled.""",Surprise,Baffled,1,344,350
0,56051643,"""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine""",Anticipation,am attempting,1,3,15
0,56051643,"""I am attempting the following tutorialSo I replaced some part with my own bucket and key(file) name:(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?I have made sure to set the bucket and object public etc but I keep getting an error:I also tried to access my bucket using:and I am able to display the content fine""",Sadness,:(,1,100,101
0,40281166,"""Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.This is how I am fetching the credentials, from the examples that the documentation provided. Is there something wrong?""",Trust,credentials,1,149,159
0,49752955,"""I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?This is my CameraSource BuilderHere I have to try to access/get the camera from mCameraSource object.but thereturns null only, And My 2nd Question is how to do brightness option...""",Joy,could achieve,1,224,238
0,48077116,"""I use Google Cloud Vision Document OCR API. The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?""",Anticipation,resulted,2,49,56
1,48077116,"""I use Google Cloud Vision Document OCR API. The resulted text that is returned byis a little bit messy and lose the text formatting presented on the original image.Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?""",Anticipation,resulted,2,260,267
0,46693204,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""",Joy,successfully,1,619,630
0,46693204,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""",Joy,successful,1,451,460
0,46693204,"""We have a project where we are scanning the front and back of a Driver's License for information.We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because ofreasons. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations.Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information.Using this code:This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server.There is plenty of information on client-side ways of doing this, IE:1)2)But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc).Does anyone have any ideas as to how this can be done server-side?""",Anticipation,attempt,1,542,548
0,46760602,"""I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it saysOn the Emotion API explanatory page it says that Response 202 means thatThen there is, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.I have tried to call the API with Python using at least three code versions that I found on the Internet that all amount to the same, I found the code here :python-upload-video-from-memoryNote that changingtodoesn't help.However, afterwards I wait and run every half an hour:The outcome has been 'Running' for hours and my video is only about half an hour long.Here is what my Testing Console looks likeHere I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage examplethat uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrongHere the code:There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?""",Trust,manage,1,8,13
0,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",Anticipation,am suspecting,1,380,392
0,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",Anticipation,call,1,148,151
0,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",Anticipation,for,1,153,155
0,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",Joy,appreciated,1,644,654
0,42125608,"""I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.Any help appreciated""",Trust,to enable,1,200,208
0,48445901,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""",Anger,silly,1,287,291
0,48445901,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""",Disgust,silly,1,287,291
0,48445901,"""I am using the Google Vision API to flag adult images uploaded my application.I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.""",Joy,would like,1,81,90
0,38048320,"""I am working on the python example for Cloud Vision API from.I have already setup the project and activated the service account with its key. I have also called theand entered my credentials.Here is my code (as derived from the python example of Vision API text detection):This is the error message I am getting:I want to be able to use my own project for the example and not the default (google.com:cloudsdktool).""",Trust,credentials,1,180,190
0,41126381,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""",Joy,successfully,1,376,387
0,41126381,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""",Joy,gem,1,363,365
0,41126381,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""",Anticipation,am attempting,1,62,74
0,41126381,"""How do I upload an S3 URL image properly for Google Vision?I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in thelisted below:I am using the.I have tried, with a minor modification:I have tried images at AWS URLs and images at other urls.Every time I get this error from the Google-Cloud-Vision gem:Update - successfully encoded and decoded image in ruby onlyI have confirmed that this code:works via the following:So what's google's problem with this? I am properly encoded.""",Trust,have confirmed,1,429,442
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,escaped,3,208,214
1,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,escaped,3,328,334
2,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,escaped,3,502,508
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,'s escaped,1,266,279
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,Escaped,1,419,425
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Anticipation,Non-Escaped,1,445,455
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,escaped,3,208,214
1,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,escaped,3,328,334
2,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,escaped,3,502,508
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,'s escaped,1,266,279
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,Escaped,1,419,425
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Fear,Non-Escaped,1,445,455
0,47574353,"""We have been struggling with posting images for recognition through the Google Vision API for some days now..We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format)... cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.POST :Escaped JSON (not working)Non-Escaped JSON (working) :Edited : Screenshot of json (escaped json)""",Trust,valid,1,282,286
0,54439669,"""I'm doing a system in C # that needs to parse an image of a keyboard returning the position of the characters in it.I tried to use IBM Watson but it does not return the position of the classifications, after that I tried to use Google Cloud Vision because in the site demo it returns the positions of the characters in JSON format, however I had problems with GOOGLE_APPLICATION_CREDENTIALS (look).I would like to know if there is any other alternative, preferably free or with a lot of free access, to do this kind of reading of the image and return the position of the characters?I do not need OCR I want to return the position of the character in the image""",Joy,would like,1,401,410
0,54439669,"""I'm doing a system in C # that needs to parse an image of a keyboard returning the position of the characters in it.I tried to use IBM Watson but it does not return the position of the classifications, after that I tried to use Google Cloud Vision because in the site demo it returns the positions of the characters in JSON format, however I had problems with GOOGLE_APPLICATION_CREDENTIALS (look).I would like to know if there is any other alternative, preferably free or with a lot of free access, to do this kind of reading of the image and return the position of the characters?I do not need OCR I want to return the position of the character in the image""",Trust,CREDENTIALS,1,380,390
0,44543092,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""",Trust,enable,1,305,310
0,44543092,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""",Trust,Community,1,197,205
0,44543092,"""I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. For that, I'm using the Google Vision API with Visual Studios Community 2017.I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.I wrote this code :This code opens me my WindowsForm, looking like this :But when I run the application ( No errors ), and clic on the button, nothing happens !And I can't fix my error ...Any ideas ?Thanks a lot !""",Fear,command,1,475,481
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Anticipation,training,1,746,753
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Anticipation,would be appreciated,1,624,643
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Trust,certain,1,65,71
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Trust,support,1,545,551
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Joy,would be appreciated,1,624,643
0,53886444,"""I am building an OCR based solution to extract information from certain financial documents. As per the regulation in my country (India), this data cannot leave India.Is it possible to find the region where Google Cloud Vision servers are located?Alternately, is it possible to restrict the serving region from the GCP console?This is what I have tried:I went through GCP Data Usage FAQ:GCP Terms of Service:(Look at point 1.4 Data Location on this page)Talking to the GCP Sales rep. He did not know the answer.I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.PS - Edited to make things I have tried clearer.""",Sadness,:(,1,408,409
0,50553795,"""I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.""",Anticipation,to subscribe,1,495,506
0,53117918,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Anticipation,result,2,914,919
1,53117918,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Anticipation,result,2,985,990
0,53117918,"""Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android & iOS. Its sort of a facial recognition app. When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.Here is the code I am using to list collections on android:This is the log result:This is the python code I using to list collections:This is the result:That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.The reason I think iOS app is fine because the collections listed by both iOS and python are same. Is there anything I need to change? Is there any additional setup I need to do to make it work?Please let me know. Thanks.""",Anger,dumb,1,81,84
0,36125830,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""",Anticipation,result,2,148,153
1,36125830,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""",Anticipation,result,2,173,178
0,36125830,"""Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.I cannot find any result or hint on the internet at all. Any help is appreciated.""",Joy,is appreciated,1,221,234
0,39551502,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",Anticipation,expected,1,121,128
0,39551502,"""I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back aas expected. However, I try to transition that to Python code and it's currently giving me this error:I read thisbut it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the headerbut it's not working. Any ideas?""",Trust,labeling,1,339,346
0,52308804,"""I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...An example:Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)""",Trust,valid,1,353,357
0,54365930,"""i'm studying the Azure Custom Vision service for object detection, but I would like also to extrapolate text information within a tagged image zone.Is it possible with Custom Vision?If not, is it in the service roadmap?Thank you""",Joy,would like,1,74,83
0,52397039,"""I have a large set of difficult words to recognize through    .The image are all of lowercase latin alphabet without diatrics, but despite using languagehints, I end up with results likein a lot of cases.So how to force Google cloud vision to use a specific set of letters and not just hint ?""",Anticipation,results,1,175,181
0,55568129,"""I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly. How to use this API more appropriately so that I can extract MICR accurately.""",Surprise,results,1,117,123
0,54306129,"""I'm trying the Rekognition API from aws in my new Android app in kotlin, but when I try to create the client, my app crash.I put the json file in the raw folder.This is my code:""",Sadness,crash,1,118,122
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Trust,label,1,163,167
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Trust,familiar,1,513,520
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Trust,credentials,2,356,366
1,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Trust,credentials,2,402,412
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Anticipation,am attempting,1,3,15
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Anticipation,attempt,1,98,104
0,51109673,"""I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receivingwhen I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account).My CodeWhen I run this code I getMy Question(s)Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?""",Anticipation,attempting,1,210,219
0,53259815,"""I use Google Cloud Vision API with the Go SDK.In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,How can I get that JSON from annotation structure? Is it possible?""",Anticipation,results,1,111,117
0,42984821,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",Anger,complains,1,296,304
0,42984821,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",Joy,is,1,371,372
0,42984821,"""I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.The following code is good when opening an image from file:Here a file_stream is used to open the file.I tried serializing my Mat like this:This serialization works as I can decode if after and rebuild the image. How can I send to server the opencv Mat image through the client?""",Joy,good,1,374,377
0,44832036,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",Anticipation,result,1,276,281
0,44832036,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",Anticipation,attempt,1,593,599
0,44832036,"""App Engine gives the error:when I make call to Google Vision API inside Callable in async Servlet.How to make it working?servlet:Full stack trace:API call makes the error:I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.How can I do this?Thank you for any help.UPDATE:I tried toas mentioned in error message but it gives the same error. Here is my updated servlet:Next test passed OK:UPDATE 2:(Reply to the proposing do staff in request's thread.)Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.I have the same error if I don't use multithreading in my app:code of servlet - no multithreading:next test passed ok:It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices.""",Sadness,:(,1,475,476
0,46061561,"""I am trying to do translate a document with google translate from the package google.cloudI already did:and the result was:then I called the package in Spyder (Python 3.5):I obtained this error:""",Anticipation,result,1,113,118
0,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Anticipation,instructions,2,19,30
1,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Anticipation,instructions,2,186,197
0,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Anticipation,result,1,623,628
0,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Trust,instructions,2,19,30
1,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Trust,instructions,2,186,197
0,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Trust,credentials,1,324,334
0,44804442,"""I've followed the instructions thefor the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:Added google-cloud-vision and google-cloud libraries to my requirements.txt file.Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALSran gcloud init successfully while ""ssh'd"" into my web app's docker containerCopied the client library example in the link above exactly into a test view to run the codeThe steps above result in the error below:I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!""",Joy,successfully,1,454,465
0,43910100,"""It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. My code below use to work but know it says there is no ""images founds"" .What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ?""",Trust,to associate,1,33,44
0,43844506,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Anticipation,Result,2,533,538
1,43844506,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Anticipation,Result,2,860,865
0,43844506,"""I've used google cloud vision OCR to extract business card email string text fromand used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799""Result from the running the above code = NULL; Desired output: philip@algen.comsgOCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg""Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;""",Surprise,results,1,160,166
0,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Anticipation,result,3,239,244
1,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Anticipation,result,3,779,784
2,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Anticipation,result,3,1140,1145
0,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Anticipation,expects,1,527,533
0,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Joy,:b,2,1146,1147
1,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Joy,:b,2,1252,1253
0,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Disgust,does not work,1,47,59
0,38322210,"""The adapter worked before and now the adapter does not work.Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.The.java serversend the image toalchemyand just returns thealchemyxml result to MobileFirst adapter.The Debug information from Android and iOS are different, so I list both.a)iOS MobileClient:The information on the iOS is different from android.It seems there could be abugin the cordova mfp-pluging implementation forXMLadapter:Because it seems the adapter expects a""responseJSON""in the response json structure, this is what I think, based on the debug information..The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.Just take a look here in thexCode Debugoutput:Reason the it samesb)Android MobileClient:I get followingerrorwhen I execute the javascript adapter  XML from a cordova mobileandroidclient.The full error information in the chrome console for theandroid app:When I inspect the XML on the server log or in a poster, I don t see any  information.a) poster xml result:b) .java server logs in bluemix:The implementation of the Apdater and the MobileClient:a)The adapter code:b)The cordova client code:""",Trust,valid,1,749,753
0,51145859,"""When an image gets captured it defaults to left orientation. So when you feed it into theinside the, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.I have tried to recreate the Image with a new orientation and that won't change it.Does anyone know what to do?I have tried all of these suggestions""",Trust,to support,1,206,215
0,55921253,"""i am using react-native-camera module to take pictures but i want to set the camera brightness via a slider, does this module support brightness settings like the one in samsung's native camera appmy current configurations are:""",Trust,support,1,127,133
0,48709133,"""I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :This is the JSON response I'm getting:How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?""",Trust,certain,1,43,49
0,45849380,"""Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:I grab a full http path to my file, which is valid, but when I:I get the error:I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?""",Trust,valid,1,145,149
0,36944481,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""",Sadness,'s failing,1,227,236
0,36944481,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""",Sadness,failed,1,540,545
0,36944481,"""I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.Below is my code, CURL error #7 is returned, failed to connect to host.""",Trust,enabled,1,268,274
0,54351470,"""I'm trying to get bounding box from an image in Rekognition, i get the label but i get:""",Trust,label,1,72,76
0,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Anticipation,to predict,1,25,34
0,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Anticipation,predict,2,211,217
1,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Anticipation,predict,2,415,421
0,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Anticipation,results,2,344,350
1,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Anticipation,results,2,400,406
0,55892455,"""I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.The region_name is.It seems like all fine, there is no error in the predict proccess, but the response i get is:I not understand why the container is empty, what should be in the container, and how my results should be shown.Anyone know how to get the real results of the predict proccess?Thanks.""",Trust,labels,1,42,47
0,50785134,"""I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:""",Trust,are supported,1,72,84
0,33482245,"""I am trying to develop a face tracking app using the Google Vision API ()This is my manifest:This is my code:This is the error in Logcat:Why does this happen (...on an Xperia Z3 compact 5.1)?UPDATE:I spotted a new error. I think it might be the reason why my code is not working.How can I resolve this problem?""",Disgust,is not working,1,265,278
0,50780962,"""I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.Here is my python code:document 1 output:document 2 output :Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data.Thanks in advance.""",Anticipation,in,1,683,684
0,55252358,"""I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:As of now, it seems to be producing this error:Could someone help me with this issue? This is my first time using Google Cloud Vision.""",Trust,certain,1,200,206
0,55950028,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""",Trust,is enabled,1,271,280
0,55950028,"""I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabledI've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file.I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.""",Trust,CREDENTIALS,1,381,391
0,54131993,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""",Anticipation,result,1,179,184
0,54131993,"""Let's say I have the following document:I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Canbe used somehow?""",Joy,would like,1,136,145
0,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Anticipation,result,2,378,383
1,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Anticipation,result,2,405,410
0,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Anticipation,expected,1,428,435
0,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Joy,like,2,101,104
1,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Joy,like,2,391,394
0,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Surprise,'m exploring,1,2,13
0,38679651,"""I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like 'black-dog' (i.e. black-dog_positive_example),  this classname is later returned as 'black_dog' (withunderscorereplacingdash) when I classify an image using theendpoint.But when I retrieve the classifier details with thethe class is correctly listed as 'black-dog'.So, my result foris like:While my result forisSo is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?""",Trust,rules,1,511,515
0,40512241,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",Anticipation,ready,1,128,132
0,40512241,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",Anticipation,result,1,321,326
0,40512241,"""I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status:I am executing the following curl command to test this classifier:and the paintings.json file has the following content:Running this query returns the following result:Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here:which states that the JSON parameters are:classifier_ids- An array of classifier IDs to classify the images against.owners- An array with the value(s) ""IBM"" and/or ""me"" to specify which classifiers to run.threshold- A floating point value that specifies the minimum score a class must have to be displayed in the response.""",Joy,successfully,1,54,65
0,41562347,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""",Anticipation,results,2,152,158
1,41562347,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""",Anticipation,results,2,425,431
0,41562347,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""",Anticipation,Results,1,452,458
0,41562347,"""I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts.Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's?Here's an image with both results and the receiptOcr Results""",Joy,can achieve,1,353,363
0,45376533,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""",Anticipation,I 'm wondering,1,4,16
0,45376533,"""So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?I already know the answer to (A), as found here:And I even found an example that uses Amazon Rekognition, as found here:Only nuisance with the rekognition example I found was that it seems to take the pictureAND THENdo the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).Disclaimer: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve.Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close.""",Joy,to achieve,1,855,864
0,41388926,"""I'd like to try, but I don't see a full example of the syntax for using the HTTP API. Assuming I have two images, how would I call this API from Python to retrieve a similarity score?""",Joy,'d like,1,2,8
0,38280779,"""I'm working on an OCR android application using Google Cloud Vision APIFor testing I've used the sample application which is provided by GoogleI've tested it for type ""LABEL_DETECTION"" and it works fineI've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""I've tested it using this image and it return ""nothing"" result[ocr_image]Appreciate if anyone knows what is the issueThanks in advance""",Anticipation,in,1,426,427
0,38280779,"""I'm working on an OCR android application using Google Cloud Vision APIFor testing I've used the sample application which is provided by GoogleI've tested it for type ""LABEL_DETECTION"" and it works fineI've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""I've tested it using this image and it return ""nothing"" result[ocr_image]Appreciate if anyone knows what is the issueThanks in advance""",Anticipation,result,1,358,363
0,38280779,"""I'm working on an OCR android application using Google Cloud Vision APIFor testing I've used the sample application which is provided by GoogleI've tested it for type ""LABEL_DETECTION"" and it works fineI've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""I've tested it using this image and it return ""nothing"" result[ocr_image]Appreciate if anyone knows what is the issueThanks in advance""",Trust,LABEL,2,169,173
1,38280779,"""I'm working on an OCR android application using Google Cloud Vision APIFor testing I've used the sample application which is provided by GoogleI've tested it for type ""LABEL_DETECTION"" and it works fineI've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""I've tested it using this image and it return ""nothing"" result[ocr_image]Appreciate if anyone knows what is the issueThanks in advance""",Trust,LABEL,2,286,290
0,50829647,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""",Trust,labels,1,23,28
0,50829647,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""",Trust,labelled,1,140,147
0,50829647,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""",Trust,can label,1,169,177
0,50829647,"""I am trying to detect labels of multiple images using AWS Rekognition in Python.This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel?Since I have restrained using boto3 sessions, please provide the code snippet, if possible.""",Anger,have restrained,1,212,226
0,53938721,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""",Disgust,is not working,1,523,536
0,53938721,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""",Joy,happy,1,266,270
0,53938721,"""I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me:and I am able to get the estimated age.(is an array of the type)However,when I try to get the probability that the face is happy, I am running into the following error:This is how I am getting the probability that the person ishappy:Similarly, when I try:, it returns.I know thatis notbecause it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?Update:For those who are experiencing the same problem, in thewhere you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I hadand that was why it was giving me an error when trying to determine emotions. The following code goes in themethod:""",Sadness,unable,1,495,500
0,55617828,"""I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobufFollowingHere's my code so far:""",Trust,efficient,1,136,144
0,54015677,"""I am using the following code for Rekognition.It is taking almost 1 min to finish the face detection between the 2 images.Is this normal? I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrongThank you""",Anticipation,I am wondering,1,162,175
0,45366479,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""",Anticipation,training,2,620,627
1,45366479,"""When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:What I have done so far:Removed all special characters in the file names as I thought that might be the problem:Tried to give other names for the classifeir, e.g. ""name=ocd""I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations:I have a free subscription.Do anyone has any recommendations for how to solve this classifier training problem?""",Anticipation,training,2,770,777
0,42631231,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""",Anticipation,results,2,215,221
1,42631231,"""I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.Is there any google reverse image search api for android? weather free or non-freeI also foundbut the results - at least in it's- is not as specific as google reverse image search.so is there any other api for reverse image search in android as powerful as google?or is there any other method to use google reverse image search results inside the android app? I foundbut seems google has detected and closed that server.""",Anticipation,results,2,441,447
0,47924385,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""",Surprise,surprising,1,883,892
0,47924385,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""",Surprise,quite surprising,1,877,892
0,47924385,"""I am working with Google Vision API and Python to applywhich is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:I have used the following different algorithms:1) Applyto the original image2) Enlarge the original image by 3 times and then apply3) Apply,,on a mask (with OpenCV) and thento this4) Enlarge the original image by 3 times, apply,,on a mask (with) and thento this5) Sharpen the original image and then apply6) Enlarge the original image by 3 times, sharpen the image and then applyThe ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.The major problem is thatdoes not detect in most cases the minus sign especially the one of '-1.00'.Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.What do you suggest me to do to detect accurately the minus sign and in general the numbers?(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)""",Trust,the best,1,601,608
0,49293605,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""",Anticipation,soon,1,646,649
0,49293605,"""I have boto client like thisI am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is givingWhy it is picking ap-south-1 as i provided in client-""us-east-1""client = boto3.client('rekognition', region_name=""us-east-1"")But when I run the code locally with region-name:- ap-south-1 and in clientits running wonderfullybut not running on AWS lambdaWhile successfully running when both the regions are same(us-east-1)So great if anyone can provide any suggestion, Required Help soon!!!!!!!""",Joy,successfully,1,523,534
0,46580171,"""By that I mean:I can opt-out of using the images for training, orMy users can delete their images (if they are used for training by default).My reading of the Google Vision terms indicate that they are completely non-compliant.""",Anticipation,training,2,54,61
1,46580171,"""By that I mean:I can opt-out of using the images for training, orMy users can delete their images (if they are used for training by default).My reading of the Google Vision terms indicate that they are completely non-compliant.""",Anticipation,training,2,121,128
0,47885650,"""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?""",Anticipation,to prevent,1,296,305
0,47885650,"""I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (), whenever my program calls UploadAndDetectFaces:I also declared the keys to the endpoint:an error returns:Does anyone know what's wrong or any changes required to prevent the error?""",Trust,guide,1,122,126
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,instructions,1,1038,1049
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,ultimately,1,1495,1504
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,Attempted,1,744,752
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,likely,1,1214,1219
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,most likely,1,1209,1219
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,called,1,600,605
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Anticipation,for,1,607,609
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Trust,instructions,1,1038,1049
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Trust,the best,1,350,357
0,40077320,"""Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories.was far and away the best for this use case because of its fast native character recognition.After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).Attempted Workarounds:- works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using theUnity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.Following instructions fromto use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error(most likely an issue with the version of .Net the dlls were compiled in).Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.Has anyone run into this same problem in Unity and ultimately found a good solution?""",Disgust,does not support,1,671,686
0,50417917,"""I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS). Is internet connection required to use Amazon rekognition?""",Joy,would like,1,3,12
0,45727079,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""",Trust,CREDENTIALS,2,304,314
1,45727079,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""",Trust,CREDENTIALS,2,466,476
0,45727079,"""I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:My code is the following:Controller:SafeSearchDetection.isSafe(int idUser):detectSafeSearch(String path):""",Fear,command,1,333,339
0,47671289,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""",Disgust,doesn't support,1,150,164
0,47671289,"""I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?""",Trust,the best,1,78,85
0,37718233,"""i have created a method - (IBAction)clickToGenerateEmotion:(id)senderThanks in advance!!!""",Anticipation,in,1,77,78
0,37718233,"""i have created a method - (IBAction)clickToGenerateEmotion:(id)senderThanks in advance!!!""",Sadness,:(,1,59,60
0,49589780,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""",Anticipation,attempt,1,122,128
0,49589780,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""",Fear,command,1,29,35
0,49589780,"""I need to convert this cURL command in PHP to use it on my site in WordPress.Parameters object that I'm using:This is my attempt:This is the error:This is the JSON I get as a return:I want use my custom model of IBM Watson Visual Recognition.I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.Using WordPressVersion: 9.4.4Plugin:I am using the following links to guide me:Remember that I am not installing any library or using Composer.""",Trust,to guide,1,415,422
0,49657006,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Joy,would love,1,29,38
0,49657006,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Joy,will be appreciated,1,221,244
0,49657006,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Joy,much will be appreciated,1,229,244
0,49657006,"""Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.The data set:The desired format is this:It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.Thank you so much.""",Trust,would love,1,29,38
0,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",Trust,labels,1,307,312
0,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",Trust,fact,1,675,678
0,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",Anticipation,result,1,542,547
0,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",Fear,command,1,370,376
0,44652637,"""So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.Here is my code:Here is the it gives me error:There is another error that says something about the text to speech but I think that is the result of this error.I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error.""",Sadness,crashes,1,396,402
0,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",Joy,enlighten,1,677,685
0,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",Joy,was,1,218,220
0,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",Joy,good,1,226,229
0,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",Disgust,is not working,1,589,602
0,53735551,"""I am currently trying to use Google Cloud Vision API on C#.After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.However,the code is not working to properly authenticating my JSON file to call Google Vision API.Please enlighten me with your knowledge! Thanks.Here is my code.""",Trust,authenticating,1,616,629
0,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",Joy,gem,2,38,40
1,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",Joy,gem,2,79,81
0,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",Joy,gems,2,162,165
1,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",Joy,gems,2,199,202
0,47639286,"""Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies!Is there a better way to configure AWS gems for Paperclip?These are all the gems installed when you use aws-sdk:""",Disgust,ridiculous,1,89,98
0,50739245,"""hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name mybossso In new database entry will beimage1   mybossimage4   mybossand for other bosses as well same case . ATM using this""",Joy,like,1,198,201
0,38869448,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""",Trust,'m dealing,1,96,105
0,38869448,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""",Trust,label,1,386,390
0,38869448,"""I've been aroundGoogle Vision APIbut I have a problem I can't really solve. This is the image I'm dealing with:In the image above,Google Vision API(also happens withIBM (Watson)andMicrosft (Cognitive Services)) does not understand that 2,99  is something to read because it is not treated as a single line, so the output is all but what I expect him to do (understand the price of the label).If I was using Tesseract, I would solve this by using theoption in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.Has anyone done something similar before? I cannot figure out how to solve this problem...""",Anticipation,expect,1,340,345
0,46090578,"""Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects.I've also tried using. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings withbut withinstead. Sorry andthank youall for taking the time to help me out.""",Trust,managed,1,570,576
0,51935317,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""",Trust,efficient,1,31,39
0,51935317,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""",Trust,accurate,1,18,25
0,51935317,"""Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.Please any help that could be rendered will be appreciated.""",Joy,will be appreciated,1,267,285
0,49527406,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""",Joy,would like,1,115,124
0,49527406,"""I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?Thank you!""",Trust,community,1,62,70
0,53464278,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Joy,would be appreciated,1,616,643
0,53464278,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Joy,greatly would be appreciated,1,625,643
0,53464278,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Anticipation,would be appreciated,1,616,643
0,53464278,"""`import boto3This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.""",Sadness,failed,1,314,319
0,54996104,"""We are doingand we do have a requirement to validate text and graph in images.I triedbut it is not giving a consistent result.I came to know about, but don't know how to set it up with Eclipse Java.Can someone tell me,Is it paid?How to set it up with the existing selenium maven project.Any other accurate alternative for validating images?Thanks,Nilesh""",Anticipation,result,1,120,125
0,54996104,"""We are doingand we do have a requirement to validate text and graph in images.I triedbut it is not giving a consistent result.I came to know about, but don't know how to set it up with Eclipse Java.Can someone tell me,Is it paid?How to set it up with the existing selenium maven project.Any other accurate alternative for validating images?Thanks,Nilesh""",Trust,accurate,1,298,305
0,56326059,"""I want to extract text from the image and using google vision API but getting an error ""str object has no attribute batch annotate images"".The error showing to me is the followingI will be good if you could provide me with some source link where I could learn more about the GOOGLE API""",Joy,will be,1,182,188
0,56326059,"""I want to extract text from the image and using google vision API but getting an error ""str object has no attribute batch annotate images"".The error showing to me is the followingI will be good if you could provide me with some source link where I could learn more about the GOOGLE API""",Joy,good,1,190,193
0,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Anticipation,in,1,348,349
0,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Anticipation,Results,3,173,179
1,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Anticipation,Results,3,201,207
2,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Anticipation,Results,3,424,430
0,47297666,"""I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.Amazon Rekognition Results:Google Cloud Vision Results:Here's the 2 python files that I'm using:amazon-detect.pyglabel.pyHow can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!edit 1:I'm now using the cURL api for Cloud Vision. Results are much better!edit 2:For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:""",Trust,label,1,116,120
0,53658135,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",Joy,will be appreciated,1,676,701
0,53658135,"""I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.Exception:Unhandled Exception:System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.Code:So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?Any help will be highly appreciated.Thanks,Ghalib""",Joy,highly will be appreciated,1,684,701
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Joy,notable,1,134,140
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Joy,are appreciated,1,1292,1311
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Joy,much are appreciated,1,1296,1311
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Anticipation,'d expect,1,929,937
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Disgust,reliable,1,1035,1042
0,46752575,"""I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any).Gapic FlavorgRPC/protocol flavor:To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:andalso, theAPI client comes only in gRPCis the opposite.which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""YMMV"" and ""choose the tool that will do the job"" are assumed.documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.Your insights are much appreciated.""",Trust,label,1,1272,1276
0,40372942,"""I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.Here is an example: this sample pageHas a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...I tried:Twice the resolution (2048 x 1536)BMP 24-bitBMP 32-bitAll of the above, in grayscaleAll of the above, inverted (black background and white letters)No luck ...Any hint on why this is happening? Is it the API or my image format could use some formatting?""",Trust,the best,1,326,333
0,48529675,"""I want to create a barcode scanner that uses full screen.I used google's vision API samples that can be foundThis is my result:I want to make a preview of the camera to the full height, how can I do it?""",Anticipation,result,1,121,126
0,41903995,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""",Joy,will be appreciated,1,416,442
0,41903995,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""",Joy,greatly will be appreciated,1,424,442
0,41903995,"""I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service.The error i am getting isThe following code is running on an Express server.In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson.The error:Any help will be greatly appreciated.Thanks""",Anger,lies,1,304,307
0,44465669,"""I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:which is marked as Error 400. Bad request on my console.This is thepage on how to use this request:Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file.""",Joy,successful,1,462,471
0,56406581,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""",Anticipation,expected,1,344,351
0,56406581,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""",Joy,achieve,1,208,214
0,56406581,"""Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)Please suggest how can i achieve thisI tried microsoft computer vision OCR services but unable to integrate the codeNeed to automate this taskFind the Input and expected Output file""",Sadness,unable,1,271,276
0,43302771,"""I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation.The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?""",Disgust,doesn't work,1,420,431
0,52475518,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""",Joy,successfully,1,270,281
0,52475518,"""I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Visionpage, where it says TYR THIS API.""",Trust,label,1,91,95
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,'m managing,1,79,89
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,438,442
1,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,487,491
2,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,644,648
3,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,744,748
4,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,786,790
5,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,valid,6,951,955
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,team,1,1044,1047
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Trust,credentials,1,1129,1139
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Anticipation,immediately,2,276,286
1,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Anticipation,immediately,2,1253,1263
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Joy,is,1,1272,1273
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Joy,good,1,1275,1278
0,44166894,"""I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy usingwith a smoke test.One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up.Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong.Is there an option I've missed?""",Sadness,fails,1,461,465
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Joy,would be appreciated,1,578,605
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Joy,greatly would be appreciated,1,587,605
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Joy,like,1,126,129
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Anticipation,result,1,101,106
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Anticipation,would be appreciated,1,578,605
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Trust,managed,2,289,295
1,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Trust,managed,2,332,338
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Fear,afraid,1,487,492
0,52172303,"""So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid.Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online.Any help would be greatly appreciated.My current code:(yes it's a mess and not very solid)""",Sadness,:(,1,622,623
0,44856326,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",Anticipation,result,1,173,178
0,44856326,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",Fear,missing,1,156,162
0,44856326,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",Joy,like,1,370,373
0,44856326,"""I'm trying to use the Google Cloud Vision API to OCR this image:I'm using the following code the make the request:This works but there is some information missing from the result. If we look at thefield:Here's that visualized:There are boxes around the characters which were recognized. But, if we put this image into the gcv, we get this instead:And this is whatlooks like:Here's awith the requests + responses. I'm authenticating using a API token.Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?""",Trust,'m authenticating,1,415,431
0,55500377,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""",Anticipation,I was wondering,1,197,211
0,55500377,"""I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},""",Fear,missing,1,241,247
0,55514812,"""I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.see below errorAnd my code is""",Sadness,invalid,1,23,29
0,43124732,"""Im trying to install thepython package, and I encounter the foloowing meesage whenn the installation fails:what could cause this? I installed other packages without problems.""",Sadness,fails,1,102,106
0,40258634,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""",Anticipation,in,1,607,608
0,40258634,"""I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in. I have 3 errors, some of them is:Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10.Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.Thanks in advance.""",Sadness,fail,1,535,538
0,43759806,"""I am trying to use the google Vision API.I did the following steps:Enabled the Google Vision APICreated the service account idSetted the enviorement varialbe 'GOOGLE_APPLICATION_CREDENTIALS' with the json pathdownloaded the API with composerwrote the following code:$image = $vision->image(file_get_contents($path), ['FACE_DETECTION']);$result = $vision->annotate($image);When I lunch this code I obtain the following error:Somebody can help me to resolve this problem?Thanks a lot!!!""",Trust,Enabled,1,68,74
0,43759806,"""I am trying to use the google Vision API.I did the following steps:Enabled the Google Vision APICreated the service account idSetted the enviorement varialbe 'GOOGLE_APPLICATION_CREDENTIALS' with the json pathdownloaded the API with composerwrote the following code:$image = $vision->image(file_get_contents($path), ['FACE_DETECTION']);$result = $vision->annotate($image);When I lunch this code I obtain the following error:Somebody can help me to resolve this problem?Thanks a lot!!!""",Trust,CREDENTIALS,1,179,189
0,43759806,"""I am trying to use the google Vision API.I did the following steps:Enabled the Google Vision APICreated the service account idSetted the enviorement varialbe 'GOOGLE_APPLICATION_CREDENTIALS' with the json pathdownloaded the API with composerwrote the following code:$image = $vision->image(file_get_contents($path), ['FACE_DETECTION']);$result = $vision->annotate($image);When I lunch this code I obtain the following error:Somebody can help me to resolve this problem?Thanks a lot!!!""",Anticipation,result,1,338,343
0,53081398,"""Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?Apparently there is new functionality released, described here:Improved detection models are now available for the following features:Logo DetectionText Detection (OCR)Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.We'll support both the current model and the new model the next 90 days. After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.This is what my code looks like now:""",Trust,'ll support,1,331,341
0,52149997,"""My application run normal until now.But suddenly one service account (used for google cloud vision) create many Computer Engine instances(about 32 instances).I checked log and see this line.My billing increase from 10 to 200$/day :(Anyone help me?""",Sadness,:(,1,231,232
0,52149997,"""My application run normal until now.But suddenly one service account (used for google cloud vision) create many Computer Engine instances(about 32 instances).I checked log and see this line.My billing increase from 10 to 200$/day :(Anyone help me?""",Surprise,suddenly,1,41,48
0,54345710,"""I'm facing an issue in the Azure Face API. It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same.""",Anger,forcibly,1,122,129
0,54345710,"""I'm facing an issue in the Azure Face API. It was working fine earlier.Facing the below error An existing connection was forcibly closed by the remote host.Could you please let me know what could be the reason for the same.""",Fear,forcibly,1,122,129
0,44096947,"""Android Dev withI see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Androidwe use thedifferent from. Does the translator-API do the same like vision-api?Latest versions of libraries:google-api-services-translate:google-cloud-translate:""",Joy,like,1,193,196
0,49669981,"""I am trying to use google cloud vision OCR API to read text from image.var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")""",Anticipation,Deadline,1,186,193
0,47838580,"""The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:This is the source code processing of the gem:Why is it telling me string contains null byte? How can I get a bitmap in ruby?""",Joy,gem,1,182,184
0,35532645,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""",Joy,is appreciated,1,157,170
0,35532645,"""I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.Error Details from Google APIs Explorer""",Trust,is enabled,1,120,129
0,45139378,"""I try to analyze multiple images with AWS RekognitionIn main function in a loop:Rekognition Class:But one api call takes about 2 second at least, and I would like to make more parallel calls if possible""",Joy,would like,1,153,162
0,56391486,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",Anticipation,would be appreciated,1,334,353
0,56391486,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",Anticipation,I am wondering,1,87,100
0,56391486,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",Joy,:-),1,355,357
0,56391486,"""i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format belowAt the moment i can get the normalized vertices fine usingI have tried to do this , but it doesnt workAny Help would be appreciated :-)""",Joy,would be appreciated,1,334,353
0,53744481,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:Note :img_stris a byte arrayFirst error: sendall() argument 1 must be string or buffer, not dictReason in my understanding: { ""Bytes"" : image } is a Json and NOT a stringMy Solution: Make the json a string ( not sure whether I can concatenate img_str ( a byte array )Now error: Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)QuestionHow do I concatenate a byte array (img_str) with strings without losing the array ?Can i convertimagevariable to string WITHOUTgetting the can't decode byte 0xffexception ? orCan we do something else to overcome this issue ?Thanks in advance guys !!""",Anticipation,in,1,951,952
0,51103236,"""Not sure what the issues is as my code just stopped working overnight, but the text detection on Google Vision is either returning nil or returning words that are non-existent on the subject.Here's my request function:Part of my analyze results function:""",Anticipation,results,1,238,244
0,36655630,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",Anticipation,expected,1,496,503
0,36655630,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",Fear,missing,1,1383,1389
0,36655630,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",Joy,successfully,1,967,978
0,36655630,"""I cross posted this to the google group for Cloud Vision...and have added some additional findings.Here are all of the specifics I believe are relevant:Using VB.NET 2010Using service account authenticationLimited to .NET 4.0Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45Performing Text and Safe Search AnalysisPassing image content in request (not using Google Drive)When just sending through 4 images or so per request, things work as expected... I get responses, and annotations.If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.According to the API documentation, the following limits apply to Google Cloud Vision API usage.I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.Any thoughts as to what I might be missing?  Are the documented limitations below correct?MB per image 4 MBMB per request 8 MBRequests per second 10Requests per feature per day 700,000Requests per feature per month 20,000,000Images per second 8Images per request 16""",Surprise,results,1,639,645
0,39905841,"""I would like to get the data from Google cloud vision API and see the input can be given in the base64 and image uri format.But base64 appears to be too long and to upload the image as uri it take some extra time.Please let me know if anyone knows of any other work around for this.""",Joy,would like,1,3,12
0,51389440,"""So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?""",Anticipation,am attempting,1,6,18
0,50715542,"""Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.I am getting error in below line. And tried to open this url is not working:Below is my design code.Below is my code which worte in button click for display in labelI used below example url:I created service key account also in google for json file.""",Disgust,is not working,1,164,177
0,53701338,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",Trust,credential,2,416,425
1,53701338,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",Trust,credential,2,511,520
0,53701338,"""I am trying to develop C# Google Vision API function.the code is supposed to compile into dll and it should run to do the following steps.get the image from the image Path.send the image to Google vision apiCall the document text detection functionget the return value (text string values)DoneWhen I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...""",Trust,Credential,1,550,559
0,48704050,"""I got ""undefined"" while trying to Parse this JSON file:This file is th result from a XMLHttpRequest from the google vision API and This is what i'm doing to print ""description"" field:""",Anticipation,result,1,72,77
0,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Anticipation,results,4,169,175
1,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Anticipation,results,4,336,342
2,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Anticipation,results,4,542,548
3,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Anticipation,results,4,664,670
0,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Anticipation,resulted,1,257,264
0,39540741,"""I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?""",Surprise,sneak,1,420,424
0,56184573,"""hi i am following this project on github,i'm using the api amazon rekognition with success,i would like to see how to open the image of the compared face, i think i have to add something in ""index.js""the sample photos i put in the faces folder. thank you !""",Joy,would like,1,94,103
0,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",Joy,are appreciated,1,365,384
0,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",Joy,much are appreciated,1,369,384
0,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",Trust,instructions,1,211,222
0,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",Trust,CREDENTIALS,1,157,167
0,43771382,"""After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API onI tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated""",Anticipation,instructions,1,211,222
0,43586009,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""",Sadness,Unable,2,294,299
1,43586009,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""",Sadness,Unable,2,416,421
0,43586009,"""I have access to aws account with username. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.I wrote a method to generate access key, secret key and session token(using). It does not seem to be working.Error - Unable to load AWS credentials from any provider in the chainTried usingtoo, which makes more sense than. But throws same Unable to load creds error.The request its sending iswhere resourcePath is, dont know why?I'm using, asks for profile which I don't have. All I have is username and password to aws account.When I check the UI users page, I have restricted access""",Trust,credentials,1,313,323
0,48896074,"""I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file usingand now the file shows strange characters.shows(after sed)Original file shows this:Processed file shows this:This is the command I ran:Is there any way to restore it back to original encoding?""",Fear,command,1,250,256
0,32558923,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""",Joy,would like,1,63,72
0,32558923,"""I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :xThanks !""",Surprise,to interrupt,1,74,85
0,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Trust,multi-label,2,140,150
1,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Trust,multi-label,2,221,231
0,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Trust,labels,1,329,334
0,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Trust,supports,1,54,61
0,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Anticipation,training,2,158,165
1,51183169,"""Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference.Thus, can we:a) train using multi-label imagesand/orb) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it intooneof multiple output categories"".Any recommendations are also welcome.""",Anticipation,training,2,340,347
0,43660043,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""",Anticipation,results,1,198,204
0,43660043,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""",Anticipation,is expecting,1,213,227
0,43660043,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""",Fear,command,1,66,72
0,43660043,"""I have the AWS CLI installed on Windows and am using the Windows command prompt.I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with:This results in:Why is it expecting a comma?EDIT:When I try the format from the documentation I also get errors:""",Fear,commands,1,141,148
0,54672488,"""I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.Found one hereand ran it's different from what I need because it detects labels only.""",Trust,labels,1,266,271
0,54132185,"""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?""",Anticipation,result,1,424,429
0,54132185,"""I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.Any help regarding this?""",Trust,LABEL,1,214,218
0,53239821,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""",Fear,missing,1,256,262
0,53239821,"""I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.Update: Thanks to the provided answer by Dan D., I added the following line:""",Trust,valid,1,266,270
0,55043291,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""",Joy,successfully,1,202,213
0,55043291,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""",Joy,:),1,486,487
0,55043291,"""I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)This is the request code:And here's the handler code:Any help appreciated :)""",Joy,appreciated,1,474,484
0,52383178,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",Anticipation,usually,2,63,69
1,52383178,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",Anticipation,usually,2,315,321
0,52383178,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",Joy,would be,1,564,571
0,52383178,"""I am usingonto detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:Vision API output excerpt:""",Joy,good,1,575,578
0,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Anticipation,expected,1,82,89
0,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Anticipation,Expected,1,286,293
0,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Anticipation,results,2,295,301
1,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Anticipation,results,2,485,491
0,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Anticipation,Promise,1,122,128
0,56351778,"""I'm using a fluture to handle the response from an AWS service request.I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)Expected results:{ TextDetections:   [ { DetectedText: 'text1',       Type: 'LINE',       Id: 0,       Confidence: 98.7948989868164,       Geometry: [Object] },     { DetectedText: 'text2',...Actual results:c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...""",Disgust,regurgitation,1,219,231
0,47744054,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""",Anticipation,results,1,931,937
0,47744054,"""i'm trying to recognizefrom the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.thank you.""",Sadness,fails,1,329,333
0,56122848,"""Not sure if this is more google-cloud-related or pytest-related. See files below.When I run eitheror, the script runs fine.But when I run, the line in the scriptthrows ""ModuleNotFoundError: No module named 'google.cloud'"".I have tried unsuccessfully to add various package names into the requirements.txt file and/or runandwith and withoutflags. What steps can I take to overcome this error?conftest.py: (empty)requirements.txt:app/my_script.py:test/test_my_script.py:""",Joy,unsuccessfully,1,236,249
0,55466156,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",Trust,helpful,1,411,417
0,55466156,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",Trust,more helpful,1,406,417
0,55466156,"""I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.Thank you.""",Anticipation,to prepare,1,80,89
0,47570243,"""I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false onfunction. am tested on Blackberry keyone and also tested on Moto x play its working fine.Can anyone help me on this. Thanks in Advance""",Anticipation,in,1,316,317
0,56175138,"""Wrong image recognition on Azure Custom Vision Service.I have a doubt. I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks.""",Anticipation,results,1,280,286
0,56175138,"""Wrong image recognition on Azure Custom Vision Service.I have a doubt. I'm using Azure Custom Vision Service for image recognition.I uploaded my photos and I put the coca cola tag,I added 20 similar photos and to all I put their tagbut at the time of doing the test, I get these results.I'm doing a test with this image.Why does Custom Vision Service say that other soft drinks are Coca-Cola?Do I have to do other things specifically?Are my tags wrong?Thanks.""",Fear,doubt,1,65,69
0,51186135,"""I'm finding the right way to use AWS Rekognition service.My problem isHow to verify a person image on multi collections, I'm readingfrom Amazon but cannot find the implementation document for it. My point isFace verificationtitle.Update 1:My target is: Using AWS Rekognition to get person's info by their face.My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.What I tried:Upload multi captured portraits of a person with sameExternalImageIDbut I'm not sure it works or not.Finding a way to createCollectionfor each person, then upload person's portraits to theirCollectionbut I don't how to search a face through multipleCollections.I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.Update 2:What is your input material:Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").What are you trying to do:I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").Do you have it working, but it is not recognizing some people?Yes, it's work but sometimes it cannot recognize exactly people.Please tell us your use-case / scenario and what you are trying to accomplish:Create an AWS Rekognition collection with sample name (eg facetest).Register some people with their name is ExternalImageID.Submit an image to AWS Rekognition API to get ExternalImageID - his name.""",Joy,to accomplish,1,1351,1363
0,54722619,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",Anticipation,immediately,1,510,520
0,54722619,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",Fear,missing,1,220,226
0,54722619,"""I am trying to implementusing Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.I am able to see the response headers in the browser network, but theobject is always null.The Azure documentation says ""The service has accepted the request and will start processing later. It will return Accepted immediately and include an  Operation-Location  header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.""""",Sadness,unable,1,155,160
0,43875720,"""I am working on running few examples on Microsoft Face API, I would like to know if there is a possibility that i can send the request to my local cloudlet to fetch and retrieve information instead of sending the request to Microsoft?Thanks!""",Joy,would like,1,63,72
0,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Anticipation,results,3,224,230
1,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Anticipation,results,3,408,414
2,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Anticipation,results,3,551,557
0,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Anticipation,promise,1,207,213
0,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Trust,labels,2,78,83
1,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Trust,labels,2,318,323
0,49741658,"""I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains.I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end?Server.jsGoogleVisionAPI.js helperCan someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?""",Trust,label,1,564,568
0,40843164,"""I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face.As per IBM, IBM visual recognition do not support face recognition but only face detection.Can we use the custom classifiers by adding different types of images for each individuals?What is the significant pre/post-work from the developer to get at least 90% accuracy ?""",Disgust,do not support,1,348,361
0,53003814,"""So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.If anyone here could try and help me with the issue, please do. I'll be really thankful.Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.Here is my Android Code to access Rekognition:Here is my iOS code to access Rekognition:""",Anticipation,result,1,378,383
0,50546373,"""I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up.""",Anticipation,result,1,196,201
0,50546373,"""I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).I managed to get the path like this(in internet explorer):I want to know how can i send the path to a controller or to a function in c#.I also tried to build a form and add an action to the controller but the controller name didn't show up.""",Trust,managed,1,246,252
0,44276413,"""Error:Execution failed for task ':app:transformResourcesWithMergeJavaResForDebug'.""",Sadness,failed,1,17,22
0,43130920,"""I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference:Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?""",Trust,to be associated,1,467,482
0,50860448,"""Using Google Client Library interacting with the vision library.I have a function to detect labels from an image.GoogleVision.pyI have an api to call this function.However, it does not return the result and errors with the following""",Anticipation,result,1,197,202
0,50860448,"""Using Google Client Library interacting with the vision library.I have a function to detect labels from an image.GoogleVision.pyI have an api to call this function.However, it does not return the result and errors with the following""",Trust,labels,1,93,98
0,53530035,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""",Trust,Labels,1,67,72
0,53530035,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""",Trust,labels,1,233,238
0,53530035,"""I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).ThenI tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:My code is:...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.I need help please. Which library should I use and if its the second one. What should I do?Thank you""",Trust,credentials,1,486,496
0,55450247,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""",Disgust,are not working,1,128,142
0,55450247,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""",Surprise,results,1,236,242
0,55450247,"""Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:Left imageRight imageI tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.is there a way to find this out or decrease the possible cases so that i can use it in my model.""",Trust,the best,1,69,76
0,39641111,"""I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow isonce the qr-code received a detection the app should return to the main activity.If i do not callit works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash.Why is it becoming unresponsive? and where is the correct place to release the camera source?QrActivityQrReader Class""",Sadness,will crash,1,429,438
0,43657393,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",Sadness,unable,2,122,127
1,43657393,"""I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.I don't want to add Google dependency separately while using my .aar because my library project already contains the same.I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.Thanks.""",Sadness,unable,2,459,464
0,42135072,"""I am working with Watson Visual Recognition and have successfully created a custom classifier.I notice the build in default classifier can return a hierarchy eg: Animals/Dog, But how to create a custom classifier return response contains ""type_hierarchy"" such as default classifier ?It may be necessary to train a custom classifier with more positive class and negative class or it is possibly due to me being on the trial version  ?!!""",Joy,successfully,1,54,65
0,55897523,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",Anticipation,would be appreciated,1,1019,1038
0,55897523,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",Joy,would be appreciated,1,1019,1038
0,55897523,"""I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.So, I have all of my images, and I am trying to work through the steps here:I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.What software should I use to get bounding box vertices, is basically my question I think??Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious.I've looked at existing questions on here, all of the Q&A are more advanced than what I'm looking for.I don't have any code as I am using a CSV file to complete this part of the task.""",Trust,label,1,645,649
0,56213065,"""I noticed the Google Vision PDF OCR DOCUMENT_TEXT_DETECTION takes about 15 seconds to detect a single PDF page.But if I submit the same PDF page as JPG it takes less than 3seconds to detect textsI used the code provided here (C#)I noticed it takes about 15 seconds for the following line of code to say all text in PDF is detected and saved to gsBucketMy GsBucket is ""Multi-Regional Storage"" USI'm also uploading from a US locationI was wondering what else I can do to speed up the process or this is expected?""",Anticipation,is expected,1,499,509
0,39378862,"""I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form.The problem I'm having is that some of the responses come back with only an X coordinate. Example:I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue:. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000.Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error?""",Anticipation,attempting,1,594,603
0,55465835,"""I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You""",Fear,Missing,1,214,220
0,55465835,"""I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :Error :When try this code :I used aservice accountkey.Why i got error : 403 Permissin Denied and Missing a valid API Key ?Edited :I have follow this youtube tutorial :Thank You""",Trust,valid,1,224,228
0,55268232,"""I am followingarticle trying to make my Python script read labels related to an image using. The problem is that I am getting this error when trying to include a reference tovisionfrom google.cloud module.The error that I am getting says:This is weird because when I do:I can see it is there and its files are located at:Except for that, when I dopip freezein my working folder I can see them both that I need available:I am now wondering what could be the reason for not being able to see include this module in my Python script.""",Trust,labels,1,60,65
0,46152610,"""I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.Does anyone have any experience in this? ThanksMy node-red flow is here""",Sadness,crashes,1,193,199
0,51776654,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",Trust,to manage,1,1159,1167
0,51776654,"""Python 3.6.6, Pillow 5.2.0The Google Vision API has a size limit of 10485760 bytes.When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).To shrink it I've tried getting a shrink ratio and then simply resizing it:Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?""",Trust,the best,1,1146,1153
0,40668684,"""I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text. Here is the code:This is the error I recieve:Does anyone know what the issue is here?Thanks in advance.""",Anticipation,in,1,222,223
0,56012355,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",Anticipation,I was wondering,1,66,80
0,56012355,"""I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way."" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments""""",Trust,support,1,405,411
0,48999636,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",Trust,Inspired,1,100,107
0,48999636,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",Trust,official,1,116,123
0,48999636,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",Trust,credentials,1,238,248
0,48999636,"""I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g.and) Icreated a new project,attached the Vision API to it,created a service account and downloaded the credentials/key-JSON file,set up an VS project and got all relevant packages from NuGET.My code looks like this:While stepping through the code, the app hangs at(no exception or anything). The same happens, if I use other methods (e.g.or). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).What am I doing wrong here?Thanks!""",Anticipation,Inspired,1,100,107
0,56050457,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""",Fear,command,2,245,251
1,56050457,"""Im following this tutorial to set up the google vision ocr:. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.""",Fear,command,2,270,276
0,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",Anger,brute,1,2168,2172
0,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",Disgust,reliable,1,550,557
0,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",Fear,brute,1,2168,2172
0,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",Joy,Like,1,1144,1147
0,35785224,"""This question was asked but never answered-- but it is somewhat different than my need, anyway.I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found)It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.Like this:That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)I did this in CameraSource.java.This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end.To add this code, I tried putting it in the     private class FrameProcessingRunnable in the run() method.Right after the FrameBuilder Code, I added this:Which calls this class:I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.""",Sadness,Unfortunately,1,628,640
0,47867995,"""I am using AWS, boto3 and Pycharm to write a very simple program on python that compare one faces from one face image to another face from another face image.I written the following simple source code:However I get the following error:(Obviously in the place of xxx and yyy I am using the real keys)What is the problem and how can I fix this?""",Sadness,:(,1,235,236
0,42146912,"""I am having an issue getting my image to upload to the microsoft face api.I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.It posts to the server, and returns the following in the command line:What do I need to manipulate so that it works with the base64 encoding? It was posting with an image url off the internet prior to the modifications.""",Fear,command,1,272,278
0,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Anticipation,results,1,173,179
0,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Anticipation,Results,1,234,240
0,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Anticipation,result,2,719,724
1,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Anticipation,result,2,828,833
0,44446544,"""I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image.My goal is to present results from text extraction from an image using the OCR (). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.Using the OCR I got bounding box for each word of extracted text.This is how I solved highlighting:Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.But if boxes overlap, I got result like thisand I want thisMy question is if there is a better way how to define SVG path element to get result I want.""",Joy,would like,1,70,79
0,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",Trust,to label,1,53,60
0,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",Trust,credentials,3,126,136
1,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",Trust,credentials,3,151,161
2,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",Trust,credentials,3,373,383
0,38620455,"""I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:Here is the section of code that obtains the credentials:And here are the imports:I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.The key is a generated JSON file.""",Trust,CREDENTIALS,1,243,253
0,52448751,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""",Trust,decided,1,71,77
0,52448751,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""",Trust,would love,1,488,497
0,52448751,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""",Joy,would love,1,488,497
0,52448751,"""I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).I found this open source:and I hoped that it would work properly.It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.So I would love to combine these two. Unfortunately, I don't know how to, yet.I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.Help me, please.""",Sadness,Unfortunately,1,521,533
0,52446033,"""I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.Do you know if this is possible and how can I do that?""",Joy,would like,1,287,296
0,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,results,5,102,108
1,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,results,5,114,120
2,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,results,5,147,153
3,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,results,5,377,383
4,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,results,5,613,619
0,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Anticipation,doesn't attempt,1,455,474
0,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Fear,missing,1,531,537
0,47415374,"""I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results.The results given usingdiffer to the results returned from an android app which calls the same API.  The android example code is provided by Google on(I have modified it to use text detection rather than labels API)Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text.Can someone tell me why this is, am I missing some kind of setting?** Also using android play services, gives incorrect results - It would be my preference to use play services""",Trust,labels,1,314,319
0,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Sadness,to crash,1,162,169
0,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Sadness,crash,3,223,227
1,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Sadness,crash,3,387,391
2,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Sadness,crash,3,692,696
0,54292200,"""TLDR: How does one catch runtime errors that occur with the detectedLanguages() google vision call?Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.Full details of the crash are as follows:We are using the readDocument call with a callback function.The callback can be completely empty (so it isn't the callback that is causing the crash).Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?An example image that is causing the crash is:""",Trust,Certain,1,100,106
0,47109354,"""I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.""",Trust,official,1,71,78
0,54212819,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",Joy,good,2,923,926
0,54212819,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",Joy,tablet,1,909,914
0,54212819,"""On my website I'm making login system by user's face. And I'm trying to verify person, who is member inlargePersonGroup, haspersonIdand fivepersistedFaceIds(i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.My question is:Is this a problem, because of photos. Where I'm using differentdevices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.""",Joy,is,1,920,921
0,53952217,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""",Sadness,unable,2,74,79
1,53952217,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""",Sadness,unable,2,467,472
0,53952217,"""I know that this may seem to be a rather broad question, but I have been unable to figure outhow to create a Person in a Person Group using the Microsoft Face API in Android Studio.I have tried the following code to make aobject in Android:The above code outputs:""Creation failed: null""which means thatthewasfor some reason.In Visual Studio, to create aI simply have to do the following:Does anyone know how I can create thein a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.""",Sadness,failed,1,274,279
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Trust,valid,1,124,128
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Trust,labels,2,130,135
1,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Trust,labels,2,291,296
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Trust,label,1,356,360
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Joy,:),1,556,557
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Joy,seems,1,459,463
0,31741189,"""I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.1st I want to get the list of valid labels:I open the API tester:I issue an empty stringResponse Body: no content, Response Code: 0While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""I open this link:I upload an images and set label to ""Animal""Response Body: no content, Response Code: 0Any idea what I'm doing wrong?The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)""",Joy,well,1,479,482
0,43746016,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",Joy,successful,1,210,219
0,43746016,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",Joy,would like,2,41,50
1,43746016,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",Joy,would like,2,108,117
0,43746016,"""I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.Here is my error message:Does anybody know what I need to do?Any help would be much appreciatedCheers,Andi""",Trust,label,1,65,69
0,51116095,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""",Trust,labels,3,156,161
1,51116095,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""",Trust,labels,3,292,297
2,51116095,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""",Trust,labels,3,663,668
0,51116095,"""I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as ""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':WebDetection is a json file that was created using this link and stored into a local folder ==>The function of ""detect web"" is taken from this link -->Here is the function copied from the above link for your ready reference.However, When i execute the above function using this codeI am getting the below error:I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error.What might have caused the issue?""",Anticipation,ready,1,508,512
0,37306516,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",Anticipation,in,1,444,445
0,37306516,"""I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!""",Fear,missing,1,226,232
0,42117805,"""My target is to use Microsoft face API cognitive service to detect the faces in a frame , then using the landmarks returned for each face I would track it using optical flow for example!.My question is about the accuracy .. is this approach would work properly , or there are some other logical constrains exists behind tracking face using its landmarks?""",Trust,landmarks,2,106,114
1,42117805,"""My target is to use Microsoft face API cognitive service to detect the faces in a frame , then using the landmarks returned for each face I would track it using optical flow for example!.My question is about the accuracy .. is this approach would work properly , or there are some other logical constrains exists behind tracking face using its landmarks?""",Trust,landmarks,2,345,353
0,48428894,"""Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint? The purpose is to find the best match between a specific photo and the existing photos in the repository.""",Trust,the best,1,138,145
0,38363182,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""",Trust,labels,3,53,58
1,38363182,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""",Trust,labels,3,215,220
2,38363182,"""I've been testing out Google's Vision API to attach labels to different images.For a given image, I'll get back something like this:--> My questions are:Does anybody know if Google has published their full list of labels () and where I could find that?Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!""",Trust,labels,3,263,268
0,48400312,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""",Sadness,crashes,1,105,111
0,48400312,"""I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below.This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?Here is the code that manages the code scanning activity:""",Trust,manages,1,356,362
0,55994493,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""",Anticipation,ready,1,141,145
0,55994493,"""I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:What does it mean? How can I solve it?""",Trust,have managed,1,307,318
0,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",Joy,would like,2,3,12
1,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",Joy,would like,2,546,555
0,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",Joy,like,1,188,191
0,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",Anticipation,promising,2,533,541
0,18142659,"""I would like to know if there are good face detection & recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like), but either they are old or the answers do not satisfy me.I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.The only resource I know isand looks promising. I would like to have more options though.Thanks a bunch!""",Trust,promising,1,533,541
0,49340214,"""I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below.As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan""Hello World""from the camera. Is it possible to use some kind of touch event just to focus on the desired textPlease find the code used for text recognition""",Joy,would like,1,354,363
0,49463736,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""",Disgust,is not working,1,251,264
0,49463736,"""The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.The output is:As in other post in stackoverflow guide to useto upload the image. But it is not working.i think this part should somehow refactor to read image instead of a URL.Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.""",Trust,guide,1,211,215
0,51580768,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""",Disgust,isn't working,1,616,628
0,51580768,"""I am trying to send an image to the Google Vision API using Node.js by following this tutorial:I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.However when I run the following Node.js code:I receive the following 2 errors in the console:The second error about the Auth error continues to print out over and over again until I terminate execution.I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?""",Trust,CREDENTIALS,1,228,238
0,51611001,"""Its been so much of time exploring the Google vision API, I am trying to get the Vision API Response in English Language only , below is my request object to API which has language hints :Even this request object not getting correct response(multiple languages) from Vision API  ..if there is any steps is there to get response in English only please let me know, as of now response contains multiple languages like below :""",Surprise,exploring,1,26,34
0,55868948,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""",Disgust,doesn't work,2,241,252
1,55868948,"""I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this?If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce:This line works fine :""",Disgust,doesn't work,2,578,589
0,51008892,"""I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document.I ve tried to merge the base64 strings but it do not work.""",Disgust,do not work,1,269,279
0,52017315,"""I'm new to Google Vision API Client LibI'm using Vision API Client Lib for PHP to detect text in images, this is my code:All I need is using response as an array for handling, but $result returns something likes array of object/object ( sorry because I don't know much of OOP/Object)Although i convert $result to array $res, but if I use foreach loopI get thisHow do we get value (text detected) in above response for using ?.""",Anticipation,result,2,182,187
1,52017315,"""I'm new to Google Vision API Client LibI'm using Vision API Client Lib for PHP to detect text in images, this is my code:All I need is using response as an array for handling, but $result returns something likes array of object/object ( sorry because I don't know much of OOP/Object)Although i convert $result to array $res, but if I use foreach loopI get thisHow do we get value (text detected) in above response for using ?.""",Anticipation,result,2,304,309
0,52017315,"""I'm new to Google Vision API Client LibI'm using Vision API Client Lib for PHP to detect text in images, this is my code:All I need is using response as an array for handling, but $result returns something likes array of object/object ( sorry because I don't know much of OOP/Object)Although i convert $result to array $res, but if I use foreach loopI get thisHow do we get value (text detected) in above response for using ?.""",Joy,likes,1,207,211
0,55022748,"""I am trying to use Google Cloud Vision API.I am using the REST API in this.POSTMy request isBut the response is always only ""name"" like below:My ""gs"" location is valid.When I write the wrong path in ""gcsSource"", 404 not found error is coming.Who knows why my response is weird?""",Joy,like,1,132,135
0,55022748,"""I am trying to use Google Cloud Vision API.I am using the REST API in this.POSTMy request isBut the response is always only ""name"" like below:My ""gs"" location is valid.When I write the wrong path in ""gcsSource"", 404 not found error is coming.Who knows why my response is weird?""",Trust,valid,1,163,167
0,44816006,"""I am working with Google cloud vision API with Python()But I could not understand why the annotation result of a single image consists ofofs.Thesays:Whyreturns multiple annotations for a single image?It seems unnecessary because detection results are contained in each attributes (,, etc.).And when I try the api with my own image it returns theof length 1.So my question is:Why python's vision api client returns multiple annotations for a single image?Do I need to parse everyin the list?""",Anticipation,result,1,102,107
0,44816006,"""I am working with Google cloud vision API with Python()But I could not understand why the annotation result of a single image consists ofofs.Thesays:Whyreturns multiple annotations for a single image?It seems unnecessary because detection results are contained in each attributes (,, etc.).And when I try the api with my own image it returns theof length 1.So my question is:Why python's vision api client returns multiple annotations for a single image?Do I need to parse everyin the list?""",Anticipation,results,1,240,246
0,51443537,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""",Anticipation,in,1,726,727
0,51443537,"""I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:I can see the image inside my phone storage but the Bitmap returns null.Here's my code:Inside the, touchListener (Doubletap to capture the image)Inside thefunction (inserted after //Check orientation base on device):Based on the error, it deals something withWhat might be the error?Please base the codes here:andThank you in advance guys!EDIT: Additional InfoI have set user permission to use both camera and access storage.Also, I requested permission at my runtime. Please refer.""",Trust,deals,1,642,646
0,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,labels,2,934,939
1,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,labels,2,955,960
0,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,label,2,946,950
1,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,label,2,968,972
0,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,credential,1,181,190
0,54546886,"""i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the keyi took the code from """" and it shows an error at 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' membersame like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .the sample code is as follows:import iofrom google.cloud import visionvision_client = vision.ImageAnnotatorClient()file_name = 'text.png'with io.open(file_name,'rb') as image_file:    content = image_file.read()image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' memberlabels = image.detect_labels()for label in labels:print(label.description)""",Trust,CREDENTIALS,1,252,262
0,51223852,"""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in.""",Joy,would like,2,42,51
1,51223852,"""I'm working with the Google Vision API.I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.What I would like is to get the vertices for each block of words in.""",Joy,would like,2,211,220
0,41254591,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""",Joy,is appreciated,1,697,710
0,41254591,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""",Sadness,:(,1,49,50
0,41254591,"""I am reading the Google Vision API documentation:()It says something like the follwing:My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!""",Trust,fundamentals,1,544,555
0,34643033,"""under my IBM Bluemix account, I have registered a Watson Visual Recognition service.My intention is to call the service from Bizagi BPMS as REST service.Bizagi brings an ""unauthorized"" error.The URL for the REST Service isThe Service URL is(x and y are the credentials from the service instance in Bluemix.When entering the Service URL directly into the browser, I can enter the authentication credentials in a popup window, but the response is ""Error 404: SRVE0190E: File not found: / """"",Trust,credentials,2,258,268
1,34643033,"""under my IBM Bluemix account, I have registered a Watson Visual Recognition service.My intention is to call the service from Bizagi BPMS as REST service.Bizagi brings an ""unauthorized"" error.The URL for the REST Service isThe Service URL is(x and y are the credentials from the service instance in Bluemix.When entering the Service URL directly into the browser, I can enter the authentication credentials in a popup window, but the response is ""Error 404: SRVE0190E: File not found: / """"",Trust,credentials,2,395,405
0,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",Anticipation,in,1,429,430
0,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",Anticipation,training,2,29,36
1,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",Anticipation,training,2,197,204
0,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",Anticipation,result,1,412,417
0,44215222,"""I am trying to evaluate the training function of the Watson visual Recognition API.Has anyone some experience with costumizing classifers for Visual Recognition?I have some expierence myself with training the classifier and found some infomation in this blog:What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?How long does it take to get such a result?Thank you in advance for your help.""",Joy,would like,1,274,283
0,39009764,"""I'm trying to use the Microsoft Emotion API.I can use the image version without any issues but when I try to use the video version I get an empty response.It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.Here is my code. Would appreciate any help!""",Joy,successfully,1,176,187
0,44166139,"""Hello I'm trying to test out the google cloud vision api for my android app.I enabled the api and created an OAuth 2.0 client ID and I'm using the sample code from google:and here is the json resonse:I am pretty sure the api key provided is the one I am using.What could I be doing wrong?""",Trust,enabled,1,79,85
0,54898476,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""",Joy,to be appreciated,1,390,406
0,54898476,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""",Sadness,crashes,1,13,19
0,54898476,"""When an app crashes for live capture detect text in getting an image from thecall back did output.Get image call back from the send Google vision API I have used that code Did anyone work?My codeget let rget only doller value for example live captureI got an Output for like$16.542451) I have set frames rate but not working2) I have used Dispatch but not working.> Please share your code to be appreciated Thanks.""",Trust,share,1,374,378
0,54863800,"""I have this code for Google Vision API. I have Google credentials as a path and also as enviromental variable, butreturn nothing and error appears atHere is code:And here is console with emptyand error:""",Trust,credentials,1,55,65
0,47287749,"""I'm using Google Vision API, yet I've noticed that it is limited for the top 10 labels, and does not return results under 70% confidence.Is there a setting or a way to receive results that are lower than the 70% threshold?""",Anticipation,results,1,177,183
0,47287749,"""I'm using Google Vision API, yet I've noticed that it is limited for the top 10 labels, and does not return results under 70% confidence.Is there a setting or a way to receive results that are lower than the 70% threshold?""",Surprise,results,1,109,115
0,47287749,"""I'm using Google Vision API, yet I've noticed that it is limited for the top 10 labels, and does not return results under 70% confidence.Is there a setting or a way to receive results that are lower than the 70% threshold?""",Trust,labels,1,81,86
0,48456300,"""I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!""",Trust,certain,1,62,68
0,54816799,"""I am usingto link Rekognition results to a DynamoDB table. It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}""",Anticipation,results,1,31,37
0,54816799,"""I am usingto link Rekognition results to a DynamoDB table. It is giving me this error:The code used from GitHub is.I made sure the region-name is the same for the lambda-bucket and the table.I am a starter in this, so any help will be appreciated!Thanks!Edit:I made some modifications and now it is giving me this:}""",Joy,will be appreciated,1,228,246
0,44195115,"""I have one project that integrates with Google vision APIs.I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all. For my case I need to proof that every photos uploaded must not contain any glasses.This image, it seems that the Google Vision API can not detect wearing glasses at all.""",Trust,need to proof,1,173,185
0,48514721,"""Edited:I installedGoogle Cloud Speechby using the followning command:when I runPython Programto convertAudiointoTextusingPython3, I get an error:But If I run the same program usingPython(Python2),I get the correct output.I want to run the program usingPython3. I am usingOpenSuse Tumblebeed.So, I tried the following commands to installgoogle-cloud:But I am getting the following error by all these commands:I thinkGCCis trying to findpython.hso the process is exiting with error code 1.""",Fear,commands,1,318,325
0,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Trust,credentials,5,218,228
1,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Trust,credentials,5,299,309
2,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Trust,credentials,5,424,434
3,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Trust,credentials,5,1093,1103
4,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Trust,credentials,5,1108,1118
0,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Joy,:),2,2156,2157
1,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Joy,:),2,2679,2680
0,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Sadness,failing,1,957,963
0,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Sadness,incompatible,1,1531,1542
0,44994201,"""I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)""""the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)Is there a very simple example on how to connect to a watson service using this type of credentials? :""credentials"": {""url"": """",""note"": ""It may take up to 5 minutes for this key to become active"",""api_key"": ""********************************************************"" }I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.When opening the source code in VS, it says ""incompatible"" with all the files.""""When trying to install with Nuget I get Errors:in VS2013:Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommandin VS 2015:Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.At line:1 char:1+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommandThere is no example on how to use or install the SDKs except for using Nuget so I'm lost here.""",Fear,Commands,1,2276,2283
0,52818392,"""I'm using the Python SDK snippet provided byI want to return face attributes, The docssuggest that addingTo the Base URl will return age and gender attributes. It's throwing me an error, am I missing something?This is my first time using Azure Face API.""",Fear,missing,1,193,199
0,52809726,"""I would like to use Google Cloud Vision service in commercial purpose.But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.Briefly my question is 2 points below.Is there any license which should be followed in order to commercial use?the condition for commercial useIf there is someone who have answer about them,it would be highly appreciated that you could give me answers.""",Joy,would be appreciated,1,420,446
0,52809726,"""I would like to use Google Cloud Vision service in commercial purpose.But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.Briefly my question is 2 points below.Is there any license which should be followed in order to commercial use?the condition for commercial useIf there is someone who have answer about them,it would be highly appreciated that you could give me answers.""",Joy,highly would be appreciated,1,429,446
0,52809726,"""I would like to use Google Cloud Vision service in commercial purpose.But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.Briefly my question is 2 points below.Is there any license which should be followed in order to commercial use?the condition for commercial useIf there is someone who have answer about them,it would be highly appreciated that you could give me answers.""",Joy,would like,1,3,12
0,52809726,"""I would like to use Google Cloud Vision service in commercial purpose.But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.Briefly my question is 2 points below.Is there any license which should be followed in order to commercial use?the condition for commercial useIf there is someone who have answer about them,it would be highly appreciated that you could give me answers.""",Anticipation,would be appreciated,1,420,446
0,54120224,"""I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production.in the above code I am getting error:Also tried different ways to initiate the client using:With the second way I am getting :SDK version:""aws/aws-sdk-php"": ""^3.82""I am not sure what I am doing wrong here.""",Disgust,are not working,1,177,191
0,55739476,"""Currently the maximum preview resolution supported by Fotoapparat library for the frame processor is set to 1280x720 but Im using Google Cloud vision API and the recommended resolution can go up to 1600x1200 as mentioned inHow can I increase the maximum preview resolution to suit my needs for firing the appropriate detection request?""",Trust,supported,1,42,50
0,53737023,"""After followingI get one issue that after weeks of searching I haven't solved.At this point:As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.My requirements.txt (the relevant parts):""",Trust,advised,1,96,102
0,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",Trust,label,1,47,51
0,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",Trust,labels,2,156,161
1,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",Trust,labels,2,221,226
0,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",Joy,can achieve,1,262,274
0,45917756,"""I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:It works very well. It displays all the labels. But on the Googleit also displays the percentages of the labels. See the image for an example.How can I achieve this by using the .NET library?""",Joy,would like,1,3,12
0,43639575,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",Anticipation,result,2,49,54
1,43639575,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",Anticipation,result,2,191,196
0,43639575,"""i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. result like below""",Joy,like,1,198,201
0,49386572,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""",Sadness,isolated,1,305,312
0,49386572,"""I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.Often the API has troubles in recognizing single digits, as you can see in this image:I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)At other times Vision confuses digits with letters:But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?""",Trust,managing,1,414,421
0,51192145,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",Trust,label,1,316,320
0,51192145,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",Trust,certain,1,1278,1284
0,51192145,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",Joy,to achieve,1,1387,1396
0,51192145,"""I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect theAPI as the problem doesn't occur with. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:A similar code definesas a list which includes all FaceDetails:When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,The answer I am receiving is 1At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the, the response structure ofis separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:I would greatly appreciate any tips and tricks on how to achieve this.""",Sadness,unable,1,845,850
0,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",Trust,Credentials,1,164,174
0,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",Trust,CREDENTIALS,2,306,316
1,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",Trust,CREDENTIALS,2,438,448
0,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",Trust,credentials,1,366,376
0,49672973,"""I am trying to use google cloud vision api text detection.I am getting the following error at""var client = ImageAnnotatorClient.Create();""""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. Seefor more information.""I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.Where am i actually going wrong. Am i missing some important steps?""",Fear,missing,1,510,516
0,49425490,"""I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:The server replies (Error 404):How can I fix this? I tried to do with an ajax request and via browser too.Thanks in advance!""",Anticipation,in,1,224,225
0,55292198,"""I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to thefor watson-developer-cloud. Themakes use of thepackage hence I get theerror when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using thepackage for cross origin calls.Thisis pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directoryas it seems less clunky than pasting the multiple portions below.The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!""",Trust,helpful,1,444,450
0,53281993,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""",Anticipation,results,1,101,107
0,53281993,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""",Fear,missing,1,805,811
0,53281993,"""I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that.I have a nodejs project that uses theand that sample project uses themodule and then uses this kind of structure to get the key file (referenceHowever, when using themodule directly, you can also use theenvironment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:So, considering the project will be tracked with git, my questions are:What is the code level advantage of using either approach in the above?What security issues need to be addressed in either approach?Any other considerations I'm missing?Granted I don't want my keyfile stored in git but I still want the project tracked there.""",Joy,happy,1,162,166
0,48923406,"""I don't find a way to use a ktor application inside an AWS lambda...That is, instead of starting an embedded server or using an external server as described in, I just need to ""execute"" the pipeline.I suppose this is more or less like the TestEngine but I am not so familiar with the ktor framework to be sureNote :I have already found examples to run one kotlin function per lambda (the best tutorial IMHO being).The problem is I dont want to manage one lambda per function (I want one microservice per lambda, the microservice being responsible for multiple tightly coupled operations)""",Trust,to manage,1,442,450
0,48923406,"""I don't find a way to use a ktor application inside an AWS lambda...That is, instead of starting an embedded server or using an external server as described in, I just need to ""execute"" the pipeline.I suppose this is more or less like the TestEngine but I am not so familiar with the ktor framework to be sureNote :I have already found examples to run one kotlin function per lambda (the best tutorial IMHO being).The problem is I dont want to manage one lambda per function (I want one microservice per lambda, the microservice being responsible for multiple tightly coupled operations)""",Trust,responsible,1,536,546
0,48923406,"""I don't find a way to use a ktor application inside an AWS lambda...That is, instead of starting an embedded server or using an external server as described in, I just need to ""execute"" the pipeline.I suppose this is more or less like the TestEngine but I am not so familiar with the ktor framework to be sureNote :I have already found examples to run one kotlin function per lambda (the best tutorial IMHO being).The problem is I dont want to manage one lambda per function (I want one microservice per lambda, the microservice being responsible for multiple tightly coupled operations)""",Trust,the best,1,385,392
0,47634218,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",Anticipation,result,1,369,374
0,47634218,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",Joy,:),1,235,236
0,47634218,"""I try text_detection to photo. However, strange problem happens now.I post very simply debug program.the flow is:1:defining variable(photo, google storage's path, vision's url)2:generating the bearer token. Add it to ServiceToken.(ref:)3: rbody is Google Cloud Vision's request.4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~).In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?""",Trust,confirmed,1,403,411
0,50805898,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""",Joy,gem,1,67,69
0,50805898,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""",Joy,Gem,1,179,181
0,50805898,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""",Joy,awesome,1,222,228
0,50805898,"""I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.Gem File:Bundle env:Bundle Doctor:Alright, awesome everything is clearly installed, so lets try this.Bundle exec kitchen test:How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above.Bundle info aws-sdk-core:Bundle show aws-sdk-core:""",Trust,Doctor,1,206,211
0,51365884,"""When I start the stream processor it fails.I create the processor and I check (list stream processors), the initial status is STOPPED, then I execute the command to start stream processor, I check again and the status returned is FAILED.I'm using: - Raspberry Pi and Camera Pi;- AWS SDK for PHP 3.x ()More info:CREATE COLLECTION = OKCREATE STREAM PROCESSOR = OKSTART STREAM PROCESSOR = FAILEDPrint Screens folder:1 - Kinesis video stream:2 - ""streamA"" (1):3 - ""streamA"" (2):4 - ""dataA"":5 - Role ARN:6 - Stream Processor Failed:What am I doing wrong?""",Sadness,fails,1,38,42
0,51365884,"""When I start the stream processor it fails.I create the processor and I check (list stream processors), the initial status is STOPPED, then I execute the command to start stream processor, I check again and the status returned is FAILED.I'm using: - Raspberry Pi and Camera Pi;- AWS SDK for PHP 3.x ()More info:CREATE COLLECTION = OKCREATE STREAM PROCESSOR = OKSTART STREAM PROCESSOR = FAILEDPrint Screens folder:1 - Kinesis video stream:2 - ""streamA"" (1):3 - ""streamA"" (2):4 - ""dataA"":5 - Role ARN:6 - Stream Processor Failed:What am I doing wrong?""",Sadness,is FAILED,1,228,236
0,51365884,"""When I start the stream processor it fails.I create the processor and I check (list stream processors), the initial status is STOPPED, then I execute the command to start stream processor, I check again and the status returned is FAILED.I'm using: - Raspberry Pi and Camera Pi;- AWS SDK for PHP 3.x ()More info:CREATE COLLECTION = OKCREATE STREAM PROCESSOR = OKSTART STREAM PROCESSOR = FAILEDPrint Screens folder:1 - Kinesis video stream:2 - ""streamA"" (1):3 - ""streamA"" (2):4 - ""dataA"":5 - Role ARN:6 - Stream Processor Failed:What am I doing wrong?""",Fear,command,1,155,161
0,45395459,"""i'm kind of new to the Azure Computer Vision API but i'm interested to use this for parsing a lot of mathematical documents.I wanted to usefor evaluating the output string from the Vision API but currently only text recocgnition is supported.Does somebody know if the API is usable for this kind of scenario in any way (or will be in the future)?If this is not a good use-case for  this kind of AI API what would you recommend to use for generating a usable string expression from handwritten mathematical documents ?""",Joy,is not,1,355,360
0,45395459,"""i'm kind of new to the Azure Computer Vision API but i'm interested to use this for parsing a lot of mathematical documents.I wanted to usefor evaluating the output string from the Vision API but currently only text recocgnition is supported.Does somebody know if the API is usable for this kind of scenario in any way (or will be in the future)?If this is not a good use-case for  this kind of AI API what would you recommend to use for generating a usable string expression from handwritten mathematical documents ?""",Joy,good,1,364,367
0,45395459,"""i'm kind of new to the Azure Computer Vision API but i'm interested to use this for parsing a lot of mathematical documents.I wanted to usefor evaluating the output string from the Vision API but currently only text recocgnition is supported.Does somebody know if the API is usable for this kind of scenario in any way (or will be in the future)?If this is not a good use-case for  this kind of AI API what would you recommend to use for generating a usable string expression from handwritten mathematical documents ?""",Trust,is supported,1,230,241
0,42391009,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""",Sadness,to bottom,2,282,290
1,42391009,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""",Sadness,to bottom,2,350,358
0,42391009,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""",Anticipation,would have expected,1,514,532
0,42391009,"""I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.For example, consider the image:The API returns the text like this:Whereas, I would have expected something like this:I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line?Thanks for your help.Alex""",Joy,liked,1,379,383
0,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,labels,2,120,125
1,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,labels,2,406,411
0,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,official,1,246,253
0,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,label,2,314,318
1,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,label,2,369,373
0,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,rules,2,390,394
1,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Trust,rules,2,446,450
0,45075891,"""I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs.On the official document, I could't find the description about the teacher label list or something. Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)""",Joy,awesome,1,33,39
0,46731393,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""",Fear,missing,1,278,284
0,46731393,"""I am trying to Implement Google Vision OCR Request. Here is My Code,Request HandlerProblem is getting ""Bad Request, 400 Status, Request must specify image and features."".I've Checked the Request body for, getting true. API is working fine on Postman.Please let me know if i am missing something, Any Help will be appreciated.Thank You""",Joy,will be appreciated,1,306,324
0,52471113,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""",Anticipation,results,1,111,117
0,52471113,"""I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:Exactly the same request but with a smaller imageI have added the request over 16k to pastebin:Failing request is here:I could only find a 20MB limitation in the docs () but nothing like the weird issue I have. Thanks.""",Sadness,Failing,1,331,337
0,51417691,"""How to darken the area around the field to be scanned?I would like the view as in the. I have a frame around, but I do not know how to dim the area around. I use google vision.[]""",Joy,would like,1,57,66
0,49664844,"""I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?""",Trust,to authenticate,1,155,169
0,38288634,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",Disgust,does not work,1,381,393
0,38288634,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like ( , ) etc.I have tried to set local in the request but it does not work.I just want to get swedish letters in response. I have no idea what should i do?if someone give some link to google-vision discussion thread what will be helpful.""",Trust,helpful,1,549,555
0,39478404,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""",Anticipation,result,2,204,209
1,39478404,"""I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?Here is the code I use for these requests:""",Anticipation,result,2,305,310
0,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",Anticipation,results,2,100,106
1,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",Anticipation,results,2,511,517
0,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",Trust,landmarks,1,110,118
0,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",Trust,labels,1,120,125
0,51095417,"""After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google apiThanks for taking the time to help.""",Joy,successfully,1,65,76
0,54961012,"""I would appreciate some guidance on the following issue.Use Case:Create a collection with known faces.Search a stored video toidentify ""known"" faces & draw a bounding box around them in the videoframeSteps taken:I'm able to create a collection and index faces I'm able to analysethe stored video and get the results of PersonMatch and FaceMatchusing getFaceSearch()I'm able to draw the bounding boxes around Persons found in the video, etc., however...Issue:The response of getFaceSearch() returns an array of FaceMatches.However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video.I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame. Here is the API document that I'm referring to.Thanks for your help on this issue!""",Anticipation,results,1,309,315
0,39407269,"""So I just want to detect text or labels from an image using the google cloud vision API. But When I run this code I always get:But I don't know why...here is the full json output what I get:My test code is here:So the question is.. what is wrong with this code?""",Trust,labels,1,34,39
0,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",Anticipation,instructions,1,16,27
0,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",Anticipation,am expected,2,69,79
1,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",Anticipation,am expected,2,347,357
0,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",Trust,instructions,1,16,27
0,50733961,"""I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -As per the github doc, I am expected to be given a classifier ID. But I get request too large error.So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?, I get this error when I try posting one zip file instead of posting a few, as said before.""",Trust,credentials,1,530,540
0,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",Trust,credentials,4,89,99
1,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",Trust,credentials,4,227,237
2,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",Trust,credentials,4,264,274
3,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",Trust,credentials,4,513,523
0,52925404,"""I've a Go app that uses theGoogle Vision APIandGoogle Video intelligenceAPI.To enter my credentials, I set the environment variable called. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.Problem:My credentials arenotinitially saved in a file. Instead they are assigned to a  string variable inside my app.As a workaround, I store that value to a temporary file and then assign it's path to, like described above.Question:Is it possible to set API credentials forwithout this file?""",Joy,like,1,457,460
0,54824911,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",Anticipation,result,2,165,170
1,54824911,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",Anticipation,result,2,482,487
0,54824911,"""I am using Lambda to detect faces and would like to send the response to a Dynamotable. This is the code I am using:My problem is in this line:I am able to see the result in the console.I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table.Do do this: 1. What to add as a key and partition key in the table?2. How to transfer the whole response to the tablei have been stuck in this for three days now and can't figure out any result. Please help!I tried this code:It gave me two of errors:Can you pleaaaaase help!""",Joy,would like,1,39,48
0,40472437,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",Anticipation,results,2,218,224
1,40472437,"""I am using microsoft face api from my client side code using java script/Jquery.Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?Thank you!""",Anticipation,results,2,276,282
0,47906157,"""I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.My source code is taken from the Google Cloud tutorial for this API and it is the following:However I get the following error:This is weird because:1) I created a new service account2) I addedto my .bash_profile (I put the json file at the Pycharm file of this project)Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.How can I fix this bug and make the program running?By the way the problem is solved if I simply addto my source code.""",Anticipation,would expect,1,495,506
0,40037830,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""",Joy,would be appreciated,1,538,562
0,40037830,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""",Joy,much would be appreciated,1,547,562
0,40037830,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""",Anticipation,would be appreciated,1,538,562
0,40037830,"""I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.I just explored Google Vision API () to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.Is this possible in Google Vision API or any other solution available for this?Any help would be much appreciated.""",Surprise,explored,1,145,152
0,54015378,"""I try to compare the two image which is in s3.So I have completed the code by referring to the following:I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).But it makes that error.com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)I thought if I make a mistake of IAM, the app can't upload the file.I try to upload the file with same credentialsProvider, upload success.I don't think that's what happened because of permission.S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2is there any information to get s3 object with Rekognition?""",Sadness,Unable,1,272,277
0,43814654,"""I am trying to usein my Google App Engine Python application. Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?""",Fear,missing,1,234,240
0,43814654,"""I am trying to usein my Google App Engine Python application. Are there examples for using it?I followed the tutorial:andI am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:ErrorAm I missing a dependency configuration?""",Trust,enabling,1,175,182
0,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Trust,labels,3,84,89
1,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Trust,labels,3,233,238
2,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Trust,labels,3,422,427
0,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Trust,credentials,2,135,145
1,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Trust,credentials,2,563,573
0,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Anticipation,was expecting,1,522,534
0,47534024,"""I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.I am using the sample program given at: ''I can successfully detect labels for the local images and images on internet which are publicly accessible.When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.e.g.When it is private:When I mark it as 'public':I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.Can you help?""",Joy,successfully,1,213,224
0,53039190,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""",Anticipation,results,2,268,274
1,53039190,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""",Anticipation,results,2,626,632
0,53039190,"""I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results.I am using '@google-cloud/vision'() node npm for the same. Also noticed that some of the characters are mismatching in the resultseg: In most of the cases '0' is recognizing as O(), 5 as S (), / as I (), etcSame image giving different resultsPlease let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results.""",Anger,ie,1,208,209
0,42970980,"""I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google:. Apparently it is possible to send a base64 encoded image.Here is my code:As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.Thanks in advance.""",Anticipation,in,1,375,376
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Trust,gRPC-inspired,1,419,431
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Trust,supported,1,45,53
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Trust,the best,1,609,616
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Fear,trapped,1,347,353
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Fear,fear,1,787,790
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Sadness,crashes,1,326,332
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Sadness,crash,1,1018,1022
0,47428173,"""I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.It is imperative for me to be able to minimize the time spent uploading and processing the images.I started by wrapping calls to GCV in aand, but this sporadically crashes my code (not trapped by a python) thus:According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g.andGiven that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?Update:With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a laExcept that I had to addtoin my Docker container in order to pick up.I made no changes to any other python packages.I then did:Obviously this gets expensive, but the crash rate is <1/20 calls (and counting) compared to 1/3 calls before.Now:How can I testconclusivelythat this is fixed?""",Anticipation,gRPC-inspired,1,419,431
0,44862532,"""I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:I've been using the following docs for help:, to no avail.FYI, I've tried the shorthand too, but no worky, same error:""",Anticipation,'m attempting,1,2,14
0,42525482,"""I want to use Amazon Rekognition for some Detection project from India, each and every time I use to connect, either check for Region error pops up or S3 metadata issue?botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectLabels operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.thanks in advance.""",Anticipation,in,1,401,402
0,42525482,"""I want to use Amazon Rekognition for some Detection project from India, each and every time I use to connect, either check for Region error pops up or S3 metadata issue?botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectLabels operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.thanks in advance.""",Sadness,Unable,1,304,309
0,50598790,"""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks""",Joy,successfully,1,213,224
0,50598790,"""I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below.When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.[now here is the problem]//---------------call made from node js to azure face detection api----//---------------Error:Thanks""",Joy,like,1,85,88
0,36976312,"""I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?""",Disgust,accurate,1,173,180
0,53799577,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""",Anticipation,results,1,174,180
0,53799577,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""",Anticipation,would be appreciated,1,306,325
0,53799577,"""I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?Is this the issue as I'm using trial version?Any help would be appreciated.""",Joy,would be appreciated,1,306,325
0,55947906,"""I am using the amazon rekognition API to analyse my video to find and Search faces.A one minute video has been on processing for nearly an hour now. But have not received any result .Is this normal ?""",Anticipation,result,1,176,181
0,53109098,"""I was trying to create an ios app for text recognition with Google Vision text recognition.I had integrated all the required pods into my project as mentioned inIn order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class.Whenever I create an instance of that class and with a variable and try to access the properties of that class it there is aRed errorindication at the line where I try to access the properties with the instance variable of the class and aGRAY ERRORindication at the top of theViewController ClassThe ERROR MESSAGEGoogle ML DocumentationIn that documentation, they have used let and I also checked with that but every time same problem.Here is the code also:""",Anticipation,order,1,165,169
0,50343162,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""",Anticipation,call,1,50,53
0,50343162,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""",Anticipation,for,1,55,57
0,50343162,"""I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I founddocumentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.Is it available somewhere else?""",Sadness,unable,1,11,16
0,53718791,"""I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparisonAfter running this code I gotAfter I removed the \ I got another problemHelp me to solve this...""",Fear,command,1,57,63
0,50868017,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""",Anticipation,results,1,165,171
0,50868017,"""I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop () and from local image detection in python.My code is as followsA sample image that highlights this is attachedThe python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?.""",Anticipation,result,1,519,524
0,53249139,"""I'm trying to integrate my project withHere is the maven dependency for you to check the client version I'm trying to integrate with:The way the documentation offers to set the API authentication credentials is the following:I'm wondering if there is a way to set the credentials explicitly in code as that is more convenient than setting environment variables in each and every environment we are running our project on.As I know for a former client versionthat was possible doing the following:But for the new client API I was not able to find the way and documentation doesn't say anything in that regards.""",Trust,credentials,2,197,207
1,53249139,"""I'm trying to integrate my project withHere is the maven dependency for you to check the client version I'm trying to integrate with:The way the documentation offers to set the API authentication credentials is the following:I'm wondering if there is a way to set the credentials explicitly in code as that is more convenient than setting environment variables in each and every environment we are running our project on.As I know for a former client versionthat was possible doing the following:But for the new client API I was not able to find the way and documentation doesn't say anything in that regards.""",Trust,credentials,2,269,279
0,53249139,"""I'm trying to integrate my project withHere is the maven dependency for you to check the client version I'm trying to integrate with:The way the documentation offers to set the API authentication credentials is the following:I'm wondering if there is a way to set the credentials explicitly in code as that is more convenient than setting environment variables in each and every environment we are running our project on.As I know for a former client versionthat was possible doing the following:But for the new client API I was not able to find the way and documentation doesn't say anything in that regards.""",Anticipation,I 'm wondering,1,226,238
0,51257124,"""I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.I have tried three different methods of generating base64 strings for images, on two OS systems respectively:Using plain Python:Using:Using plain Python after usingfor whatgets:With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.On the same OS (either MacOS or Linux),!===.Across OSes,==, but!=and!=.Why is that? Is there any way to solve it? Or is it a bug needed reporting?This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.""",Anticipation,results,1,746,752
0,48527517,"""I am studying Amazon Rekognition API. I would to like to know if it is possible to call Amazon Rekognition API via curl?""",Joy,to like,1,47,53
0,48549064,"""I'd like to use the Google Cloud Vision API with PHP.Inside mydirectory I executed the following command line:So now insideI have adirectory which has the following files/directories:,,,,,,,,,,,I'd like now to use the Google Vision API in PHP, following this tutorial:But I'm getting:I tried to add ""vendor"" when I'm defining the namespace (as my Google files are under vendor)But it didn't help.""",Joy,'d like,2,2,8
1,48549064,"""I'd like to use the Google Cloud Vision API with PHP.Inside mydirectory I executed the following command line:So now insideI have adirectory which has the following files/directories:,,,,,,,,,,,I'd like now to use the Google Vision API in PHP, following this tutorial:But I'm getting:I tried to add ""vendor"" when I'm defining the namespace (as my Google files are under vendor)But it didn't help.""",Joy,'d like,2,196,202
0,48549064,"""I'd like to use the Google Cloud Vision API with PHP.Inside mydirectory I executed the following command line:So now insideI have adirectory which has the following files/directories:,,,,,,,,,,,I'd like now to use the Google Vision API in PHP, following this tutorial:But I'm getting:I tried to add ""vendor"" when I'm defining the namespace (as my Google files are under vendor)But it didn't help.""",Fear,command,1,98,104
0,53578143,"""I am doing some thing wrong over here, while comparing two images in different S3 Bucket.Even though, I am comparing images of male and female it would give 99% confidenceor am i missing something in the declaration yetMaybe This line is causing a problemOr my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below. What else do i need to correct so that it will return the confidence within the rang specified""",Fear,missing,1,180,186
0,47466195,"""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next. I am completely stuck right now.""",Fear,command,2,346,352
1,47466195,"""For an application, I have to use Google Vision API.I am able to useand do image analysis in my computer.But, when I deploy my app on developmental server I am getting error:When I createfile that contains:I am getting error:When I tried the hack mentioned in the link below:I am getting error:Then I followed the instruction here:And used this command:And updatedfile:I am getting error:Then I followed the instruction here:And used this command:I copiedandfiles from here:I am getting error:I don t know what to do next. I am completely stuck right now.""",Fear,command,2,440,446
0,54406629,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",Trust,Label,1,72,76
0,54406629,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",Trust,LABEL,1,129,133
0,54406629,"""Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?Expample.. I am doing text detection for one creativeSo my code isthis only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.""",Sadness,unable,1,162,167
0,45942150,"""Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches () within a particular website, rather than the entire web, as is the default behavior.I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"" (for example) in the search bar.Is this possible with the Google Cloud Vision API?""",Joy,'d like,1,28,34
0,51036159,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",Joy,appreciated,1,344,354
0,51036159,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",Joy,really appreciated,1,337,354
0,51036159,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",Trust,label,2,76,80
1,51036159,"""I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?How to get video codec when I upload a video into s3 bucket usingwithout label detection?Kindly provide your thoughts.Any inputs here really appreciated.""",Trust,label,2,276,280
0,44041039,"""I have an issue with detecting image whether it is painting image or real picture taken. I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that.Appreciate if you can share algorithm how to detect it.""",Trust,can share,1,205,213
0,44449053,"""I want to create an application android that films on the road as long as it is open, and if it detects an accident it sends a request to a database, I directly thought to google vision, but unfortunately it paid and so, I found watson's vision, how i can use it for android studio""",Sadness,unfortunately,1,192,204
0,47647693,"""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py file.The error i'm getting is as followsThanks in advance!""",Anticipation,in,1,152,153
0,47647693,"""I'm trying to build an application using this -Unfortunately, i'm getting a syntaxerror in the build.py file.The error i'm getting is as followsThanks in advance!""",Sadness,Unfortunately,1,48,60
0,50545515,"""I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.After this advice, I have not understood yet why Python threw ""too many open files"".I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.What I wrong? Please help me.My Environment isUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)Python 3.5.2google-cloud-vision (0.31.1)The error logs:The script exported above errors is following:""",Trust,label,1,57,61
0,55927749,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",Anticipation,results,1,352,358
0,55927749,"""I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:And never get's out. Not sure if anything is wrong with the code or configurations or whatnot.What are the problems? Why I don't see any results back? Here is the code also in the link.Looking at my SQS dashboard, thehas no available messages when running the code:""",Trust,credentials,1,130,140
0,53117283,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",Anticipation,result,4,87,92
1,53117283,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",Anticipation,result,4,189,194
2,53117283,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",Anticipation,result,4,373,378
3,53117283,"""I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.I have experienced the same with Microsoft's OCR API.So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?""",Anticipation,result,4,426,431
0,48473858,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""",Joy,LOVE,1,287,290
0,48473858,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""",Joy,I LOVE YOUI,1,285,295
0,48473858,"""I followed tutorial on codelabs developer google for Google vision api, it's worked fine for meThere is a method calledONTAP, when the user clic the camera screen TexttoSpeech, speak the text loud.Here is the method:NOW what i want to do is when the camera detect the sentence string:I LOVE YOUI want it to make in action in a TOAST for example to say:Ok this sentence has been detected.I tried this its not working:Please somebody helps me. Thanks you.""",Trust,LOVE,1,287,290
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,label,1,68,72
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,instructions,1,565,576
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,to authenticate,1,518,532
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,credentials,3,611,621
1,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,credentials,3,808,818
2,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,credentials,3,1066,1076
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,CREDENTIALS,2,711,721
1,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,CREDENTIALS,2,1558,1568
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,credential,1,847,856
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Trust,Credentials,1,1030,1040
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Joy,would like,1,221,230
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Joy,did like,1,651,658
0,38914432,"""I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.First of all, I want to send http request to the api locally to see that all its ok.In the end I would like to deploy my application to AWS EC2.My question is: if there is any problem to use vision api if I am using AWS EC2?I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this:"" The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:Go to the API Console Credentials page.From the project drop-down, select your project.On the Credentials page, select the Create credentials drop-down, then select Service account key.From the Service account drop-down, select an existing service account or create a new one.For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.""This is the right thing to do in my case?Thanks very much!""",Anticipation,instructions,1,565,576
0,48625509,"""I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?Thanks""",Anticipation,am preparing,1,3,14
0,48022812,"""I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.The code is written in Node & Express (returns the answer by GET request).I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.The JSON is structured like this:Which I wrap in an array belowSomehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.I hope someone could help me out. Have been stuck on this for some time now.""",Joy,would return,1,375,386
0,55619304,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",Trust,landmarks,2,100,108
1,55619304,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",Trust,landmarks,2,622,630
0,55619304,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",Trust,landmark,1,1301,1308
0,55619304,"""I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.Can anyone (especially from Google) shed light on this?The API reference for Google Cloud Vision's face landmarks:What I've learned about UNITS:I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.What I've learned about the REFERENCE POINT:On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).""",Joy,so good,1,64,70
0,46838135,"""When I load my application I get this:I am trying to follow this:I have run the command:Then on the Client Libary it is saying I have to set up a Client Library? I have done this with all he correct things then it says to-:And the execute this:Where I am stuck is, where do I execute this, how do I set the environment variable?""",Fear,command,1,81,87
0,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",Trust,label,3,125,129
1,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",Trust,label,3,204,208
2,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",Trust,label,3,307,311
0,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",Anticipation,order,1,281,285
0,42850135,"""I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'.""",Joy,would like,1,241,250
0,55708104,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",Disgust,are not working,1,619,633
0,55708104,"""I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:Complete log file of the error can be found in this link:.Here are mydocker-compose.ymlfile and theDockerfileof the service causing problem:DockerFiledocker-compose.ymlEDITAfter some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.I foundthat try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)""",Sadness,will fail,1,769,777
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Joy,:),1,1029,1030
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Joy,gem,2,211,213
1,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Joy,gem,2,240,242
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Joy,would be appreciated,1,1000,1027
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Joy,greatly would be appreciated,1,1009,1027
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Anticipation,expected,1,944,951
0,51500118,"""I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platformLink to the code samples :Steps to reproduce the errorsrails new test_appcd test_appadd gem 'google-cloud-vision' && gem 'google-cloud-storage' in gemfilebundle installadd config.autoload_paths += %W(#{config.root}/lib) in application.rbadd file ocr_google_test.rb in libin file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code :change line def detect_pdf_gcs with def self.detect_pdf_gcscreate a free key api and generate your key_file on the google platformcreate a bucket on google cloud storage to serve as source and destination urisadd arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.newgo in console rails consolelaunch OcrGoogleTest.detect_pdf_gcsRESULT : ArgumentError (wrong number of arguments (given 6, expected 0))If anyone could help me understand this, it would be greatly appreciated :)Thanks a lot and have a great day""",Anticipation,would be appreciated,1,1000,1027
0,52579907,"""I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in,,andhierarchy.However, when convertingtotext andtotext, I need to understand the DetectedBreak property.I went through.But I did not understand few of the them.Can somebody explain what do the following Breaks mean? I only understoodand.EOL_SURE_SPACEHYPHENLINE_BREAKSPACESURE_SPACEUNKNOWNCan they be replaced by either a newline char or space ?""",Anticipation,result,1,71,76
0,45176829,"""Since google vision has someon input image size, I want to first resize input image and then use thefunction.Here's theirthey useto open the image file. I wonder in this way, how to resize the imagein memoryand then call?""",Anticipation,I wonder,1,154,161
0,56301560,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""",Anticipation,result,1,366,371
0,56301560,"""I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.How do I determine which result is the best to retain and which one can be discarded?I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable""",Trust,the best,1,376,383
0,52549743,"""I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.below is the class of face graphicsany help?""",Trust,need to deal,1,94,105
0,51863232,"""I have one index calledwith documents containing a fieldthat contains blobs of text returned from the Google vision image transcription API. I have another indexwith documents containing afield (i.e. ""John Smith""). I'd like to run a query to return the top 5documents matched onto thefield of a givendocument. Can anyone help me out with this or point me in the right direction?""",Joy,'d like,1,217,223
0,53341184,"""I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""""",Joy,Like,1,397,400
0,51880350,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""",Anticipation,subscribed,1,265,274
0,51880350,"""Has anyone successfully completed a FaceSearch?I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OKI have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.Here is the code... (identifiers redacted)""",Joy,successfully,1,12,23
0,54008514,"""I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image.I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data. The field separator(|) is being read as ISo,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image havingcomb fields?Or Is there an option to blur or remove the pipes in between the letters before reading the text?""",Trust,support,1,339,345
0,55744464,"""Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects? Like Coca Cola brands or what ever you would like to detect?""",Joy,would like,1,162,171
0,44606352,"""By using Google Cloud Vision it detects face and returns only 34 facial landmark points.Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with?""",Trust,landmark,3,73,80
1,44606352,"""By using Google Cloud Vision it detects face and returns only 34 facial landmark points.Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with?""",Trust,landmark,3,137,144
2,44606352,"""By using Google Cloud Vision it detects face and returns only 34 facial landmark points.Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with?""",Trust,landmark,3,174,181
0,44606352,"""By using Google Cloud Vision it detects face and returns only 34 facial landmark points.Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with?""",Joy,like,1,190,193
0,54557026,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""",Disgust,is not supported,1,325,340
0,54557026,"""I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. From the official site:I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?Link to documentation of mlkit:""",Trust,official,1,115,122
0,56250899,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""",Trust,legal,1,110,114
0,56250899,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""",Trust,accurate,1,237,244
0,56250899,"""I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.So, is there any way to increase confidence of handwritten parts or can I customize API to do this?""",Anticipation,results,1,246,252
0,41434746,"""I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error.If I make the image public, everything works. But this is user data and can't be public. How can I solve this?My code for calling the vision api:My code for uploading images to storage:""",Trust,are authenticated,1,29,45
0,38967064,"""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you""",Anticipation,subscribe,1,97,105
0,38967064,"""Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?Thank you""",Joy,would like,1,200,214
0,53882629,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""",Joy,to achieve,1,173,182
0,53882629,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""",Joy,should achieve,1,404,417
0,53882629,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""",Joy,can achieve,1,442,454
0,53882629,"""I am developing an Android App that will scan Recharge card Pins and automatically recharge.I have tried to integrate different sdk`s of which have not reached what I want to achieve.I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.What I want is to scan only the Recharge card pins and ignore other text.CurrentlyThe app should achieve something like thisHow can i achieve this? Thank you.""",Trust,have managed,1,186,205
0,54418688,"""I would like to specify multiple modules to install by version number.If this is my:How can I adjust the contents of the mods list to specify the version number?Edit:The utilities.py file begins:The error I get after applying the advice from @jpeg, is as follows:My pip freeze is:The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear.""",Joy,would like,1,3,12
0,48749859,"""My users need to be able to authenticate themselves using a picture.So that when they create a account on the phone a picture is selected and saved.When they log in awithshould take a picture and compare it to the saved picture.I've found a possible duplicate, but this is very old and not really relevant anymore since the introduction of ARKit and Vision..I has to be done locally sois unfortunately not option, the same goes for&.The Vision Frameworks has aclass, that can detect faces, but I don't know how one would compare that to a saved one.""",Sadness,unfortunately,1,389,401
0,48749859,"""My users need to be able to authenticate themselves using a picture.So that when they create a account on the phone a picture is selected and saved.When they log in awithshould take a picture and compare it to the saved picture.I've found a possible duplicate, but this is very old and not really relevant anymore since the introduction of ARKit and Vision..I has to be done locally sois unfortunately not option, the same goes for&.The Vision Frameworks has aclass, that can detect faces, but I don't know how one would compare that to a saved one.""",Trust,to authenticate,1,26,40
0,37796580,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",Anticipation,instructions,1,113,124
0,37796580,"""I am trying to useMicrosoft face APIsoftware in. The first step says I need to authorize the API read below for instructions:Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:As an alternative, the subscription key can also be specified in the HTTP request header:When using a client library, the subscription key is passed in through the constructor of theclass. For example:The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.Now I am confused at how I would go about this. So first I tried using the query string way of doing it .But after i do this i get an error saying :""",Trust,instructions,1,113,124
0,56346851,"""I'm getting an error while sending Image for recognition to AWS Rekognition. This is the code which I use:And this is an error:Exception looks likeis empty, I have debugged and checked that ByteBuffer is valid and is not empty""",Trust,valid,1,205,209
0,45134020,"""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However, I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient.""",Anticipation,usually,1,29,35
0,45134020,"""Google Vision Post requests usually look like this:As far as I know, this only supports one 'type'.However, I want Google to analyze for two types:and.Is it possible to ask for both in one request?I'm currently sending two seperate requests, which is kind of inefficient.""",Trust,supports,1,80,87
0,51894464,"""I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below.What module should I import with anaconda? (I've already imported,,,,,,,,)Could anyone solve this? Thanks in advance.""",Anticipation,in,1,267,268
0,49014560,"""When I send a photo to the Microsoft Azure Face API (,), I am receivingBut when I am debugging the application and when I inspect the following codeactually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.P.S. I tried using different images and the behaviors is the same. I would appreciate any help.The application:""",Anticipation,result,1,190,195
0,56379854,"""I am querying the Azure Custom Vision V3.0 Training API (see) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:u'precision': 0.9859485, u'precisionStdDeviation': 0.0, u'recall': 0.3752228, u'recallStdDeviation': 0.0}The precision and recall uncertainties,andrespectively, always seem to be 0.0. Is this user error and if not are there any plans to activate these stats?""",Anticipation,Training,1,44,51
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Anticipation,likeliness,2,162,171
1,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Anticipation,likeliness,2,330,339
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Anticipation,results,1,469,475
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Surprise,results,1,116,122
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Surprise,surprise,1,194,201
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Surprise,surprised,1,291,299
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Anger,anger,1,184,188
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Anger,angry,1,282,286
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Sadness,sorrow,1,176,181
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Sadness,sad,1,277,279
0,35883234,"""I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine:The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.The likeliness of joy is something that moves easily by just making a fake smile.Is there something I can do to improve the sentiment analysis results?Currently I'm calling the API this way (using the Java client):""",Joy,joy,1,344,346
0,48075707,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""",Anticipation,results,3,88,94
1,48075707,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""",Anticipation,results,3,188,194
2,48075707,"""I used google vision online into get the detected text for an image file. I received 5 results.Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.Can anyone tell how to get the same results?""",Anticipation,results,3,291,297
0,52322574,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""",Joy,is,1,230,231
0,52322574,"""I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?""",Joy,well,1,241,244
0,55323321,"""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model""",Anticipation,to predict,1,76,85
0,55323321,"""I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this jsonThe model has been training for 24 hoursit should return the image predicated classes as trained by the model""",Anticipation,training,1,231,238
0,48721858,"""I've been accessing the Google Cloud Vision API from a script in a Google Sheet using UrlFetch and the REST API.Until I got ""UrlFetch failed because too much upload bandwidth was used"" I didn't even know there was a quota on UrlFetch!Is the a way to access Google Cloud APIs from a Google Apps Script so I can dodge the quota?""",Sadness,failed,1,135,140
0,46332861,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",Trust,have enabled,1,615,626
0,46332861,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",Trust,CREDENTIALS,1,881,891
0,46332861,"""I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:(Note that this is essentially copied and pasted from Google's docs at). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:Setting GOOGLE_APPLICATION_CREDENTIALS from the terminalGiving that service account ""Suite Domain-wide Delegation""Making the file publicDoing the ""gsutil"" command recommended by the answer here:Any ideas as to what the problem is?""",Sadness,:(,1,289,290
0,41228649,"""I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.I've made a few versions (), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)- original frame- opened in Photoshop, ""save for web""d- run through ImageOptim (which I believe compresses with jpegtran)Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.""",Trust,valid,1,56,60
0,42112519,"""I'm trying to disable multipledetection at time.How to disableusing, I couldn't find any solution from official siteI have downloaded sample fromCodeEven if remove below line, I cannot detect at all.""",Trust,official,1,104,111
0,54231123,"""I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?""",Trust,establishing,1,469,480
0,55005298,"""im trying to upload a image file to aws s3 using the following code and it gives an following error sayingrequests.expectations.HTTPError:400 Client Error: BadRequest for url:the complete error is attached as an image =the problem is mainly in upload part is the im not able to figure out the file_namewill it be rfid tag which is 0001249950 or will it be the scan variable that holds the value of rfid or some thing else""",Anticipation,expectations,1,116,127
0,51310239,"""Is there a way to filter the label results by category type when calling the Google Vision API?I have an iOS app that will take photos of food and want to only receive or read labels in category of food from the API.Thanks.""",Trust,label,1,30,34
0,51310239,"""Is there a way to filter the label results by category type when calling the Google Vision API?I have an iOS app that will take photos of food and want to only receive or read labels in category of food from the API.Thanks.""",Trust,labels,1,177,182
0,51310239,"""Is there a way to filter the label results by category type when calling the Google Vision API?I have an iOS app that will take photos of food and want to only receive or read labels in category of food from the API.Thanks.""",Anticipation,results,1,36,42
0,48056251,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""",Joy,would be appreciated,1,379,405
0,48056251,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""",Joy,highly would be appreciated,1,388,405
0,48056251,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""",Anticipation,would be appreciated,1,379,405
0,48056251,"""If you visitand scroll down a bit you will see the sectionIf I drag and upload a Kannada (Indian language)file it reads it and shows text.Can I know the underlying query string or URL so that I can pass the image files through my script?PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.Any help would be highly appreciated.""",Disgust,support,1,273,279
0,54118524,"""I'm making a request with the google vision api that appears to have worked, I get an operation number back. The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.This is the script I ranThis returns backand when I do a lookup on the operation I get thisHowever the output folder is completely empty and I am not sure what to make of the state created.""",Anticipation,results,1,176,182
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,certain,2,736,742
1,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,certain,2,1388,1394
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,label,3,841,845
1,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,label,3,1127,1131
2,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,label,3,1249,1253
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,labels,3,1212,1217
1,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,labels,3,1452,1457
2,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Trust,labels,3,1783,1788
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Anticipation,training,2,819,826
1,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Anticipation,training,2,881,888
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Anticipation,result,1,914,919
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Anticipation,results,1,1599,1605
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Anticipation,more training,1,876,888
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Joy,good,3,804,807
1,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Joy,good,3,965,968
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Joy,set,1,828,830
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Joy,is,1,799,800
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Joy,would be,1,946,953
0,49120897,"""I am new to Machine/Deep learning area!If I understood correctly, when I am using images as an input,the number of neurons at input layer = the number of pixels (i.e resolution)The weights and biases are updated through back-propagation to achieive low as possible error-rate.Question 1.So, even one single image data will adjust the values of weights & biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )Question 2.If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?Question 3. (continue)A bit different angle question,There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.(e.g, when i put an online game screenshot, it will produce as below,)Can this type of data be used as an input to MLP to categorise certain type of images? ( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.If so, what would be the performance difference ?e.g) when classifying 10 different types of images,(pixels trained model) vs (output labels trained model)""",Fear,missing,1,490,496
0,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Anger,conflicts,1,305,313
0,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Anger,conflict,2,413,420
1,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Anger,conflict,2,534,541
0,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Sadness,conflicts,1,305,313
0,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Sadness,conflict,2,413,420
1,53955753,"""I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at, but I just ran into this error (Error updating property googleVisionBarcodeDetectorEnable)that requires an upgrade to.It looks like there are Google Play Services and Firebase conflicts whenis bumped up tofrom:I've triedbut it gave meerror. Bumping up to 17.0.2 would cause a version conflict from.Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?Here is the dependencies in android/app/build.gradleExt in android/build.gradlePackage:""",Sadness,conflict,2,534,541
0,49380672,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",Sadness,apologize,1,3,11
0,49380672,"""I apologize if my question is not worded properly, I don't post here very often.I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:The error I am getting is this:What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through.The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??Thanks so much,Dhruv""",Trust,is confirmed,1,382,393
0,47415721,"""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But, when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?""",Anticipation,result,1,148,153
0,47415721,"""I want to learn Microsoft Emotion API on android.So, I try to run the Android SDK example.()But, when I select a photo, there is crash and exit on result scene.this is log.And this is line 224How can I fix it?""",Sadness,crash,1,130,134
0,54788081,"""I'm trying to use the google vision api facetracker sample ()to detect and track faces without showing the preview (at least detect, tracking is not necessary).The main problem is that if I delete the ""preview"" I don't receive the callback in ""onNewItem(int faceId, Face item)""  (function in FaceTrackerActivity in the github link provided).I've been lookin through SO and I didn't find anything similar to this question, also it's not necessary that I use ""google vision api"", if you know another system that can achieve this objective it's perfect.Thanks for reading.""",Joy,can achieve,1,511,521
0,53029413,"""I'm reading this article :and reproduced the sample application. It currently works.On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :Exception from HRESULT: 0x88900105When the program go on this line :It is a little tricky to know where it comes from because the exception is not very explicit.I would like to know if you have encountered the same problem or have an idea where it might come from.Edit: steps for reproduce the problem.Download my model here :Clone the repository from GitHub :Run the sample with a plane picture, the sample works.Now In the solution, replace the existing (and working) PlanesModel.onnx by mine.We get the exception.Here all my project's configuration:""",Joy,would like,1,452,461
0,44635222,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""",Anticipation,order,3,104,108
1,44635222,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""",Anticipation,order,3,150,154
2,44635222,"""Google Vision API documentation states that vertices of detected characters will always be in the same order:However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:andWhy order of vertices is not the same? and not as in documentation?""",Anticipation,order,3,266,270
0,43383886,"""in google vision api label detection, can't know where object located ? any options or idea ??I have tried in sample, and then response json is does not include object position!""",Trust,label,1,22,26
0,55603140,"""I have a collection of profile images from customers I need to be able to pass a selfie of the person and scan it across the collection of images and pull up the customer information.Need to do the following using AWS Rekognition -Create a collection - DoneAdd Images to the collection - Whats the REST API syntax for thisWhile adding the images to the collection also tag it with the customer name.Take a selfie portrait and search across the collection and return the tag information which matches.Im using Flutter as a platform hence there is no support for AWS SDK so will need to make REST API calls.However the AWS docs don't provide much information for REST support.""",Disgust,support,1,550,556
0,55603140,"""I have a collection of profile images from customers I need to be able to pass a selfie of the person and scan it across the collection of images and pull up the customer information.Need to do the following using AWS Rekognition -Create a collection - DoneAdd Images to the collection - Whats the REST API syntax for thisWhile adding the images to the collection also tag it with the customer name.Take a selfie portrait and search across the collection and return the tag information which matches.Im using Flutter as a platform hence there is no support for AWS SDK so will need to make REST API calls.However the AWS docs don't provide much information for REST support.""",Trust,support,1,667,673
0,43404118,"""Recently,I use google cloud vision api for detecting image label ,follow,I  set up the  credentials for my application to authenticate its identity to the service and obtain authorization to perform task,then follow the API Documentation ,write code like this:I get error :why?""",Trust,label,1,60,64
0,43404118,"""Recently,I use google cloud vision api for detecting image label ,follow,I  set up the  credentials for my application to authenticate its identity to the service and obtain authorization to perform task,then follow the API Documentation ,write code like this:I get error :why?""",Trust,credentials,1,89,99
0,43404118,"""Recently,I use google cloud vision api for detecting image label ,follow,I  set up the  credentials for my application to authenticate its identity to the service and obtain authorization to perform task,then follow the API Documentation ,write code like this:I get error :why?""",Trust,to authenticate,1,120,134
0,51934306,"""We are just thinking whether to validate some input for google vision (OCR).We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.I can't find it the documentation. Does anyone know?""",Anticipation,result,1,139,144
0,50767594,"""I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:I basically serialized the protobuff into a string and then used regex to get the data that I want.There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.Here is the file from the github repository:Thank you all in advance.""",Anticipation,in,1,670,671
0,42702426,"""I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.I've been experimenting with the API and using PHP with no success.I've been looking through all the tutorials and always get stuck on some point.This is the closest I've came so far in phpBut then I get this error.I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.Does anyone have any experience with the google vision API that can help me out? Preferably with the android part, help me get on my way or help me get started?Thanks ahead.""",Joy,would like,1,75,84
0,49877255,"""I am trying to extract text from such images but Google Vision API does not seem to recognise majority of the text, Can someone suggest a better alternative?Results from Google OCR""",Anticipation,Results,1,158,164
0,55334563,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""",Anticipation,result,1,137,142
0,55334563,"""I'm trying to recognize vertical text using google cloud vision. Image example:I use Try This API onto test the engine.Request body:The result isAm I missing something? Thank you.""",Fear,missing,1,151,157
0,43041575,"""I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this?Thanks to all""",Anticipation,I was wondering,1,1,15
0,37508678,"""I've been testing an application example using Google Vision API.I have the code from their github, and created the relevant project and credentials file (something Google Cloud requests) and tried to run the code.I get the following:I've tried to add the relevant SSL certificates to my JRE's cacerts, I've restarted eclipse and an admin, but I still get this.EDITI've also tried to manually set the ssl properties as follows:Any ideas to resolve this issue, please?If there's anything I should provide, please let me know.Thanks in advance.""",Anticipation,in,1,532,533
0,37508678,"""I've been testing an application example using Google Vision API.I have the code from their github, and created the relevant project and credentials file (something Google Cloud requests) and tried to run the code.I get the following:I've tried to add the relevant SSL certificates to my JRE's cacerts, I've restarted eclipse and an admin, but I still get this.EDITI've also tried to manually set the ssl properties as follows:Any ideas to resolve this issue, please?If there's anything I should provide, please let me know.Thanks in advance.""",Trust,credentials,1,138,148
0,48846121,"""So I'm trying to use Google's vision api, where it takes recognizes the labeles,facial, and text detections....etcBut it unfortunately, I can't fix an error that's causing us to fall behind .SOURCE CODEIt says ""Client"" object has no attribute, ""before_request""That's where I'm stuck, and I'm not sure what to do from there.ERROR""",Sadness,unfortunately,1,122,134
0,37264402,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""",Trust,associated,2,32,41
1,37264402,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""",Trust,associated,2,139,148
0,37264402,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""",Trust,enable,1,223,228
0,37264402,"""I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?""",Trust,credentials,1,127,137
0,45695542,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""",Joy,Funny,2,78,82
0,45695542,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""",Anticipation,suspect,1,147,153
0,45695542,"""I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.Here's the deal:The response I get always is:If I try using URI instead:Response is ok...I've followed thefrom GoogleAny idea what is wrong here?""",Trust,deal,1,212,215
0,55283215,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",Fear,missing,1,310,316
0,55283215,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",Joy,like,1,73,76
0,55283215,"""I have a binary text image like this oneI want to perform OCR on images like these. They contain no more than one word.I have tried tesseract and Google cloud vision but both of them return no results.I'm using python 3.6 and Windows 10.This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!EDIT:Thanks toF10for pointing me in the right direction. This is how I got it to work with a local image.""",Surprise,results,1,194,200
0,49842534,"""I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml fileNow if doThen loc1 is {0,0} which is expected but loc2 is{-180,0}.Why is that? Shouldn't it be also 0,0?Thank youHere is the UI part from dympsys""",Anticipation,is expected,1,159,169
0,56031856,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""",Trust,certain,1,42,48
0,56031856,"""I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?""",Trust,landmark,1,256,263
0,49187806,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",Anticipation,in,1,898,899
0,49187806,"""I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting aProvisionedThroughputExceededExceptionerror as the result. I tried the Amazon Rekogntion provided demo page as well, and I got anas shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance""",Anticipation,result,1,319,324
0,50513107,"""I need to parse data from a Visa Payment QRCode withlibrary from VisaBut gradle build failed with minSdkVersion < 21 and throw transformClassesWithDesugar bellowI triedIf I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.""",Sadness,failed,1,87,92
0,54581027,"""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.""",Trust,associated,1,37,46
0,54581027,"""I am using Google Vision API to get associated labels for an image.Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.""",Trust,labels,1,48,53
0,51350903,"""In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doingResult:And I made this by querying like this""",Joy,like,1,143,146
0,46594701,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",Trust,label,1,48,52
0,46594701,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",Trust,labels,1,92,97
0,46594701,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",Anticipation,results,1,307,313
0,46594701,"""I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes availableThis is the code:And this and that aside it displays the results:""",Joy,would like,1,3,12
0,48085989,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",Sadness,to crash,1,276,283
0,48085989,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",Sadness,crashes,1,646,652
0,48085989,"""I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the oldface detector and I want to update it to the new Google vision APIsFor some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.Here is the relevant code.The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.This is the error that I see in adb logcatIt is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.I did also make a change to my build.gradle file. Here it is.I added the lineUpdateI have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.ThanksJohn Lawrie""",Anger,is complaining,1,482,495
0,51676317,"""Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:And this is the Lambda function code:When I run it, this is the Lambda function error shown in Cloudwatch:""",Disgust,didn't work,1,177,187
0,36728347,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""",Anticipation,instructions,1,161,172
0,36728347,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""",Fear,command,1,308,314
0,36728347,"""I just tested the Google Cloud Vision API to read the text, if exist, in a image.Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf??I am using the following command:Inside the text directory, I have the files in jpg format.Then to read the converted file, I don't know how to do that, just I run the following commandAnd I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?Thanks!""",Trust,instructions,1,161,172
0,47250652,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",Trust,labels,1,35,40
0,47250652,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",Trust,instructions,1,411,422
0,47250652,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",Anticipation,instructions,1,411,422
0,47250652,"""I am trying to use theto read the labels for a image.I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authenticationI keep getting the following errorThis the code I am executingUp until lineEverything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error.Pretty much following the instructions on thisNot very sure what is going wrong""",Surprise,suddenly,1,361,368
0,44594617,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",Anticipation,risk,1,241,244
0,44594617,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",Fear,risk,1,241,244
0,44594617,"""The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.How can I back up a custom classifier created using IBM Watson Visual Recognition? This questionand I am hoping someone from IBM can provide guidance here.Thank you!""",Surprise,Accidental,1,155,164
0,51273104,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""",Trust,label,2,16,20
1,51273104,"""When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case. According to thistopicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.Are these supposed to be always the same? What does that mean?""",Trust,label,2,247,251
0,40714481,"""I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directoryD:\Image\abc.jpg)The example code, as shown in their, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:But it does not seem to work. It seems that there is a method for Java (using). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.""",Anticipation,I 'm wondering,1,414,426
0,51961697,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""",Anticipation,likeliness,1,337,346
0,51961697,"""I am searching for the answer on this question on the internet, but can't find it. I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?And does anybody know how about for Microsoft Cognitive Services?""",Trust,certain,1,364,370
0,52928909,"""I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance""",Anticipation,in,1,189,190
0,52928909,"""I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.dependency used-Exception i am getting -Can anyone help me with this??Thanks in advance""",Disgust,does not work,1,86,98
0,46764797,"""I'm testing outand noticing it's replacing underscores with spaces.Does the API provide a feature to ensure these underscores are not omitted?Code example:Here is the example_image.png that I'm using:Which produces the output:(no underscore).""",Sadness,:(,1,226,227
0,46921518,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""",Anticipation,am attempting,1,3,15
0,46921518,"""I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using thelibrary . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.I think updated my application.properties file to include the value like so:I'm also setting my property source in my controller like so:However, after doing that I'm still getting an error saying:""",Trust,credentials,1,106,116
0,48140339,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""",Anger,conflict,1,69,76
0,48140339,"""I am using guava 23-5 in my application and1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use. Can some one let me know how can I get around this?VersionsEDITAs mentionedI tried to shade the Hbase dependency in a new module named.Then excluded hbase & hadoop dependencies from the module and addedas dependency. The dependency tree of maven looks like belowBut I am still getting the same error.""",Sadness,conflict,1,69,76
0,51959287,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""",Trust,credentials,2,154,164
1,51959287,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""",Trust,credentials,2,208,218
0,51959287,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""",Anticipation,attempting,1,101,110
0,51959287,"""Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".The error that I am receiving is:ServiceResponseException: The API query failed with status code Unauthorized: UnauthorizedHere is my code:Also, let me know if I can provide anymore information that might help""",Sadness,failed,1,294,299
0,48521816,"""I want to integrateAmazon Rekognitionfor the Face Recognition.I have created bucket and IAM user. I am trying to hit""RekognitionService.ListCollections""for the testing in POSTMAN but getting error as follows;My request header is as follows;Can anyone please guide me how to test AWS apis in POSTMAN ?""",Trust,guide,1,259,263
0,56155219,"""I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected.I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.This is the OcrGraphic activitythis is my Send Email activityI want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.""",Joy,is welcomed,1,548,558
0,42761695,"""I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. [I have the access permissions for VideoIntelligence API for my personal-project]Please provide some suggestions to make it work.I tried the following commands:But I am getting this as the Error:It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.[since I have access for 'personal-project' and not 'usable-auth-library']How can I make this work ? Any Suggestions please ?Thanks""",Fear,commands,1,253,260
0,51803569,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""",Anticipation,results,2,126,132
1,51803569,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""",Anticipation,results,2,372,378
0,51803569,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""",Anticipation,expect,1,636,641
0,51803569,"""I'm using theto extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.every time I change the image from the original I lose accuracy in detecting some characters.I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.Example:some variations on the image from gray scale or b&wWhat ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?some ideas I already tried, also some thresholding.""",Sadness,have isolated,1,242,254
0,40893623,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""",Anticipation,in,1,449,450
0,40893623,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""",Anticipation,results,1,104,110
0,40893623,"""We noticed that Google Vision API doesn't work well if an image has a lot of text.It returns 'strange' results.Here is an exapmle:- Will return something like this:If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision).We tried on different images and get the same problem.Can you advise us if we are doing something wrong or this is problem on Google's side?Thank you in advanced!""",Disgust,doesn't work,1,35,46
0,40949801,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""",Trust,valid,1,269,273
0,40949801,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""",Trust,enable,1,305,310
0,40949801,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""",Trust,to enable,1,541,549
0,40949801,"""Is there a way to test the Google Vision API in an application without activating my free trial?I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added.Error Message:  "" The API requires a valid billing method.""When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial?I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...""",Anger,wasting,1,690,696
0,52877398,"""The problem I am facing is my application needs to support older devices ( at least API 14 ) so I am using google play service maps 7.3.0.However, I am unable to use newer play service APIs such as Google Vision because my google play service version is low.The problem is my users are unable to update their google play services app so I cannot simply upgrade my library.""",Sadness,unable,2,153,158
1,52877398,"""The problem I am facing is my application needs to support older devices ( at least API 14 ) so I am using google play service maps 7.3.0.However, I am unable to use newer play service APIs such as Google Vision because my google play service version is low.The problem is my users are unable to update their google play services app so I cannot simply upgrade my library.""",Sadness,unable,2,287,292
0,52877398,"""The problem I am facing is my application needs to support older devices ( at least API 14 ) so I am using google play service maps 7.3.0.However, I am unable to use newer play service APIs such as Google Vision because my google play service version is low.The problem is my users are unable to update their google play services app so I cannot simply upgrade my library.""",Trust,needs to support,1,43,58
0,49590288,"""I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:What is the cause for this issue and is there a way around it?""",Anticipation,results,1,211,217
0,46718939,"""I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl( image/png ) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.Thanks!""",Sadness,Invalid,1,303,309
0,53844994,"""I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).I had check below option.1.The palette library attempts to extract the following six color profiles.2. Get color of a particular pixelif I break bitmap in small size then find out color and save in list.Then there is time taken and OutOfMemoryError.Please suggest any library  to find color of images.EditPick from gallaryGet Color code from Pixel""",Anticipation,attempts,1,153,160
0,47368685,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""",Anticipation,I was wondering,1,526,540
0,47368685,"""Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this.""",Sadness,unable,1,374,379
0,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,results,3,61,67
1,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,results,3,257,263
2,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,results,3,312,318
0,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,result,3,94,99
1,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,result,3,118,123
2,48491815,"""So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points.""",Anticipation,result,3,181,186
0,45680183,"""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log (), how do I get this""id""received and use it in""Get"" to return the results?The API name is:""",Anticipation,results,2,100,106
1,45680183,"""I need to do a""Post""to get thetextOperationsand use this received value to do a""Get""and return the results.I'm doing the""Post""however I do not get anything in console.log (), how do I get this""id""received and use it in""Get"" to return the results?The API name is:""",Anticipation,results,2,239,245
0,51918848,"""I have two AWS lambda functions which are processing all images uploaded to an S3 bucket(One is for creating thumbnail and another is for image moderation[rekognition]).While I am doing it, I found it invalid to add event notifications with overlapping prefix and suffix.For example, let's assume that I want to set the two event notification like the below.If it is not available I think there must be a kind of pattern commonly used in this case(for instance, making a proxy lambda to call the two lambdas passing the same event notification.)What is the best way to handle the case?""",Joy,like,1,344,347
0,51918848,"""I have two AWS lambda functions which are processing all images uploaded to an S3 bucket(One is for creating thumbnail and another is for image moderation[rekognition]).While I am doing it, I found it invalid to add event notifications with overlapping prefix and suffix.For example, let's assume that I want to set the two event notification like the below.If it is not available I think there must be a kind of pattern commonly used in this case(for instance, making a proxy lambda to call the two lambdas passing the same event notification.)What is the best way to handle the case?""",Sadness,invalid,1,202,208
0,51918848,"""I have two AWS lambda functions which are processing all images uploaded to an S3 bucket(One is for creating thumbnail and another is for image moderation[rekognition]).While I am doing it, I found it invalid to add event notifications with overlapping prefix and suffix.For example, let's assume that I want to set the two event notification like the below.If it is not available I think there must be a kind of pattern commonly used in this case(for instance, making a proxy lambda to call the two lambdas passing the same event notification.)What is the best way to handle the case?""",Trust,the best,1,554,561
0,45372938,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""",Trust,to authenticate,1,12,26
0,45372938,"""I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using theenvironmental variable which specifies the path to the JSON file itself.However, I am required to specify this in my application itself and authenticate using the JSON file contents.Now, I have tried to specifyto then pass it in as a parameter to themethod. Sure enough, aobject can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter toseems to make no difference as themethod is still looking for the environmental variable and throws anexception, specifying that the environmental variable cannot be found.Any idea how I can get the desired behavior?""",Trust,authenticate,1,235,246
0,43598191,"""I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""Anyone knows how I could fix it?""",Sadness,Invalid,1,112,118
0,39616361,"""I am getting an error while querying Google Vision API:I have passed a pdf file which contains images and then extracted image usingto createlistAnd passed the list created above to vision service :I have also tried removing the base64 encoding of image bytearray, but still get the same error listed on the top.The bytearray length is ""774800""Is there something which I am missing because when I multipart an image to the servlet and pass the bytearray obtained from the inputstream it works fine.I am running the application on Tomcat V8dependecies used :""",Fear,missing,1,375,381
0,42663690,"""I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:1) ""textAnnotations"" 2) ""fullTextAnnotation""I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.""",Anticipation,result,1,655,660
0,39982559,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""",Anticipation,immediately,1,641,651
0,39982559,"""I've been using Google Vision API to perform OCR tasks in some documents using Python.It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).How can I avoid having such error message without having to restart the process? Can anyone help me?Thanks a lot!EDIT:This is the code of the request (part of it).""",Trust,decided,1,237,243
0,50815200,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""",Trust,label,2,602,606
1,50815200,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""",Trust,label,2,708,712
0,50815200,"""I have been exploring to get the count of the objects in an image / video using AWS Rekognition & Google's Vision, but haven't been able to find a way out. Though atsite, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured.Attached is a snapshot from that URL.Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. ThanksEdit:For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.""",Surprise,have been exploring,1,3,21
0,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Joy,would be appreciated,1,1635,1662
0,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Joy,greatly would be appreciated,1,1644,1662
0,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Joy,would like,1,137,146
0,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Anticipation,would be appreciated,1,1635,1662
0,43740356,"""After using the Google Cloud Vision API, I received MID values in the format of(not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the termVehiclemight belevel 1while the termVanmight belevel 2.I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I foundthat discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!""",Sadness,unfortunately,1,420,432
0,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Anticipation,result,3,132,137
1,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Anticipation,result,3,398,403
2,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Anticipation,result,3,770,775
0,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Joy,fantastic,1,1027,1035
0,40013910,"""I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this?Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this. As Google announced, their TEXT_DETECTION was fantastic: ""Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt"". But for some of my pics, what happened was really funny. For example with, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm.""",Joy,funny,2,1273,1277
0,38580989,"""I would like to classify images by calling Watson Visual Recognition APIs.So, I set my end point as(a) End Point :"""";(b) Captured Request Message(c) Captured Response Message""",Joy,would like,1,3,12
0,47478044,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""",Anticipation,usually,1,164,170
0,47478044,"""I'm using Google Cloud Vision API to perform OCR on my documents. Inpage, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.My question is:1- Does OCR of Vision API care about colors for example in RBG format?2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?3- If not, why isn't it documented in the optimization page?""",Trust,worth,1,528,532
0,44915072,"""I have an IOS app where users can upload images.I want to run all these images through Google's Vision API.Could someone please let me know how realistic this idea is?Let's say that I want to run 1000 images through their API.How much would this cost me?In the question title, I used the word scalable because I'm worried that using this service would be really expensive.This is mainly a question about how much it costs to get Google to scan each an image.Thanks!""",Anticipation,'m worried,1,312,321
0,44915072,"""I have an IOS app where users can upload images.I want to run all these images through Google's Vision API.Could someone please let me know how realistic this idea is?Let's say that I want to run 1000 images through their API.How much would this cost me?In the question title, I used the word scalable because I'm worried that using this service would be really expensive.This is mainly a question about how much it costs to get Google to scan each an image.Thanks!""",Fear,'m worried,1,312,321
0,55408126,"""I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:""",Trust,have enabled,1,51,62
0,55408126,"""I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:""",Trust,enabled,1,100,106
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Joy,good,2,443,446
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Joy,starter,1,448,454
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Joy,is,1,440,441
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Trust,guide,1,146,150
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Trust,support,1,60,66
0,52614091,"""Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api.kind of like starter tutorialref:The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json formatShould i use my windows command line or cloudshell on google cloudThere is good starter reference for other services , if you look at this, they have clearly given commands to use in different envs""",Anticipation,results,1,339,345
0,50685445,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",Anticipation,in,1,724,725
0,50685445,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",Sadness,invalid,1,553,559
0,50685445,"""I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.""",Trust,Rules,1,540,544
0,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",Anticipation,results,1,158,164
0,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",Anticipation,order,1,545,549
0,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",Trust,label,1,84,88
0,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",Trust,labelling,1,494,502
0,48857882,"""I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?Here's an example of a request:and the code:I send a list of requests withand get. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:The only thing I could come up with is:It does the trick, but seems to be rather convoluted when compared to labelling a.Is there a better way?If not, I assume order of my requests passed in a list tomatches exactly the response I get?""",Joy,would like,1,3,12
0,51317429,"""i want to set CCL option in google vision api..but api document is not support this infomationI found that Google Image Search provides the following URL.tbs=sur:fcI wonder if this is also available in the GOOGLE VISION API. If possible, I want to know the URL that contains the method.""",Disgust,support,1,72,78
0,48262231,"""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After inclusion of these lines in the root-context its gives page not found.""",Trust,certain,1,165,171
0,48262231,"""The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-After inclusion of these lines in the root-context its gives page not found.""",Trust,credentials,1,203,213
0,51744886,"""I try OCR image by Google Vision API:Result text return from API:Result text i get from website of google:From website of google :It can get correct value [], but from API , it only return []Why result of google vision api difference with ocr from website?""",Anticipation,Result,2,38,43
1,51744886,"""I try OCR image by Google Vision API:Result text return from API:Result text i get from website of google:From website of google :It can get correct value [], but from API , it only return []Why result of google vision api difference with ocr from website?""",Anticipation,Result,2,66,71
0,51744886,"""I try OCR image by Google Vision API:Result text return from API:Result text i get from website of google:From website of google :It can get correct value [], but from API , it only return []Why result of google vision api difference with ocr from website?""",Anticipation,result,1,196,201
0,46483447,"""I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition () with examples only in Java and very limited.I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels.I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Bodyhow can I initialize Rekognition and build a Request.Thanks you!""",Trust,labels,1,416,421
0,54133439,"""So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly.From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :Basically making the video fill the screen then centering.So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.Before the frame is uploaded i turn it into base64 using a canvas as follows:I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?""",Trust,landmarks,1,54,62
0,56040881,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",Joy,will be appreciated,1,777,803
0,56040881,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",Joy,greatly will be appreciated,1,785,803
0,56040881,"""I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:I didn't have to include the parametersince according to the API documentation seen, it was optional and the default value waswhich was what I already wanted. However if I add the parameter(just in case I change my mind and switch to something else) in the request as shown below:The API returns a response status codeand status description. The whole JSON response is below:I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response issince I am using aimage file that is supported and set its content type asin the request.Any help will be greatly appreciated.""",Trust,is supported,1,713,724
0,49890108,"""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:""",Sadness,bottom,1,119,124
0,49890108,"""So I'm trying to follow The microsoft face api documentationfor the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:""",Sadness,invalid,1,222,228
0,38534147,"""Maybe the answer is simple, however I could not find anything that could help me yet.Basicaly, I want to add the Google Vision API to my project. I tried this by puttinginin the Android module, like in this. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this. There is said that there components cannot be applied to.I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.So how to make it work with LibGDX?""",Disgust,did not work,1,214,225
0,50498421,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""",Anticipation,results,1,48,54
0,50498421,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""",Fear,missing,1,415,421
0,50498421,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""",Joy,successfully,1,392,403
0,50498421,"""I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches.I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.Here is output from API:I have isolated the timestamps (just testing my approach) using the following:However, when I try the same thing with FaceMatches, I get an error.What I need to end up with is for each face that is matched:Can anybody shed some light on this for me?""",Sadness,have isolated,1,480,492
0,49067894,"""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.""",Joy,will be appreciated,1,112,137
0,49067894,"""I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.""",Joy,highly will be appreciated,1,120,137
0,54574681,"""I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image urlbelow are the things i have triedbelow is the code""",Sadness,invalid,1,222,228
0,50181484,"""I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.My bucket's region is us-east-1 and I have configured the same in my code.what am I doing wrong?""",Sadness,Unable,1,260,265
0,49915680,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""",Disgust,doesn't work,1,202,213
0,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",Joy,:),1,470,471
0,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",Joy,achieve,1,369,375
0,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",Anticipation,in,1,460,461
0,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",Sadness,fails,1,132,136
0,54821969,"""I am trying to detect and grab text from a screenshot taken from any consumer product's ad.My code works at a certain accuracy but fails to make bounding boxes around the skewed text area.Recently I triedGoogle Vision APIand it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar!My test image:Google Vision API after bounding boxes:Thank you in advance:)""",Trust,certain,1,111,117
0,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Anticipation,training,2,208,215
1,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Anticipation,training,2,545,552
0,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Anticipation,expect,1,464,469
0,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Anticipation,Training,1,515,522
0,55464541,"""I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:Error encountered while training.Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.How do I fix this problem?I renamed the classes with names with no spaces.Zip file names don't contain spaces.I deleted and remade the project.I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""""",Surprise,Unexpected,1,261,270
0,50764331,"""I'm using the Google Cloud Vision API to detect landmarks, webEntities and other things from a given image (check the docs), I am specifically using the images:annotate endpoint, and I want to specify the language, I want the returned results to be in English.Is there a way I can achieve that?""",Anticipation,results,1,236,242
0,50764331,"""I'm using the Google Cloud Vision API to detect landmarks, webEntities and other things from a given image (check the docs), I am specifically using the images:annotate endpoint, and I want to specify the language, I want the returned results to be in English.Is there a way I can achieve that?""",Joy,can achieve,1,278,288
0,50764331,"""I'm using the Google Cloud Vision API to detect landmarks, webEntities and other things from a given image (check the docs), I am specifically using the images:annotate endpoint, and I want to specify the language, I want the returned results to be in English.Is there a way I can achieve that?""",Trust,landmarks,1,49,57
0,41959043,"""I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text.As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?""",Joy,to like,1,503,509
0,53739849,"""AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :When  the function is called in AWS it also  complains in the cloudwatch logs with :Here is  the full function provided by AWS :Can rename  this  public class and file name. Did not need to create a separate file.How can I correct this code so no errors are showing for the public class?""",Anger,complains,1,183,191
0,34925092,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",Anticipation,couldn't discover,1,371,387
0,34925092,"""I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.Invert the bitmap on the fly is very slow and is out of question.I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?Thank you for your attention.""",Disgust,doesn't support,1,766,780
0,46962077,"""how do I have to ""translate"" the following curl command into a valid php curl function?It seems that I'm doing something wrong and I can't figure out the problem:I tried several variations and the Watson Visual Recognition API error is now:before it was:Thank you for your help!""",Fear,command,1,49,55
0,46962077,"""how do I have to ""translate"" the following curl command into a valid php curl function?It seems that I'm doing something wrong and I can't figure out the problem:I tried several variations and the Watson Visual Recognition API error is now:before it was:Thank you for your help!""",Trust,valid,1,64,68
0,38480586,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",Anticipation,training,2,374,381
1,38480586,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",Anticipation,training,2,439,446
0,38480586,"""The documentation for the Watson Visual Recognition Services indicates that the costs for the service areSo if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would beis my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?""",Anticipation,more training,1,369,381
0,42041693,"""I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TTI try to detect faces by sending an image file from an external storage(from the emulator)I don't know what i did wrong(I'm not good at coding)Here is my codeand here is my errorswhat is a null object reference?i try to change the file path but he said no such file ... and when I change to this path, there's errors above.by the way I've already asked a user for a permission to access a folder from Emulator in Androidplease help mePS. sorry for my bad EnglishThank you in advance.""",Anticipation,in,1,693,694
0,43631861,"""I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.The error says""",Joy,successfully,1,45,56
0,50500341,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""",Joy,would be appreciated,1,202,229
0,50500341,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""",Joy,greatly would be appreciated,1,211,229
0,50500341,"""I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. I do have a server up and running atand the app source code looks like.Any help would be greatly appreciated.""",Anticipation,would be appreciated,1,202,229
0,53756545,"""I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this linkBut I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .""",Joy,would like,1,74,83
0,50866887,"""I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.return base64 of image that has captured.Below is my code so far:Thanks in advance!""",Anticipation,in,1,219,220
0,46078769,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""",Joy,enjoying,3,41,48
0,46078769,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""",Joy,very much,1,53,61
0,46078769,"""I'm playing with the Azure Face API and enjoying it very much.I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?Thanks!""",Anticipation,I was wondering,1,63,77
0,40450515,"""I have a very basic python app that calls the google vision API and asks for OCR on an image.It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked.All my images are ~500kBHowever, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...The google quotas limit page lists:And I am way below any of these limits (by orders of magnitude) - any idea what might be going on?It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand?""",Anticipation,orders,1,494,499
0,42657315,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""",Trust,Label,1,97,101
0,42657315,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""",Trust,official,2,375,382
1,42657315,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""",Trust,official,2,476,483
0,42657315,"""I am in process of integration Google Cloud Vision API to my app.I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator.The issue is reproducible at official GoogleTested on 2 different real devices and 2 emulators.Any ideas why it happens?Code from official sample:Time results for processing one image after 3 tests:real device484096084714983757341848430621961emulator772568251779670069775478519794I used simple""",Anticipation,results,1,497,503
0,44373059,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""",Sadness,Invalid,1,141,147
0,44373059,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""",Sadness,failed,1,216,221
0,44373059,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""",Anticipation,results,1,361,367
0,44373059,"""I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.I get the error:Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""I try a different image on both servers and it works on both web servers and Google Vision API returns good results.The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.I would paste the Base64 data here, but it is a lot of text...Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?""",Disgust,does not work,1,89,101
0,50199393,"""I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:Python code can be found here:""",Joy,successful,1,60,69
0,51590523,"""The question has beenand the commercial solution from BlinkID is working well for me.I am trying to extend the application to recognise more type of document. For the moment I am using Google Cloud Vision with aalgorithm to (heuristic) detect the keyword (,,etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short () and not always visible etc.Having no background in computer vision, I'm looking for a way to improve the current solution or a better one.""",Anticipation,result,1,272,277
0,49648719,"""I am trying Watson visual recognition with Python, following this:while tried to install the library:I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.I have tried to install twisted by from .wl asandboth failed with error:I am using Windows 7 64""",Sadness,failed,1,284,289
0,50273951,"""Can you please help with list of video formats and codecs that are supported by Google Cloud Video Intelligence API""",Trust,are supported,1,64,76
0,52468352,"""Trying to use Google`s CloudVisionwith Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request.Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:[Q]Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?""",Anticipation,Couldn't ready,1,294,337
0,52154222,"""I am trying to get the details of the image using google vision api.everything works fine,but when i use this line of codethen it doesn't find the image,but if i use like thisthen this works fine and it classifies the image.If you see i have also used the same method to plot the image in,and for plotting image this datapath works fine, then why it is not working for this""",Anticipation,to plot,1,269,275
0,52154222,"""I am trying to get the details of the image using google vision api.everything works fine,but when i use this line of codethen it doesn't find the image,but if i use like thisthen this works fine and it classifies the image.If you see i have also used the same method to plot the image in,and for plotting image this datapath works fine, then why it is not working for this""",Anticipation,plotting,1,298,305
0,52154222,"""I am trying to get the details of the image using google vision api.everything works fine,but when i use this line of codethen it doesn't find the image,but if i use like thisthen this works fine and it classifies the image.If you see i have also used the same method to plot the image in,and for plotting image this datapath works fine, then why it is not working for this""",Disgust,is not working,1,351,364
0,35519689,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""",Joy,good,2,145,148
0,35519689,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""",Joy,quality,1,134,140
0,35519689,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""",Joy,is,1,142,143
0,35519689,"""Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].[1][2][3]""",Anticipation,are ordered,1,272,314
0,55086411,"""My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier?My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.""",Anticipation,prediction,1,255,264
0,40671175,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",Joy,successfully,1,88,99
0,40671175,"""I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.""",Trust,has to be supported,1,167,185
0,43069723,"""We are using Google Vision API to extract text from image. Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.Can someone explain why we are getting that error and how we can rectify it?""",Surprise,Suddenly,1,60,67
0,53827914,"""I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.I want to detect faces after every 1 second in the video that means run CF.face.detect once/second on video frame.Please tell how to do itThanks in advance""",Anticipation,in,1,258,259
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Joy,is appreciated,1,785,806
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Joy,greatly is appreciated,1,788,806
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Trust,credentials,2,145,155
1,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Trust,credentials,2,184,194
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Anticipation,immediately,1,577,587
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Fear,missing,1,366,372
0,51527259,"""I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:Here is my MainActivity Kotlin code:Here are my grade dependencies:This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.""",Sadness,crashes,1,569,575
0,36120746,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""",Sadness,:(,1,217,218
0,36120746,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""",Sadness,fail,1,507,510
0,36120746,"""I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page):but I get the following error:The image that I am using for tests is this one:(found it on the internet in a quick search)It respect all the requisits set by Microsoft, size and format... If I use it in the site it worksThefrom the convertion of my array of bytes to a string in base64 is also ok, I test it in this website:The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?UPDATEThe variable:""",Trust,respect,1,265,271
0,54082512,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""",Trust,CREDENTIALS,1,147,157
0,54082512,"""I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.Google explains that you have to set an environment variable as such:I generated my credentials (in a .json file) and I have already set my system environment variable manually to:Previously, I had a similar approach working.Does anybody have ideas of things I could try to make this work?""",Trust,credentials,1,243,253
0,51747822,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""",Anticipation,predictions,1,106,116
0,51747822,"""I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCVfunction takes in.Here's an example of what comes back from the service as bounding box:Currently, I'm doing these calculations below. Theandvalues look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to.And here is the resulting image from the above code:The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.Does anyone know how to calculate these correctly from normalized coordinates?""",Anticipation,prediction,1,194,203
0,44615959,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""",Anticipation,repeatedly,1,348,357
0,44615959,"""I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3When I go to doI am greeted with:Same thing for the CLI, when I doI get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.""",Sadness,invalid,1,236,242
0,52541577,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",Anticipation,in,1,237,238
0,52541577,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",Anticipation,would be appreciated,1,200,227
0,52541577,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",Joy,would be appreciated,1,200,227
0,52541577,"""I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts fileAnd here's the error I am getting when I build the application.Any help would be greatly appreciated, thanks in advance.""",Joy,greatly would be appreciated,1,209,227
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Anticipation,resulting,1,522,530
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Anticipation,result,1,968,973
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Joy,would be,1,185,192
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Joy,good,1,194,197
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Joy,like,1,758,761
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Trust,decided,2,174,180
1,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Trust,decided,2,356,362
0,48412094,"""So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google.When I looked at the response from google it says:Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceedsbefore sending to google. However, I tried 2 different solutions, both resulting in a byte size ofwhich is not greater than the max size on Google's end.I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result):Am I doing something wrong?""",Sadness,do apologize,1,742,753
0,51977903,"""I'm working on a face tracking thought video(). It will return the job id. I have setup everythingparts.Here my problemis i can't able to get a message from SQS using. Evenalso not working. It gives this below error.The code as follow. AWS PHP SDK ver 3.64.11.Thanks in advance!""",Anticipation,in,1,268,269
0,42713068,"""I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.""",Anticipation,in,1,209,210
0,55301066,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""",Trust,managed,1,188,194
0,55301066,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""",Trust,share,1,407,411
0,55301066,"""I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.But when I installed it into my other test model, a Huawei Mate 9, it never works.is always false, never true. Why is this, and how can I get around it?I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.""",Joy,like,1,447,450
0,43748559,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""",Trust,credentials,1,140,150
0,43748559,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""",Trust,CREDENTIALS,1,186,196
0,43748559,"""I am using the Google Cloud Vision Java API client documented here:.The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?""",Trust,to authenticate,1,292,306
0,45696336,"""I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine.This is the request and for some reason the content-type is not there. Could some one help to explain? ThanksThis is the api reference""",Trust,did manage,1,221,230
0,55251217,"""I have a directory with images, and I'd like to query the Google Vision API for each and store the aggregate output in one tibble.I tried what seemed like an easy solution: ifworks, then all I need is:Only to get:I found this answer, which involves writing a function from scratch, but that seems untidy and overkill, no?I also foundthat aimed to address this, but it did not get merged.""",Disgust,untidy,1,298,303
0,55251217,"""I have a directory with images, and I'd like to query the Google Vision API for each and store the aggregate output in one tibble.I tried what seemed like an easy solution: ifworks, then all I need is:Only to get:I found this answer, which involves writing a function from scratch, but that seems untidy and overkill, no?I also foundthat aimed to address this, but it did not get merged.""",Joy,'d like,1,38,44
0,47281901,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""",Anticipation,results,2,282,288
1,47281901,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""",Anticipation,results,2,414,420
0,47281901,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""",Anticipation,order,1,335,339
0,47281901,"""The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?Output-""",Surprise,order,1,54,58
0,51926971,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",Disgust,did not work,1,592,603
0,51926971,"""please read the full question before marking it as duplicate or down-vote it.i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.This is the image type i want to slice in pieces""",Sadness,crashes,1,384,390
0,53543793,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""",Anticipation,am attempting,1,3,15
0,53543793,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""",Anticipation,attempt,1,187,193
0,53543793,"""I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.Error code:Traceback (most recent call last):  File ""main.py"", line 37, in     response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call    return self._make_api_call(operation_name, kwargs)  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call    raise error_class(parsed_response, operation_name)botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.""",Sadness,Unable,1,856,861
0,51399511,"""I can't able to set multiple region for AWS Service Manager. (Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.PS: Info plist configuration also not taking here.Thanks in Advance.""",Anticipation,in,1,381,382
0,52684763,"""I found this project on GitHub that is an application with the Google Cloud Vision API:.I've edited a few things to avoid compatibility errors, and now it perfectly works.There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.This is the entire code of MainActibity.java:.Go to the line 169, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".Is there a way to change the feature type?""",Trust,LABEL,2,249,253
1,52684763,"""I found this project on GitHub that is an application with the Google Cloud Vision API:.I've edited a few things to avoid compatibility errors, and now it perfectly works.There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.This is the entire code of MainActibity.java:.Go to the line 169, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".Is there a way to change the feature type?""",Trust,LABEL,2,402,406
0,52684763,"""I found this project on GitHub that is an application with the Google Cloud Vision API:.I've edited a few things to avoid compatibility errors, and now it perfectly works.There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.This is the entire code of MainActibity.java:.Go to the line 169, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".Is there a way to change the feature type?""",Disgust,doesn't work,1,483,494
0,52684763,"""I found this project on GitHub that is an application with the Google Cloud Vision API:.I've edited a few things to avoid compatibility errors, and now it perfectly works.There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.This is the entire code of MainActibity.java:.Go to the line 169, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".Is there a way to change the feature type?""",Joy,would like,1,203,212
0,52848759,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""",Joy,would like,1,3,12
0,52848759,"""I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)This script allows me to receive the the annotations by an individual, pre-downloaded script (here)My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?""",Trust,labels,1,303,308
0,54236946,"""I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. And my question is:Can I see all my data which is saved in azure's database?""",Joy,like,1,253,256
0,52252580,"""I'm fairly new to AWS and for the past week, been following all the helpful documentation on the site.I am currently stuck on bring unable to pull the External Image Id data from a Reko collection after a 'search face by image', I just need to be able to put that data into a variable or to print it, does anybody know how I could do that?Basically, this is my code:ifname== ""main"":at the end of it, I receive:What I need is the External Image Id instead of the FaceId.Thanks!""",Sadness,unable,1,133,138
0,52252580,"""I'm fairly new to AWS and for the past week, been following all the helpful documentation on the site.I am currently stuck on bring unable to pull the External Image Id data from a Reko collection after a 'search face by image', I just need to be able to put that data into a variable or to print it, does anybody know how I could do that?Basically, this is my code:ifname== ""main"":at the end of it, I receive:What I need is the External Image Id instead of the FaceId.Thanks!""",Trust,helpful,1,69,75
0,51712998,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",Fear,missing,2,855,861
1,51712998,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",Fear,missing,2,919,925
0,51712998,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",Anticipation,I am wondering,1,878,891
0,51712998,"""I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the:The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?""",Trust,LABEL,1,154,158
0,54794136,"""I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.""",Anger,ie,1,318,319
0,46289477,"""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?""",Disgust,snake,1,99,103
0,46289477,"""When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?""",Fear,snake,1,99,103
0,40020225,"""This question has been asked before () but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.So I would need to extract some text from a photo to perform some analysis on it, for an Android App. I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...Do you know if there is any API that would allow this kind of feature for standalone apps ?""",Trust,the best,1,330,337
0,37935601,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""",Anticipation,results,1,76,82
0,37935601,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""",Anticipation,result,1,178,183
0,37935601,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""",Anticipation,I was wondering,1,348,362
0,37935601,"""I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.I was running some tests found the result a LOGO_DETECTION on a image showing only Google s plain logo (image below) returned a score of only0.28542563.Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn t find any reference to any of this on the documentation.Does anyone here know about this?""",Trust,reliable,1,97,104
0,51354969,"""I need a count of all thecarsincluded in a image with the Google Cloud Vision API in Python. I take only the labels of the image right now.""",Trust,labels,1,110,115
0,53671813,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",Anticipation,prediction,1,180,189
0,53671813,"""I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.""",Trust,label,1,115,119
0,46848590,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""",Anticipation,in,1,444,445
0,46848590,"""I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.I went through a few of the available API's like Google Vision API but could find a solution to it.Any help will be appreciated. Thanks in advance.""",Joy,will be appreciated,1,416,434
0,50237770,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",Anticipation,result,1,438,443
0,50237770,"""When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error:The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result.Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error?Update:Image sizes: source: 1189   750target: 360   480""",Joy,successfully,1,404,415
0,44517510,"""I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.I have attached below the:a) HTTP Requestb) Response ErrorOther than this, in HTTP Header Manager I have set:Content-Type: application/jsonWhat is wrong with the attached request?""",Sadness,FORBIDDEN,1,135,143
0,51493545,"""I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this..I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)Things I tried:I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..Not sure how to proceed and get the groups on single line..""",Anticipation,result,1,79,84
0,45981451,"""I got a problem when using the Google Vision API.I'm looping the process, to analyze several pictures, but when I print the results, all is coming in a block after 5 minutes of process, but I wanted to know if it's possible to start the program and make it print the results after each picture analyzis ?Here's my code for the algorithm :I'm looping this with a foreach(Image file in Directory).Any ideas to help me ?Thanks a lot !""",Anticipation,results,2,125,131
1,45981451,"""I got a problem when using the Google Vision API.I'm looping the process, to analyze several pictures, but when I print the results, all is coming in a block after 5 minutes of process, but I wanted to know if it's possible to start the program and make it print the results after each picture analyzis ?Here's my code for the algorithm :I'm looping this with a foreach(Image file in Directory).Any ideas to help me ?Thanks a lot !""",Anticipation,results,2,268,274
0,50657314,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""",Trust,enabled,1,238,244
0,50657314,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""",Trust,to authenticate,1,96,110
0,50657314,"""I am trying to use Google Cloud Vision via a rest http request using c#. As described, I tried to authenticate with the api key as a parameter:However, I always get a 403 PERMISSION DENIED Code:Of course I checked, The API is activated, enabled and there are no API restrictions:Since there seemed to existwith that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account:Still, same error message. Same goes with curl:What am I missing out?""",Fear,missing,1,624,630
0,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",Joy,successful,1,293,302
0,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",Joy,more successful,1,288,302
0,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",Anger,complains,1,422,430
0,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",Anticipation,in,1,490,491
0,44966573,"""I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found, and the line the error complains about is about half way through the file (line 46).Thanks in advance,Connor""",Trust,managed,1,311,317
0,50580902,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""",Joy,like,1,415,418
0,50580902,"""I'm using Google Vision API for text & logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):I'm making up to 8 concurrent requests, whole process takes about 65 seconds.According toI should be able to:make 600 requests per minutesend up to 16 images per requestThere's also a limit for image size & JSON request object size, but with images like(under 50KB), that should not be a problem (right?).I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)Looking at Google Cloud Vision API dashboard confuses me even more, here areresults for the same minute, just after a page refresh:Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).""",Joy,would like,1,790,799
0,56068100,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""",Disgust,doesn't work,1,383,394
0,56068100,"""I made an Android library that I use locally via .aarIn the library prohect, it looks like this:AAR Demo project -: app: library (source code)The library consists of a QR scanner usingthen, I export the .aar file and then I import into the other app usingOther app project -: otherApp: library (aar)There are a couple of things that the library behaves diferently:1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module intoand it works fine, but using the .aar doesnt work, at all.2- In order to use the library, I also have to include thelibrary into theif I dont import it, I get a, I tried defining Google Vision as a transitive dependency like this:but theproject doesnt seem to read it, But i don't have to define it in themodule inside the library source code project.I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?""",Trust,ProGuard,1,994,1001
0,49181313,"""***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********could you please help with an issue I am having?I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?""",Sadness,failed,1,225,230
0,44740363,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",Anticipation,I am wondering,1,562,575
0,44740363,"""Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.Another strange thing is that, when I upload the image toin the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?""",Surprise,Suddenly,1,122,129
0,44845273,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""",Joy,like,2,109,112
1,44845273,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""",Joy,like,2,325,328
0,44845273,"""I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders.The function in my code is somewhat like:""",Trust,proof,1,120,124
0,56175881,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""",Trust,labels,2,75,80
1,56175881,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""",Trust,labels,2,112,117
0,56175881,"""Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?""",Anticipation,results,1,51,57
0,54587451,"""It would be very much help full if someone might walk me through this, i Have tried going through event structure but was not that helpful !I am still not abel to understand Many of u attended the question its grate full, but yet im not able to understand you, I'am completly new to aws.I went through the event structure, it says you will find the configuration id over here  ""configurationId"":""ID found in the bucket notification configuration"" but i could notand I'am still clue less about-----------------------event------------------error----------------------------xxxxxxxxx------------------------""",Trust,helpful,1,132,138
0,56032884,"""I'm trying to hit this URL:As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORSNow, I have my API inAWS API Getway. I have clicked onEnable CORSit works for my other API Resources but not this one. after clicking that I then clickDeploy API.Angular code:""",Disgust,nightmare,1,189,197
0,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",Trust,'s authenticating,1,163,179
0,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",Trust,to authenticate,2,336,350
1,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",Trust,to authenticate,2,630,644
0,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",Trust,CREDENTIALS,1,559,569
0,48527260,"""I'm currently able to run a local python script that calls the Google vision API using the(specifically, I'm using thepackage).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e), however, I don't recall doing this and if I run, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.The below posts provide great details about different ways to authenticate using the client libraries locally, buthow can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?...including thepart of the above pagesection of Creating and Enabling Service Accounts for Instancessection of ""Setting Up Authentication for Server to Server Production Capabilities"" pageSection of ""Getting Started With Authentication"" page:Python client librariespage:""",Trust,is being authenticated,1,732,753
0,56099062,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",Sadness,sad,2,973,975
1,56099062,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",Sadness,sad,2,1000,1002
0,56099062,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",Trust,can guide,1,42,58
0,56099062,"""I want to create anAndroid Music player, can someone guide me ifMediaStoreis the right tool to create one? as i really do not know what it is. I also want to createplaylistsin my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition usingGOOGLE VISION APIand will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.""",Trust,guide,1,474,478
0,35651277,"""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library.""",Joy,successfully,2,3,14
1,35651277,"""I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image?Note: Code is based on git hub sample of google vision library.""",Joy,successfully,2,67,78
0,50490782,"""I'm building an aws lambda usingandand I'm stuck with a little problem.I want to test my lambda handler. To do so, I need to mock a validcontaining valid attributes forand satisfy.I cannot seem to find a way to build such mock, sincealways returns me.Here's my main, with a simple handler on aevent:And here's my test for:I also tried other mocks like passing it a struct with these parameters etc, but I always get. For instance, I tried also:I checked out the source ofand I've been scratching my head for a while.Of course, it returnseven if I just pass ato it.Any idea on how should I build a validto letreturn?""",Trust,valid,1,149,153
0,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Anticipation,likely,3,272,277
1,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Anticipation,likely,3,288,293
2,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Anticipation,likely,3,680,685
0,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Anticipation,most likely,2,267,277
1,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Anticipation,most likely,2,675,685
0,53961202,"""I am using the Microsoft Face API to detect faces and emotions in Android. I have awhose key is the probability of an emotion attribute with the value being the attributes name like so:(person is an object of the type)What I want to do is rank those 7 emotions from most likely to least likely but the problem is thatthe size ofvaries. For example, when I choose to get the emotion attributes of a face that issmiling,returns2. When I choose to get the emotion attributes of a face that is mainlyneutral,returns3.Does anyone know why this behavior is occurring even though I have added 7 key-value pairs tousing? I have found that the key-value pairs whichdoexist inare the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to.""",Sadness,:(,1,185,186
0,50907959,"""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly: null""",Anticipation,results,1,106,112
0,50907959,"""I'm usingof Google vision API in my node application.I'm trying to get label polygon but it returns null.results:boundingPoly: null""",Trust,label,1,72,76
0,55037756,"""I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.""",Fear,command,1,233,239
0,47946770,"""I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.I import the Google Cloud vision client library as followand then I callHowever I get an errorTypeError: vision.detectText is not a functionInitially I usedfrom this examplebut I got the exact same error. I then read that textDetection has been replaced by detectText but no more successThanks in advance""",Anticipation,in,1,401,402
0,54916175,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""",Trust,label,1,97,101
0,54916175,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""",Trust,labels,1,146,151
0,54916175,"""Perand, Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values.Does anyone have any thoughts?My code is below:""",Joy,successfully,1,118,129
0,48868820,"""I'm using Rekognition for face authentication.When I register a user I havetheir user idmultiple photos of that userHow can I associate/label all those photos with that id when indexing/adding their faces to a collection?When I search by face in a collection I want to be able to get back their id.""",Trust,can associate,1,121,135
0,48868820,"""I'm using Rekognition for face authentication.When I register a user I havetheir user idmultiple photos of that userHow can I associate/label all those photos with that id when indexing/adding their faces to a collection?When I search by face in a collection I want to be able to get back their id.""",Trust,label,1,137,141
0,53954761,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""",Anticipation,to invoke,1,38,46
0,53954761,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""",Anticipation,invoking,1,210,217
0,53954761,"""I have an issue with my logic trying to invoke theRecognition Compare Faces api using.  There isn't any documentation foryet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.Any advice?""",Anticipation,results,1,279,285
0,53736963,"""I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.When I trigger the API from my local machine - I get a good responseWhen I trigger the API from AWS console      - I get failed responseProblemAfter much digging, I found that the 'boto3' (AWS python library) is of version:1.9.62   - on my local machine1.8.9    - on AWS consoleQuestionCan I upgrade the 'boto3' library version on the AWS lambda console ?? If so, how ?""",Sadness,failed,1,330,335
0,46630157,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",Fear,missing,1,300,306
0,46630157,"""I want to try one of google vision api on android. I added this line in gradle filealso in manifest file I added thisAnd here's the code snippetSo herealways returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?""",Sadness,fails,1,420,424
0,51189901,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""",Anticipation,warning,1,345,351
0,51189901,"""If I return anbeforecondition it works fine but if try to return something aftercondition (even hardcoded array) it does not return anything. Also it does not go in any, not even. Even print or echo does not work.Myfunction is returningbut it is not entering into related switch case.I checked error log and there is nothing there, no error or warning printed on the page.""",Disgust,does not work,1,200,212
0,49890225,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""",Sadness,fails,2,72,76
1,49890225,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""",Sadness,fails,2,254,258
0,49890225,"""i have followed the steps of the documentation but i received:The code fails en. Without this i received the jobId correctlyI set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.InI created another Role but it fails too.I am not the root user.I know I need to give more information but if someone can guide me, i will start again the configuration.I am beginner in aws and i dont understand several things at all.(english is not my first language)""",Trust,can guide,1,341,349
0,56052040,"""i am trying google vision api for crop hints and the output results are like belowi am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y.finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera?""",Anticipation,results,1,61,67
0,38890861,"""I do have a Barcode Decoding Applcation :I have Used Zbar library and Google Vision API for ScanningNow what i want is while scanning the Barcode if user taps on a button at appbar for turning on Torch (Flash) then it should be turn on and off vice-versa.But the Problem is the camera is already on with its all parameters so when user taps the button to turn on torch we need to interrupt the ongoing Camera parameters and I don't want to do that,I am searching the other way to get torch on without changing the existing Camera Parameters..Below is The Camera Activity of ZBar And Google Vision this both use some other Camera Classes for Camera Preview.And GoogleScanner""",Surprise,need to interrupt,1,373,389
0,42229158,"""I am trying to run the quick start demo byon MacOS Sierra.Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with,Environment variable is correctly set. Still script raises,There are similar questions asked when using API keys, when using Service account file was not mentioned.""",Trust,to authenticate,1,118,132
0,51811837,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""",Disgust,can't authenticate,1,66,83
0,51811837,"""I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.Previously it was as simple as adding my key to the method before calling it. It went something like this :I don't think there's a method like that anymore. I was trying to run the steps here:,including creating a service account and using that export command to export the json to my project.And yet, I still keep getting the same error :Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.""",Fear,command,1,353,359
0,51959009,"""I'm getting below error when I execute Rekognition using Image or Video (s3 source).My as code Below.When I'm uploading into S3, Getting an object from S3 and creating a collection in Rekognition All Its works fine but I can't execute searchFacesByImage (Source: S3) and startFaceSearch in Laravel PHP.Also tired after bucket policy setup in S3 like below.""",Joy,like,1,346,349
0,45812258,"""I'd like to know if is there a possibility to change the format of result returned bi Watson Visual Recognition API.For example:Instead of having this:Get this:So the sum of classes results would be 1 (100%)""",Anticipation,result,1,68,73
0,45812258,"""I'd like to know if is there a possibility to change the format of result returned bi Watson Visual Recognition API.For example:Instead of having this:Get this:So the sum of classes results would be 1 (100%)""",Anticipation,results,1,183,189
0,45812258,"""I'd like to know if is there a possibility to change the format of result returned bi Watson Visual Recognition API.For example:Instead of having this:Get this:So the sum of classes results would be 1 (100%)""",Joy,'d like,1,2,8
0,54017318,"""Currently I am usingto match photo during login (capturing using webcam while login and compared with already uploaded photo).Issue I am facing is people can use photos from mobile or some photos to share account.Is there any full proof way to verify face?Thanks,""",Trust,proof,1,232,236
0,54017318,"""Currently I am usingto match photo during login (capturing using webcam while login and compared with already uploaded photo).Issue I am facing is people can use photos from mobile or some photos to share account.Is there any full proof way to verify face?Thanks,""",Trust,to share,1,197,204
0,52048829,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",Trust,associated,1,329,338
0,52048829,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",Trust,enabled,1,622,628
0,52048829,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",Trust,CREDENTIALS,1,159,169
0,52048829,"""I'm trying to work through the Google Cloud Visionbut I'm getting an authentication error.This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:whereis the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from thisApparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.Do I really have to change the environment variable every time I change the project?""",Trust,credentials,1,559,569
0,56362468,"""I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:This is the part of the image with the issue:As you can see, there are overlapping rectangles on the 'M' and the 'a'.The result is something like this:   '''MMaay 10, 2019'''Why this is happening?Is there a way to fix it?I tried to change the image format from bmp to png. The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'.""",Anticipation,result,1,403,408
0,54513958,"""I am trying to use the Microsoft Custom Vision. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct.However, when I tried to make the same request in Java I received an HTTP 400 error.Following are the snippets.C#:Java:""",Joy,successfully,1,116,127
0,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,gem,4,81,83
1,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,gem,4,122,124
2,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,gem,4,224,226
3,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,gem,4,323,325
0,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,Gem,1,274,276
0,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Joy,like,1,71,74
0,52285908,"""I have a Gemfile with something like this:Yet, when I run a rake task like, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?""",Anticipation,can prevent,1,304,316
0,51133277,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""",Anticipation,result,1,731,736
0,51133277,"""I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.I am using the Python script supplied onto test how the service works and how I could possibly integrate it into other apps.I know that I have entered the correct data for the config and credential files, found here:as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.When running the script, on terminal everything works fine until after a few seconds the following error message is displayed:I have also tried several other availability zones such as:yet they all result in the same error.A similar issue has already been discussed in. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue.""",Trust,credential,1,286,295
0,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",Trust,to enable,1,810,818
0,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",Trust,Credential,1,1171,1180
0,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",Anger,wasted,1,1388,1393
0,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",Disgust,credentials,1,1503,1513
0,42176137,"""I am trying to get my head round GoogleVision API Java library.I have created a service account, downloaded the json and set this environment variable.I have set Application Default Credentials using:And I am following the example here:I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such asHow can I be sure I am using the service account to make the call?What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?The example in the documentation shows howAny help appreciated.Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!""",Joy,appreciated,1,1031,1041
0,45245748,"""If I was usingIndexFaces, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown byIndexFacesand/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below forIndexFacesin case it helps.""",Anticipation,I was wondering,1,428,442
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Anticipation,results,1,125,131
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Anticipation,resulted,1,1413,1420
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Anticipation,'s expecting,1,1690,1701
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Anticipation,would be appreciated,1,2365,2392
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Trust,valid,1,2290,2294
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Trust,credentials,3,1789,1799
1,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Trust,credentials,3,1848,1858
2,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Trust,credentials,3,2009,2019
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Joy,awesome,1,2326,2332
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Joy,would be appreciated,1,2365,2392
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Joy,greatly would be appreciated,1,2374,2392
0,40032758,"""The Goal:User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.According to,requires native libraries and must be compiled against the OS that lambda is running. Usingthrew an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).The Common Code:This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.Using: This code always returns the error. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.Using:This code always returns. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.According to, the image should be base64 encoded.From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.Ideally, usingwould be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.""",Sadness,Unfortunately,1,1261,1273
0,50358189,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""",Trust,definitive,1,218,227
0,50358189,"""I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, likeanddo.The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection?""",Trust,supports,1,264,271
0,47182750,"""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst.""",Joy,'d like,1,307,313
0,47182750,"""I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:my function is:So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of theconst.""",Trust,constant,1,362,369
0,47157038,"""I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error.""",Surprise,randomly,1,104,111
0,47157038,"""I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error.""",Trust,labels,1,22,27
0,45341208,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""",Disgust,did not work,1,208,219
0,45341208,"""I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file inHow could I increase the limit?I have tried to addorand it did not work.""",Trust,labels,1,65,70
0,51216224,"""On Ubuntu 14.04, I've successfully installed the Python SDK following this:Also, I am able to deploy the app to Google App Engine, using thescript.I do have a configuration file where I declare thedirectory and all third party libraries are installed there.Everything works, except when I try to import Google Cloud Vision:I have installed Google Cloud Vision both with:andNone of them work.How do I install locally? As a third party or globally? How will it work on Google when I deploy?""",Joy,successfully,1,23,34
0,56010701,"""I need to read a mark like cylinder icon, tick within the image.Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).What are the ways I can identify the cell and its value?Will including a symbol like cylinder,tick,sphere ... help?If so how can I do it?Please helpI am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.I am usingI need to identify the cell data from each cell of the table in the image.""",Trust,decided,1,311,317
0,37741299,"""Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help""",Joy,successfully,1,147,158
0,37741299,"""Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..How can i achieve this?The response from Vision API:Kindly Help""",Joy,can achieve,1,277,289
0,50415121,"""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes.""",Anticipation,results,1,93,99
0,50415121,"""I'm using the Google Text detection API for performing OCR on images.I've found that my OCR results are much better when I perform some pre-processing on the images using opencv.My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input.I want to avoid unnecessary disk writes.""",Trust,official,1,289,296
0,44258407,"""Following error message is shown when I implement these codes...Console Messsage: AmazonWebServiceClient: {cognito-identity, us-west-2} was not found in region metadata, trying to construct an endpoint using the standard pattern for this region: 'cognito-identity.us-west-2.amazonaws.com'.CognitoCachingCredentialsProvider: Loading credentials from SharedPreferencesCognitoCachingCredentialsProvider: No valid credentials found in SharedPreferencesError Message:""",Trust,credentials,2,333,343
1,44258407,"""Following error message is shown when I implement these codes...Console Messsage: AmazonWebServiceClient: {cognito-identity, us-west-2} was not found in region metadata, trying to construct an endpoint using the standard pattern for this region: 'cognito-identity.us-west-2.amazonaws.com'.CognitoCachingCredentialsProvider: Loading credentials from SharedPreferencesCognitoCachingCredentialsProvider: No valid credentials found in SharedPreferencesError Message:""",Trust,credentials,2,411,421
0,54367776,"""Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?With the (LocalizedObjectAnnotation) can Google detect only objects and creaturesGoogle OCR (fullTextAnnotation) detects only textScenario:I got an scanned formular. From this scan i would get all form field-positions (input-fields).It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.Has anyone an idee how i get the coordinates for the input-fields?""",Disgust,don't work,1,342,351
0,44362759,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""",Anticipation,results,1,50,56
0,44362759,"""The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug?""",Sadness,bottom,1,373,378
0,41560252,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""",Anticipation,results,1,207,213
0,41560252,"""My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?""",Joy,successfully,1,189,200
0,49665196,"""I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw.""",Trust,to authenticate,1,136,150
0,49842698,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Anticipation,results,1,191,197
0,49842698,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Anticipation,promise,1,261,267
0,49842698,"""I am using following nodejs code to retrieve operation instance of video annotation requestBut once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?""",Trust,polling,1,586,592
0,41488436,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""",Joy,are appreciated,1,552,571
0,41488436,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""",Joy,much are appreciated,1,556,571
0,41488436,"""I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected.An example of this problem can be found, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.The image is according to the best practices guidelines as set in the documentation.Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.""",Trust,the best,1,373,380
0,54410031,"""I am followingto use Google Vision API, but even configuring the authentication credentials I get the following error:My code in Visual Studio 2017:What can I do to fix this? Do I have to create a trial account to use the Google Cloud API?""",Trust,credentials,1,81,91
0,45427109,"""I am using Python Client for Google Cloud Vision API, basically same code as in documentationproblem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).I think today is not my dayI appreciate any clarification / help""",Anticipation,result,1,326,331
0,55072291,"""I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.I tried some generic OCR solutions such as:Amazon RekognitionGoogle VisionMicrosoft Computer VisionTeserract3.0 / 4.0 (experimental)None of these provide accurate (sometimes not at all MRZ recognition)I also tried some other tools specialized in MRZ OCR:BlinkIDfrom MicroBlink (which is very good but doesn't have a cloud solution)Accurascan(provides cloud solution but less accurate than BlinkID)Abbyy(too slow, 10~ seconds per request)Can you recommend me a good cloud solution for MRZ OCR of documents?""",Trust,accurate,2,278,285
1,55072291,"""I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.I tried some generic OCR solutions such as:Amazon RekognitionGoogle VisionMicrosoft Computer VisionTeserract3.0 / 4.0 (experimental)None of these provide accurate (sometimes not at all MRZ recognition)I also tried some other tools specialized in MRZ OCR:BlinkIDfrom MicroBlink (which is very good but doesn't have a cloud solution)Accurascan(provides cloud solution but less accurate than BlinkID)Abbyy(too slow, 10~ seconds per request)Can you recommend me a good cloud solution for MRZ OCR of documents?""",Trust,accurate,2,499,506
0,55072291,"""I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.I tried some generic OCR solutions such as:Amazon RekognitionGoogle VisionMicrosoft Computer VisionTeserract3.0 / 4.0 (experimental)None of these provide accurate (sometimes not at all MRZ recognition)I also tried some other tools specialized in MRZ OCR:BlinkIDfrom MicroBlink (which is very good but doesn't have a cloud solution)Accurascan(provides cloud solution but less accurate than BlinkID)Abbyy(too slow, 10~ seconds per request)Can you recommend me a good cloud solution for MRZ OCR of documents?""",Joy,is,1,408,409
0,55072291,"""I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.I tried some generic OCR solutions such as:Amazon RekognitionGoogle VisionMicrosoft Computer VisionTeserract3.0 / 4.0 (experimental)None of these provide accurate (sometimes not at all MRZ recognition)I also tried some other tools specialized in MRZ OCR:BlinkIDfrom MicroBlink (which is very good but doesn't have a cloud solution)Accurascan(provides cloud solution but less accurate than BlinkID)Abbyy(too slow, 10~ seconds per request)Can you recommend me a good cloud solution for MRZ OCR of documents?""",Joy,good,1,416,419
0,55127193,"""I'm trying to use volley to call Azure Computer Vision REST API using POST request to upload image to be analysed.Here's the API documentation:The input is passed within the POST body, and I wanted to send the raw image binary using.I convert the image to raw binary usingandI have no success so far in sending the raw binary image data, it's giving error 400.Here's the code:There are only few resources in the internet about Volley POST request using, so I am a little bit lost here...Any help would be appreciated!""",Anticipation,would be appreciated,1,497,516
0,55127193,"""I'm trying to use volley to call Azure Computer Vision REST API using POST request to upload image to be analysed.Here's the API documentation:The input is passed within the POST body, and I wanted to send the raw image binary using.I convert the image to raw binary usingandI have no success so far in sending the raw binary image data, it's giving error 400.Here's the code:There are only few resources in the internet about Volley POST request using, so I am a little bit lost here...Any help would be appreciated!""",Joy,would be appreciated,1,497,516
0,51294008,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""",Sadness,fails,2,593,597
1,51294008,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""",Sadness,fails,2,675,679
0,51294008,"""I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.Everything works fine using the Google Vision MLKit.I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.The initial symptom is as follows:- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework).This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):I have breakpoints in lines 2 and 3 of this code.I scan a QR code and the tagNumber is reported correctly (and printed in line 1)Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.I restarted Xcode and rebooted my Mac.   Still the same strange behavior.I am using Xcode Version 9.4 (9F1027a), and Swift 4.1Any ideas of what may be happening here?""",Anticipation,suspect,1,779,785
0,45518029,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",Anticipation,I wonder,1,560,567
0,45518029,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",Joy,'d like,1,252,258
0,45518029,"""I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment.The images are uploaded usingand are stored in Aws S3 via. This works OK. However, I'd like to conduct image recognition to the uploaded images with.I've installedand I'm able to instantiate Rekognition without a problem until I call themethod at which point I have been unable to use my attached images as arguments of this method.So fat I've tried:I've tried using:All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in.I've tried also fetching directly the s3 object to try this last bit:Still no success...Any tips?""",Sadness,unable,1,439,444
0,38227082,"""Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.Personal experience with Italian, French, English, Chinese, Spanish works. What about the ones like Hindi, Urdu etc?Thanks and appreciate your help!Suman""",Joy,like,1,238,241
0,44419153,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""",Sadness,fails,1,91,95
0,44419153,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""",Sadness,failed,1,248,253
0,44419153,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""",Anticipation,attempt,1,255,261
0,44419153,"""The Problem:When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?When I try to install the Custom Vision packageafter the first failed attempt(without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:Can anyone tell me what this means, or offer a potential fix?Qualifier:My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.UPDATE:I have succumb to stress and installed VS 2017- still getting the same error:""",Trust,team,1,449,452
0,51972479,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",Anticipation,am attempting,1,3,15
0,51972479,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",Joy,'d like,1,428,434
0,51972479,"""I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via themethod.The sample code is as follows:""",Trust,supported,1,32,40
0,35660357,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",Joy,successfully,1,112,123
0,35660357,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",Joy,would like,1,169,178
0,35660357,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",Anticipation,am attempting,1,14,26
0,35660357,"""From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }but I receive this error from my call,{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.Thanks,VIPER""",Trust,LABEL,1,383,387
0,54733517,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",Joy,would like,1,205,214
0,54733517,"""I am reading the documents and API from Azure page but I am still not sure if my though it correct here.ScenarioWe have around 1M ID photos in our local storage. Each ID contain only one single person.We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.To do the avove, I believe we need to write the software to do things belowUpload all the photos into AzureCreate Large FaceList?Train the modelThen we can do the face identify or face similarAre the steps above correct?If I use the method above that mean I need to use 'face storage' for persisted face Id right?1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M imagesWhen I do verify how many transactions will it be counted? Is it counted as 1?I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verifyI am welcome any comments pretty new in this field please guide me""",Trust,guide,1,1308,1312
0,49717845,"""I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.Can you help me please?Thanks.""",Disgust,doesn't work,1,202,213
0,45792942,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""",Joy,successfully,1,63,74
0,45792942,"""i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check, and my region was correct.Error Message:Look through theknow that my end point should be something like.Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advancepython""",Sadness,unable,1,110,115
0,44225909,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""",Trust,landmark,1,345,352
0,44225909,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""",Trust,Landmark,1,744,751
0,44225909,"""I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.From the documentation:Boundingbox:Landmark:""",Anticipation,am attempting,1,3,15
0,45481935,"""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?""",Trust,labels,2,180,185
1,45481935,"""I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.Google vision will analyze each image to a JSON file, containing the labels with the highest scores.for example:Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?""",Trust,labels,2,272,277
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Anticipation,results,1,104,110
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Anticipation,expected,1,367,374
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Anticipation,result,1,687,692
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Trust,nice,1,99,102
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Trust,helpful,1,403,409
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Trust,fairly,1,393,398
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Joy,nice,1,99,102
0,46548182,"""I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.For example, with the followingi get the below response:Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:Or close to that.Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.""",Joy,to achieve,1,671,680
0,54224197,"""how can I add languageHints to my google cloud vision python code.FromI know that it is supported but I do not know how to implement it into the code.I think I have to do it like this:""",Trust,is supported,1,86,97
0,52103546,"""I have an error about getting text in image usingserviceand this is my codeI tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?""",Trust,support,1,362,368
0,54881537,"""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ? Using,""",Trust,advisable,1,76,84
0,54881537,"""What is the best practice into add person face to the trained person.Is it advisable to add face again and again ? Using,""",Trust,the best,1,9,16
0,54803618,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",Anticipation,training,2,110,117
1,54803618,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",Anticipation,training,2,233,240
0,54803618,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",Joy,successfully,1,155,166
0,54803618,"""I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes.It successfully uploaded and created as a asset.After uploading files, I started training a model.It shows status is failed.when I went look into details it shows,But daisy class has more more than 10 samples.I have attached my screenshots for your reference.The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.I don't have any clue to solve this. What should I try now? any help would be appreciable.""",Sadness,is failed,1,266,274
0,56143548,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",Trust,certain,1,433,439
0,56143548,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",Trust,labels,1,1203,1208
0,56143548,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",Anticipation,training,1,236,243
0,56143548,"""I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g.) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:Would that constitute a feature request?Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?NB In the above, by metadata I donotmean tags/labels in the image model-side sense, but rather data-side file metadata.[Note that Azure Cognitive Services is using stackoverflow for Q&A, so this question is I believe appropriate for stackoverflow.]Thanks as ever!""",Joy,would like,1,127,136
0,42123633,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""",Joy,appreciated,2,243,253
0,42123633,"""I am having trouble using Microsoft Face API. Below is my sample request:I use the subscription id from my cognitive services account and I got below response:Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.""",Joy,Very much,1,233,241
0,41400421,"""I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.""",Joy,to achieve,1,203,212
0,52335053,"""On the Microsoft Custom Vision Portal I only have the Precision and Recall results, how can I generate the  True positive (TP), False positive (FP), True negative (TN)  and False negative (FN) from the same images that the service used to corss validate itself to then get the accurancy?tks""",Anticipation,results,1,76,82
0,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",Anticipation,results,2,134,140
1,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",Anticipation,results,2,348,354
0,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",Anticipation,would be appreciated,1,593,612
0,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",Joy,would be appreciated,1,593,612
0,36408010,"""I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".Is there a specific hint that I can give Cloud Vision which might produce better results?For example, this image below --produces this output --I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.Any help/pointers would be appreciated.""",Trust,to associate,1,230,241
0,44792573,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",Trust,labels,2,550,555
1,44792573,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",Trust,labels,2,1048,1053
0,44792573,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",Fear,make-alert-decision,1,822,840
0,44792573,"""In my first foray into any computing in the cloud, I was able to follow Mark West'son how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.I know I have to store the data externally and came acrossexplaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.Thanks!""",Joy,'d like,1,785,791
0,56219425,"""The google cloud vision api works fine on one pdfbut returns absolutely nothing on the other pdf. I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.I'm using the code given in their examples section by uploading these files in a google cloud bucket.""",Sadness,unable,1,103,108
0,37984641,"""I have the following IBM Watson Visual Recognition Python SDK for creating a simple classifier:The response with the new classifier ID and its status is as follows:The training status shows failed.print(json.dumps(visual_recognition.list_classifiers(), indent=4))What is the cause of this?""",Anticipation,training,1,169,176
0,37984641,"""I have the following IBM Watson Visual Recognition Python SDK for creating a simple classifier:The response with the new classifier ID and its status is as follows:The training status shows failed.print(json.dumps(visual_recognition.list_classifiers(), indent=4))What is the cause of this?""",Sadness,failed,1,191,196
0,49543773,"""Is there an API to see how many calls you've made this billing cycle to the Google Cloud Vision API?I would like to add this information to a UI so user's know when they're about to make queries that will be charged.""",Joy,would like,1,103,112
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Trust,have enabled,1,273,292
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Trust,enabled,1,669,675
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Trust,CREDENTIALS,1,368,378
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Fear,commands,1,248,255
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Fear,command,1,704,710
0,55831279,"""I'm trying to get Google Cloud Vision to work with node.js by following their documentation. Although I keep getting:To note thoughthe project number is very different from what I see in gcloud's outputwhen I gather information from the following commands:which outputs:I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yetas I have not yet really have had gcloud pointing to 'my-set-project'.I have also found a default.config atwhich has:So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks""",Joy,will be appreciated,1,819,837
0,41186458,"""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful.""",Joy,successful,1,404,413
0,41186458,"""I've been playing with the new rekognition API from Amazon and I am having trouble running theirJava application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in myas follows:From what I can tell, my application seems to be failing somewhere around here:...And the error that I'm getting is:I should also note that I ran the operation (see below) in AWS CLI and was successful.""",Sadness,to be failing,1,256,268
0,49955157,"""I am trying to print out results from an Amazon Rekognition call, but it returns the error:I put the index[0], I don't really see why it will happen to out of index range.Can any one help please?""",Anticipation,results,1,26,32
0,51320588,"""I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. I think there are some problems with the authentication, but I am not sure.Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).Then I get the bean from another class and make the API call.I omit some code which gets the image,packs it etc""",Trust,credentials,1,249,259
0,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,results,1,153,159
0,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,training,4,207,214
1,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,training,4,281,288
2,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,training,4,505,512
3,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,training,4,660,667
0,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Anticipation,expect,1,570,575
0,56293609,"""I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.The model training is done within the custom vision platform provided by Microsoft (). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.""",Joy,would like,1,101,110
0,52825897,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""",Anticipation,expected,1,103,110
0,52825897,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""",Joy,successfully,1,267,278
0,52825897,"""The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported.The expected output is a JobIdHowever I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified.The ErrorWhen I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?""",Sadness,fails,1,356,360
0,55164321,"""I am trying to run the code in. I have run the following terminal command correctly:However I am getting the following credential errors:""",Fear,command,1,67,73
0,55164321,"""I am trying to run the code in. I have run the following terminal command correctly:However I am getting the following credential errors:""",Trust,credential,1,120,129
0,48672879,"""I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. I tried it on Windows on version Python3.6.4 and it runs perfectly alright. However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or aI have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API?The codes are as follows:Problem: stops at:It seems that the problem happens at data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?I did try the following to no avail:""",Joy,happiness,1,491,499
0,53457755,"""Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image. But when i try it gives me result in matching them with english:Request code is as below:""",Anticipation,result,1,126,131
0,55997760,"""I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?""",Disgust,inconvenient,1,84,95
0,54813301,"""So,I take an image from a canvas using p5js and i want to send it to the Azure Custom Vision Service(the code bellow).Is p5 image even the same as the normal js image(like when you take a capture from a video) ?My problem is when i send a form as a json like :But it gaves me ""401 Acces Denied"".Is it even possible to send an image created using p5 via http request?""",Joy,like,2,168,171
1,54813301,"""So,I take an image from a canvas using p5js and i want to send it to the Azure Custom Vision Service(the code bellow).Is p5 image even the same as the normal js image(like when you take a capture from a video) ?My problem is when i send a form as a json like :But it gaves me ""401 Acces Denied"".Is it even possible to send an image created using p5 via http request?""",Joy,like,2,255,258
0,53643788,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""",Anticipation,would be appreciated,1,518,537
0,53643788,"""I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:Here is the data I am trying to query:Image of database:Any guidance would be appreciated.""",Joy,would be appreciated,1,518,537
0,36289389,"""I am having trouble understanding the concept of  API discovery  as used in Google products/services.  Here s some Python code that uses the said discovery service to access Google Cloud Vision:Here s another bit of Python code that also accesses Google Cloud Vision, but does not use API discoveryand works just fine:What I can t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call,after you have already prepared the code for calling that API?PS: I did look at the following resources prior to posting this question:I did seeanswered question but would appreciate additional insight.""",Anticipation,have prepared,1,565,585
0,55568798,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""",Anticipation,would expect,1,580,591
0,55568798,"""I am trying to write a unit test for a class that uses Google's vision API with thefrom thelib.The problem is that my mockedfor some reason still calls the realmethod and then throws a NPE, which breaks my test. I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5The class that is tested:The test:I would expect the mock to simply return(I know that this test doesn't make a lot of sense). Instead, it callswhich throws an NPE.""",Anticipation,I 'm wondering,1,266,278
0,52477690,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",Joy,good,2,221,224
0,52477690,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",Joy,point,1,226,230
0,52477690,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",Joy,is,1,214,215
0,52477690,"""We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""    "" logo.We checked the translation and it means Infiniti! Which is the good point ;-)Did anybody experience the same issue? And found a way to return the result in a specific language?Thanks""",Anticipation,result,1,304,309
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Anticipation,results,1,12,18
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Anticipation,attempts,1,370,377
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Trust,labels,1,151,156
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Trust,more labels,1,146,156
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Anger,ie,1,263,264
0,47558371,"""I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:I am able to get all images that have one or more labels with a query like:How do I get thevalue when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.I'm hoping this is obvious, but attempts to usefailed.""",Joy,like,1,171,174
0,51025861,"""Right now, my AWS account has the following policies:AmazonEC2FullAccessAmazonSQSFullAccessAmazonS3FullAccessAmazonAPIGatewayInvokeFullAccessCloudWatchFullAccessAmazonKinesisFullAccessAmazonRekognitionFullAccessAmazonKinesisVideoStreamsFullAccessAmazonKinesisFirehoseFullAccessAmazonSNSFullAccessIn order to setup an  application load balancer  with auto scaling group, target group, subnets in a VPC, what are the other policies that I would require?""",Anticipation,order,1,300,304
0,39252746,"""I am trying to use the Watson Visual Recognition API as an OCR component, however while it is doing a good job on the computerized text, I want to expand it more to recognize ""Nicely-handwritten"" text.Is it possible to use the custom classifiers to train the API? and if yes and someone did try it already, is it effective?""",Trust,effective,1,314,322
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Anticipation,attempting,1,253,262
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Anticipation,result,1,278,283
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Anticipation,expected,1,352,359
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Anticipation,promise,1,294,300
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Joy,successfully,1,399,410
0,50628383,"""Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise.I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?**** Workaround (working)aws.service}app.component""",Trust,'ve confirmed,1,303,315
0,45505722,"""I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?""",Anticipation,prepare,1,3,9
0,45505722,"""I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?""",Joy,would like,1,75,84
0,36602892,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""",Joy,successful,1,68,77
0,36602892,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""",Surprise,had,1,194,196
0,36602892,"""I am having trouble getting the Google Vision Sample App to have a successful API request.I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.The error coming back is:If you have any ideas, I would surely appreciate it.""",Surprise,luck,1,201,204
0,38753678,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",Anticipation,in,1,587,588
0,38753678,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",Fear,missing,1,536,542
0,38753678,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",Joy,joking,1,504,509
0,38753678,"""I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.If I send a photograph that shows a female subject, Watson responds with a proper identification.But if I send one, with the very same application, of a male subject, Watson does not even analyze it.I am not trying to classify. I just want to identify faces in a photo.Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?Thanks in advance for your help.PS.I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.""",Trust,credentials,1,680,690
0,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",Joy,would be appreciated,1,502,528
0,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",Joy,really would be appreciated,1,511,528
0,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",Trust,certain,1,72,78
0,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",Trust,labelled,1,226,233
0,47154016,"""I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image.I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.The code I have is:I am thinking a while loop:and change the URL: to the path with (x) But I can't get this to work.Any help would be really appreciated.Thanks, Nathan.""",Anticipation,would be appreciated,1,502,528
0,50265754,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",Anticipation,training,1,58,65
0,50265754,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",Anticipation,results,2,627,633
1,50265754,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",Anticipation,results,2,703,709
0,50265754,"""For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we gotBelow 90:  290 -  94 : 1295 - 100 : 18Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.Please help.""",Trust,certain,1,390,396
0,49168443,"""I ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url. The reference from the API () says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".I ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).It works fine if I use the url method, but I need to upload the image.This is the code I ve been using:My Bitmap works fine by the way.This is the error that the logcat gives me:So, finally, what do I have to do to send the proper image format that the api requires?Thank you in advance.""",Anticipation,in,1,715,716
0,35740396,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""",Trust,to enable,1,38,46
0,35740396,"""I am usingfor face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds becausecallback ofis called after 1, or 1.5 seconds.""",Trust,is enabled,1,186,195
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,155,161
1,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,490,496
2,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,600,606
3,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,633,639
4,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,1025,1031
5,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,1439,1445
6,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,results,7,1494,1500
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,prediction,2,244,253
1,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,prediction,2,1458,1467
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Anticipation,would expect,1,1394,1405
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Trust,is confirmed,1,1152,1168
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Trust,Label,1,1260,1264
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Trust,accurate,1,558,565
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Trust,more accurate,1,553,565
0,54580554,"""I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.I am using the TensowflowSharp library at version 1.12.0Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.Model and Label (tag) declarations:Input param for image:Processing:TensorFlowHelper.CreateTensorFromImageFile:ConstructGraphToNormalizeImage:I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.""",Fear,unsure,1,577,582
0,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Anticipation,result,2,222,227
1,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Anticipation,result,2,312,317
0,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Anticipation,expected,1,247,254
0,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,is not,2,229,234
1,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,is not,2,1317,1322
0,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,good,3,239,242
1,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,good,3,1002,1005
2,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,good,3,1326,1329
0,52843616,"""So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now.""",Joy,would be,1,991,998
0,38804790,"""The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only.""",Fear,missing,1,354,360
0,41872763,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""",Trust,guide,1,460,464
0,41872763,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""",Trust,the best,1,326,333
0,41872763,"""I am working in OCR android Application.Now I can take images and extract word easily using google vision APIbut the result is not100%according to theangleof capturing the image.andillumination.So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.(blurring,filtering)to smooth image and improve its quality.Soif there is any libraries or guide line to follow up with it in this subject.Howto improve image quality before extracting textI testes this libraries for OCR operation""",Anticipation,result,1,118,123
0,50741636,"""I'm trying to send pictures to the aws rekognition from my webcam to detect the activity of the person sitting in front of it using python.To do so I take a picture every 5 seconds and I send it to the aws.But when I do so it seems that he's always sending back information about the first frame that I sentHere is the result i get from that call:Knowing during the time of the call the image has changed a lot and should (at least I think) show me other results.I'm new to aws so I might be missing a point also it's my first post on stackoverflow so I might have forget to write something""",Anticipation,result,1,320,325
0,50741636,"""I'm trying to send pictures to the aws rekognition from my webcam to detect the activity of the person sitting in front of it using python.To do so I take a picture every 5 seconds and I send it to the aws.But when I do so it seems that he's always sending back information about the first frame that I sentHere is the result i get from that call:Knowing during the time of the call the image has changed a lot and should (at least I think) show me other results.I'm new to aws so I might be missing a point also it's my first post on stackoverflow so I might have forget to write something""",Anticipation,results,1,456,462
0,50741636,"""I'm trying to send pictures to the aws rekognition from my webcam to detect the activity of the person sitting in front of it using python.To do so I take a picture every 5 seconds and I send it to the aws.But when I do so it seems that he's always sending back information about the first frame that I sentHere is the result i get from that call:Knowing during the time of the call the image has changed a lot and should (at least I think) show me other results.I'm new to aws so I might be missing a point also it's my first post on stackoverflow so I might have forget to write something""",Fear,missing,1,493,499
0,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Trust,label,3,197,201
1,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Trust,label,3,858,862
2,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Trust,label,3,950,954
0,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Trust,Labels,1,1032,1037
0,45048382,"""Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time?Open to any suggestions on how to speed up response time from Cloud Vision.""",Anticipation,I 'm wondering,1,283,295
0,47780934,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",Anger,Crushed,1,114,120
0,47780934,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",Anger,trash,1,262,266
0,47780934,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",Disgust,trash,1,262,266
0,47780934,"""I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?Thanks.""",Sadness,Crushed,1,114,120
0,49635343,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",Anticipation,call,1,234,237
0,49635343,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",Anticipation,for,1,239,241
0,49635343,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",Fear,missing,1,472,478
0,49635343,"""I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.When I drag and drop an image on their demo page (), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.I am using their PHP client from github to make the API callls.I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine.Is there something I am missing here?""",Trust,is supported,1,119,130
0,37152708,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""",Anticipation,results,2,314,320
1,37152708,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""",Anticipation,results,2,609,615
0,37152708,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""",Anticipation,result,2,640,645
1,37152708,"""I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data.My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist.I first sent the below request through my controller, which worked great:The response of this first call is:The protocol is for one to grab the end of theurl, which is the operation id, and send it back to the results API url like below:The result I get is:I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app.Does anyone have any ideas or experience with this? I would greatly appreciate it.""",Anticipation,result,2,671,676
0,55570088,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""",Anticipation,results,1,171,177
0,55570088,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""",Anticipation,result,2,476,481
1,55570088,"""I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.""",Anticipation,result,2,558,563
0,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",Joy,successfully,1,75,86
0,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",Joy,happiness,1,289,297
0,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",Joy,like,2,158,161
1,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",Joy,like,2,272,275
0,49134014,"""I am currently working on the response from the. In the API,is used and I successfully get the HTTP response message by usingin HTTP_Request2_Response. Just like the API manual, I do the following:What I got is like this:However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode onbut I got an error:. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?Here is theoffor your reference:""",Anger,anger,1,278,282
0,51815006,"""I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.Here is an exampleAfter capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.""",Anticipation,results,1,817,823
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Trust,have enabled,1,76,87
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Trust,labels,2,275,280
1,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Trust,labels,2,606,611
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Trust,being enabled,1,361,373
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Trust,credentials,1,155,165
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Anticipation,expected,1,308,315
0,55302297,"""I have been trying to use the Rekognition API to detect text in an image.I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.Here is my code:I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled.My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.I am not sure what the issue might be, here are the docs for the method.""",Joy,gem,1,513,515
0,48019749,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""",Joy,would like,1,121,130
0,48019749,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""",Joy,'d like,1,233,239
0,48019749,"""I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.I write following javascript code but still ""undefined"" answer for thatfyi,is the overall result from JSON (Google vision API).Really appreciate your big help!!""",Anticipation,result,1,356,361
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Anticipation,in,1,515,516
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Anticipation,results,1,145,151
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Anticipation,intended,1,220,227
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Anticipation,result,2,229,234
1,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Anticipation,result,2,340,345
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Joy,enlighten,1,489,497
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Joy,:),1,503,504
0,35790590,"""I am stuck with a couple of problems when using Google Cloud Vision.After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?Also is there any way to recognize the texts within a specific area of a picture?If anyone knows about the issue that I am facing now, please enlighten me. :)Thank you in advance.""",Sadness,fail,1,203,206
0,36405717,"""I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.Any ideas?""",Sadness,bottom,1,372,377
0,42345567,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""",Anticipation,in,1,315,316
0,42345567,"""Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app.Thank you in Advance.""",Sadness,unable,1,26,31
0,55024058,"""I am developing an application that uses theGoogle vision APIand I have a question about the colors properties.Is the color shown in the properties with the highest percentage is the dominant color? And how that works ?Because the color with highest percentage is not accurate.""",Disgust,accurate,1,269,276
0,56403969,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""",Anticipation,results,1,757,763
0,56403969,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""",Anticipation,result,1,986,991
0,56403969,"""Google vision document text detection does a good job of detecting symbols & words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text.I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:The above output was generated using this:""",Trust,top 4,1,910,914
0,54839576,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""",Anticipation,prediction,1,123,132
0,54839576,"""I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!""",Joy,would like,1,164,173
0,53918980,"""I was followingfor using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:The exception is printed when the following function to add a person to an existing person group is called:This is how I am callingin the main method:I tried searching up this error in Google and came across, but that answer did not work for me.(Their answer was to pass in the subscription key and endpoint for theconstructor.)Would anyone be able to provide any insight into why this error is occurring?I have been unable to figure out what is causing it, but I believe it may have to do with. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people.""",Sadness,unable,1,567,572
0,53275683,"""I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?""",Trust,landmarks,1,284,292
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Anger,anger,2,649,653
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Anger,anger,2,1746,1750
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Anger,contempt,2,663,670
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Anger,contempt,2,1760,1767
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Disgust,contempt,2,663,670
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Disgust,contempt,2,1760,1767
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Disgust,disgust,2,680,686
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Disgust,disgust,2,1779,1785
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Sadness,Invalid,1,264,270
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Sadness,sadness,2,745,751
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Sadness,sadness,2,1846,1852
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Fear,fear,2,696,699
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Fear,fear,2,1795,1798
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Joy,happiness,2,709,717
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Joy,happiness,2,1808,1816
0,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Surprise,surprise,2,763,770
1,56405595,"""I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.this is code request azure face api,running python but code show errorthis is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]""",Surprise,surprise,2,1864,1871
0,50699149,"""Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this""",Joy,like,1,54,57
0,43882577,"""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result. It is not getting populated.""",Anticipation,result,1,228,233
0,43882577,"""I have a circle model in my project:caregiver schema:Sample Object:I have tried virtual populate for mongoosejs but I am unable to get it to work.This seems to be the exact same problem:I am only getting the object id's in the result. It is not getting populated.""",Sadness,unable,1,122,127
0,48582519,"""I am IAM user and trying to hit APIRekognitionService.CreateCOllectionfor the testing in POSTMAN, but getting thisMy header request isThough it works with sameAccess Key IdandSecret KeyusingCLI(Command Line Interface).Can anyone help me to solve this problem.""",Fear,Command,1,195,201
0,56026129,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""",Fear,missing,1,468,474
0,56026129,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""",Joy,successfully,1,286,297
0,56026129,"""I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:When I try to create my service, things start to fail:Indeed the service name 'personalize' is missing from the list.I already tried upgradingandto their latest version and restarting my kernel.Any idea as to what to try next would be great.""",Sadness,to fail,1,419,425
0,55158595,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""",Trust,labels,3,116,121
1,55158595,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""",Trust,labels,3,199,204
2,55158595,"""I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage.However the labels in the response shows 'FILTERED'Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?""",Trust,labels,3,258,263
0,48037316,"""I am trying to build an android application where I am usingfor detecting faces. I am followingtutorial. The problem is that, I am unable to produce thethat was supposed to be displayed on the screen after I click the button but theshows:Here is the code :MainActivity.javaThe picture is stored inlocation as. I have tried setting the types as,,but none of it works.Can anyone help me in this?""",Sadness,unable,1,132,137
0,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Anticipation,am attempting,1,3,15
0,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Anticipation,expects,1,212,218
0,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Joy,successfully,2,461,472
1,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Joy,successfully,2,808,819
0,52186137,"""I am attempting to perform a Google reverse image search usingon an Azure app service web app.I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is, and I have successfully set the variable here:Furthermore, the googleCred.json file has been uploaded to the app service. Here is theI followed to use FTP and FileZilla to upload the file:Also, the file permissions are as open as they can be:However, when I access the web app in the cloud, I get the following error message:What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?""",Trust,CREDENTIALS,1,293,303
0,56014699,"""I am using google vision api for detect text in image.Now, I want to know font size of text in image. How do it calculate ?or Do google vision api support?""",Trust,support,1,148,154
0,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",Surprise,results,1,197,203
0,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",Surprise,explored,1,208,215
0,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",Anticipation,ready,1,292,296
0,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",Joy,so good,1,324,330
0,55979551,"""I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue.I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?""",Trust,clarify,1,257,263
0,53269405,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",Anticipation,result,1,277,282
0,53269405,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",Anticipation,would result,1,402,413
0,53269405,"""Hey Stackers.I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;That's all fine. No excepton is thrown. However, the result ofis empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour.Am I doing something wrong or am I missing something?""",Fear,missing,1,468,474
0,53591219,"""this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance""",Anticipation,in,1,210,211
0,33687536,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",Anticipation,I wonder,1,376,383
0,33687536,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",Sadness,refused,1,280,286
0,33687536,"""I am trying to try the. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:python-server.pyI have an index.html file in which I am sending the http request:index.htmlAfter about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.""",Trust,enabled,1,75,81
0,44085992,"""How to get square crop hint from Google Cloud Vision?I'm setting the aspect ratio to 1 but the resulting hint is never an exact square.Below is the modified example fromchecking sample image from.The result is (zeroes are not printed):""",Anticipation,result,1,201,206
0,38336213,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""",Anticipation,calls,1,93,97
0,38336213,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""",Anticipation,for,1,99,101
0,38336213,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""",Anticipation,would be appreciated,1,438,457
0,38336213,"""In swift I'm using the Microsoft Cognitive Services Face API functionand trying to usewhich calls for. I need help with what to enter into the Array.According toI assumedwould work but I receive an error saying:And usinggives an error:For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.""",Joy,would be appreciated,1,438,457
0,54295015,"""I'm using thefeature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?Here is the sample web detection JSON output with entity id starting with ""/t"" & ""/m"" having no description.""",Fear,missing,1,205,211
0,40957513,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""",Joy,to achieve,1,596,605
0,40957513,"""What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.""",Trust,educational,1,234,244
0,55120982,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",Trust,supporting,1,97,106
0,55120982,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",Trust,accurate,1,392,399
0,55120982,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",Trust,quite accurate,1,386,399
0,55120982,"""I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:Format 1:I should be capable of getting:First Name: CARMENLast Name: MUESTRA MUESTRADate of birth: 01/01/1980DNI: 12345678AFormat 2:First Name: NOMBRELast Name: APELLIDO1 APELLIDO2Date of birth: 01/05/1972DNI: 99999999-RAlthough the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.The JSON response aggregates the text in different blocks informat 1for instante BLOCK 1 (ESPA A) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.1) What recommendations would you make to automate this process?2) Can you show an example of the processing of the response in a similar scenario?3) Is there a way to train the platform to aggregate the info according to what we want to extract?""",Joy,would like,1,3,12
0,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",Anticipation,expected,1,77,84
0,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",Anticipation,result,1,86,91
0,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",Anticipation,expect,1,96,101
0,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",Anticipation,results,1,168,174
0,40695516,"""I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see   for German and  ,  ,  ,  ,  ,  ,  ,   for Polish in the results. Is there a way I can do it?Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly.I use drag&drop option hereand CloudVision Android Sample. Thank you for any advices.""",Trust,supported,1,320,328
0,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Anticipation,results,3,230,236
1,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Anticipation,results,3,380,386
2,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Anticipation,results,3,492,498
0,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Anticipation,were expected,1,238,253
0,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Joy,successfully,1,133,144
0,50360498,"""I m new usingAmazon Rekognitionto analyze faces on a video.I m usingstartFaceSearchto start my analysis. After the job is completed successfully, I m using the JobId generated to callgetFaceSearch.On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can t understand why.Viewing the JSON generated as results for my second video, completely different faces are identified with the sameindex number.Please see the results below.In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?""",Joy,is completed successfully,1,120,144
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Anticipation,result,1,596,601
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Anticipation,progress,1,873,880
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Joy,successfully,1,227,238
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Joy,progress,1,873,880
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Sadness,invalid,1,518,524
0,42044047,"""I am trying to reproduce in R the Microsoft Emotion API program located. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:However, when I open the given URL from the 'operationLocation' object, it says:This seems to indicate that my requestdidn'tgo through after all.Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?To add additional info, I also tried running the next few lines of code given from the linked web site:When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.""",Sadness,Failed,1,929,934
0,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Trust,credentials,1,423,433
0,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Trust,CREDENTIALS,1,466,476
0,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Trust,credential,1,498,507
0,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Fear,command,2,206,212
1,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Fear,command,2,746,752
0,48748566,"""I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command,it was successful.I wrote test.pyand when I run this codes,the error happens.I wrote codes by seeing,so I rewroteerror happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS orexplicitly create credential and re-run the application. For moreinformation, please see. . I really cannot understand why this error happens.I installed config.json & index.js & package.json in same directory as test.py but same error happens.I run commandbut zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?""",Joy,successful,1,221,230
0,52323135,"""I have a template like thisand wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887I've got a working code that looks like thiswhich gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?Thks""",Trust,certain,1,75,81
0,43852275,"""I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the.It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:Afterwards, when I run pip, I get this:I'm not sure how to go about fixing this.""",Joy,would like,1,74,83
0,40189866,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""",Anticipation,result,1,175,180
0,40189866,"""I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?""",Trust,the best,1,156,163
0,53675809,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""",Disgust,doesn't work,1,412,423
0,53675809,"""I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.Scenario;User takes 2 photos (working)on second taken, I will parse images to bytes (working)send bytes with standard request to aws API (doesn't work)I changed the image quality to the lowest as well, but It didn't help.Main.dart codeAWS rekognition code""",Sadness,unfortunately,1,260,272
0,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Joy,great,1,274,278
0,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Joy,computer,1,291,298
0,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Joy,Like,2,117,120
1,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Joy,Like,2,474,477
0,52326489,"""I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the outputexpectedCode reference:Is there a way to get this in one line, or understand if its part of that line?Any other API that can help in this scenario?""",Surprise,'m exploring,1,2,13
0,36570132,"""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?""",Trust,credentials,2,87,97
1,36570132,"""I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:Does anyone know where I can add the API key or locate to the credentials file?EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:I received the following error:Does anyone know how I can solve this error?""",Trust,credentials,2,197,207
0,53224704,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""",Anger,smashing,1,225,232
0,53224704,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""",Anticipation,order,1,212,216
0,53224704,"""So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a crossHere is my code where I get the coordiantes and use them to place a polygonHere is a var_dump of $points(The coordintes)""",Joy,:),1,279,280
0,55956173,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",Anticipation,in,1,258,259
0,55956173,"""I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!""",Anticipation,repeatedly,1,6,15
0,50275116,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",Anticipation,in,1,563,564
0,50275116,"""I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library () as explained here:On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file.I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:What I am doing wrong? Thx in advance!""",Trust,CREDENTIALS,1,361,371
0,41501511,"""I have extracted tags from the given image using Clarifai and Google Vision APIs. Similar thing I want to achieve for videos.Can anyone suggest, if there are any APIs available to do so.Thanks!""",Joy,to achieve,1,104,113
0,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",Anticipation,results,2,256,262
1,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",Anticipation,results,2,346,352
0,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",Anticipation,result,1,375,380
0,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",Disgust,does not work,1,482,494
0,45484524,"""With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.Apparently, there are a lot of very famous logos which are not recognized at all in pictures.For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?""",Trust,team,1,14,17
0,47872907,"""Get different result for text detection from .Net code and demo app for the same imageandthis is my code:""",Anticipation,result,1,15,20
0,53105446,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",Fear,doubt,1,1066,1070
0,53105446,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",Sadness,console,1,971,977
0,53105446,"""I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):aws-rekognition-config.jsand the index.js where I do a simple test to compare the two images in my bucket:As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config:And the user credentials I'm using has more permissions than it needs to for this task:I also have to mention that I uploaded the files via api as well using the npm package multer-s3:and then it's applied as a middleware:I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...I also tried using the cli and I get the same error:The guy in this video couldn't do the same as I wanted either:and this guy could using the same privileges I haveIf I throw this other operation it works:it returns:so it must be something with the compare-faces endpoint.What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case.Thank you very much!""",Trust,credentials,1,642,652
0,41527687,"""I'm playing around with the Amazon Rekognition. I found a reallyto take an image from my webcam which works like this:I'm then trying to convert thisto a, which is what must be submitted to the Rekognition library. This is what I'm doing:However when I try and do some API call to Rekognition with the, I get an Exception:Thestate that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:However, the same Exception ensues.Any ideas? :)""",Joy,:),1,532,533
0,46046085,"""I'm developer but new in Android and I need to know how can I use camera inside a Activity Layout.I know I need to use Surface View to insert camera inside an activity and currently my app is reading QR Codes with Google Vision using default camera (a button opens camera, the user takes the photo and my app perceive the activity result).But I really need to implementation that function inside app with real-time scanner.Someone can direct me?""",Anticipation,result,1,332,337
0,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",Anger,is complaining,1,448,461
0,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",Anticipation,attempts,1,530,537
0,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",Joy,Nice,1,511,514
0,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",Sadness,fails,1,112,116
0,48343919,"""I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.I'm using the unmodified sample provided on Github:EDIT:Here is my source file:I've done the following steps:Created a working Firebase projectActivated the Vision API and the billing for the projectInitialized the Firebase Functions localy on my PCInstalled needed npm modules withTried to deploy withThen i got this error:So he is complaining about this line:My package.json looks like this:Nice to know:Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.Can someone please give me a hint what went wrong with my setup?Thank you!""",Trust,Nice,1,511,514
0,49562890,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""",Anticipation,results,1,650,656
0,49562890,"""Tenho uma quest o sobre a limita  o da Face API da Microsoft Azure, existe uma limita  o no plano standard que deixa fazer apenas 10 requisi  es por segundo. Estamos com problema quanto a esse n mero por n o atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceit vel, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisi  es por segundo?I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?""",Sadness,sob,1,264,266
0,46516766,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""",Anticipation,results,1,61,67
0,46516766,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""",Anticipation,result,2,203,208
1,46516766,"""I'm testing the OCR with Google cloud vision and I find the results are particularly bad. My documents are in french but it misses many apostrophes and commas.For example as inputWith the codeI get the result (with errors highlighted in yellow)When I test the same image with, the result is absolutely perfect, without having to indicate the language.Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?""",Anticipation,result,2,282,287
0,45508302,"""I ve faced such a problem:In the documentations AWS have Rekognition SDK of Unity3DBy downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).Through the nuget I can t get SDK in Unity3D, because it s in conflict with Core.Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.SDK Rekognition for Unity3D exists?Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?Thank you.""",Anger,conflict,2,293,300
1,45508302,"""I ve faced such a problem:In the documentations AWS have Rekognition SDK of Unity3DBy downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).Through the nuget I can t get SDK in Unity3D, because it s in conflict with Core.Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.SDK Rekognition for Unity3D exists?Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?Thank you.""",Anger,conflict,2,367,374
0,45508302,"""I ve faced such a problem:In the documentations AWS have Rekognition SDK of Unity3DBy downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).Through the nuget I can t get SDK in Unity3D, because it s in conflict with Core.Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.SDK Rekognition for Unity3D exists?Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?Thank you.""",Sadness,conflict,2,293,300
1,45508302,"""I ve faced such a problem:In the documentations AWS have Rekognition SDK of Unity3DBy downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).Through the nuget I can t get SDK in Unity3D, because it s in conflict with Core.Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.SDK Rekognition for Unity3D exists?Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?Thank you.""",Sadness,conflict,2,367,374
0,43793934,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""",Anticipation,attempting,1,118,127
0,43793934,"""Using Google vision fromI was successfully able to create aand anusingandrespectively. And then send my image using, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, fromquestion, that by setting the language to another (non-latin) language would help with this.But that is where I am stuck, I'm not sure where to set the, and yes I have seenlink to the documentation of the, but I am still confused as to where this comes in.""",Joy,successfully,1,31,42
0,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Anger,conflict,3,462,469
1,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Anger,conflict,3,563,570
2,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Anger,conflict,3,639,646
0,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Sadness,conflict,3,462,469
1,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Sadness,conflict,3,563,570
2,40025350,"""I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structureto avoid this conflict (if that is indeed the cause):""",Sadness,conflict,3,639,646
0,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Anticipation,results,2,102,108
1,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Anticipation,results,2,262,268
0,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Trust,managed,1,177,183
0,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Trust,an,1,136,137
0,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Trust,a,1,157,157
0,55194095,"""I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv.Do you have any suggestions?""",Trust,friend,1,159,164
0,44240464,"""I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.I run the code from Eclipse in my local system.I tried the below items as found in some forums;1) Degraded all the netty* jars from 4.1.6 to 4.1.32) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar3) Adding the pom.xml4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account""",Trust,CREDENTIALS,1,413,423
0,56178717,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""",Trust,reasonable,1,134,143
0,56178717,"""I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?""",Trust,the best,1,107,114
0,37900554,"""I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:I've tried the following:[base64image]{base64image}""data:image/jpeg;base64,"" + base64image""image/jpeg;base64,"" + base64imageand more.I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?Note: The operation works when sending a URL as application/json.""",Trust,valid,1,184,188
0,49732664,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",Anticipation,probable,1,320,327
0,49732664,"""So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :However it shows the following error :Where is the probable error? I looked up to this :However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.""",Joy,looked,1,338,343
0,54757580,"""I'm receiving this error when trying to train my first custom model:I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.""",Trust,credentials,1,231,241
0,35672845,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""",Trust,have enabled,2,58,69
1,35672845,"""I'm trying to use the Google Vision API. I'm following:I have enabled the Cloud Vision APII have enabled billingI have set up an API keyMade base64-encoded data from my imageMade JSON file with settings:Sent request with:After that I got response:\u003cempty\u003e means <empty>Any ideas? Somebody have the same problem?""",Trust,have enabled,2,93,104
0,36774015,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",Anticipation,result,1,742,747
0,36774015,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",Anticipation,results,1,1266,1272
0,36774015,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",Joy,funny,2,1020,1024
0,36774015,"""I've encountered the following issues with the text detection feature of Google Cloud Vision:1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code inso I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotationand now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.Do you have any hint? Thanks.""",Surprise,unpredictable,1,1661,1673
0,54881611,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""",Joy,grateful,1,466,473
0,54881611,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""",Joy,good,1,14,17
0,54881611,"""Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student).If you can provide any links, videos, or tutorial I will be grateful.Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.""",Joy,video,1,28,32
0,47155510,"""I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?""",Anticipation,training,1,239,246
0,50714894,"""Background information:I'm trying to create a PoC for Google Cloud Vision API using their.What I have done:Create a simple console apps with the following code for Vision API.Problem:The linegets stuck for a long time before ultimately throwing the error.My code sits behind a corporate proxy, but I have validated that it is not blocking internet access because I can callfrom the same apps and get its JSON response just fine.Any suggestions?""",Anticipation,ultimately,1,226,235
0,52539984,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""",Fear,unsure,1,722,727
0,52539984,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""",Joy,would like,1,660,669
0,52539984,"""I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:The image is converted to a string using base64:In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?""",Trust,certain,1,550,556
0,39797164,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Trust,official,1,133,140
0,39797164,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Trust,Sharing,1,524,530
0,39797164,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Fear,missing,1,381,387
0,39797164,"""In this,, says this:Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event.In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in theAm I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?[EDIT] Sharing code by request of pm0733464:For the record, I began with the fork of the vision api Demo which allows forMy code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things upinBarcodeCaptureActivityI changecreateCameraSourcewhere it defined the barcode detector like to this:My FocusProcessor (in the same class) looks like this:""",Joy,like,1,982,985
0,45624819,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""",Anticipation,result,2,264,269
1,45624819,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""",Anticipation,result,2,350,355
0,45624819,"""Does the Cloud Vision API return a score?public float getScore() Overall relevancy score for the web page.Thes that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.Pleas advise. Thanks.""",Trust,verified,1,286,293
0,45274013,"""I have a number of non-JPEG image files that I want to process using Google Cloud Vision, but the API only accepts certain formats (see questionand answer).I can use PIL or some such to convert a TIFF to JPEG to be uploaded, but I'd like to avoid a temp file if possible.So, in python, how do I convert a TIFF in-memory for upload to GCV? numpy array, base64, string..?""",Joy,'d like,1,231,237
0,45274013,"""I have a number of non-JPEG image files that I want to process using Google Cloud Vision, but the API only accepts certain formats (see questionand answer).I can use PIL or some such to convert a TIFF to JPEG to be uploaded, but I'd like to avoid a temp file if possible.So, in python, how do I convert a TIFF in-memory for upload to GCV? numpy array, base64, string..?""",Trust,certain,1,116,122
0,54451771,"""im trying to pull all images from a website and analyze each one using aws image recognition api, it works for some websites however some websites  return an error sayingbascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.can someone please explain what im doing wrong and how to prevent this error  ?""",Anticipation,to prevent,1,447,456
0,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",Anticipation,result,2,345,350
1,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",Anticipation,result,2,554,559
0,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",Anticipation,results,1,451,457
0,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",Anticipation,i was wondering,1,720,734
0,40079213,"""I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base onThe code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.""",Surprise,discovered,1,377,386
0,56176858,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",Fear,missing,1,464,470
0,56176858,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",Sadness,fails,1,323,327
0,56176858,"""I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an. Here's the code:When I run the second script I get the following errors:How do I authenticate for this service? What am I missing?""",Trust,authenticate,1,423,434
0,54089791,"""I am using Google Vision via Rest API v1 with featureand  API is returning correct result:what I want to deserialize via Json.Net to Google vision object (fromnuget), but when I run thisit will return me empty resultsWhen I try to deserialize it to, then I am getting this error message..I cannot use nuget package directly for detection, because I need to call it via rest API""",Anticipation,result,1,84,89
0,55666678,"""I created a collection using boto3 with following code:which is appearing as created when I fetch it using the following code in python boto3:But when I try to fetch the same collection using Javascript SDK ""aws-sdk"" in nodeJs using following code I get empty results:RESPONSE JS:""",Anticipation,results,1,261,267
0,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",Anticipation,results,2,538,544
1,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",Anticipation,results,2,880,886
0,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",Anticipation,result,1,1043,1048
0,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",Joy,is appreciated,2,908,921
1,47543228,"""So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (whichalwaysends in with a '?')Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult.Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.Here is my current codeResults of running the codeWhat I need the result to be""",Joy,is appreciated,2,957,975
0,53799908,"""I am using  this Java  Lambda  code provided by  AWS   to detect labels in a  video:When  checking the  results in cloudwatch  I receive an error:I  am using the root   account and have  set  up   AWS  as described in the  "" Create the AWS Toolkit for Eclipse Lambda Project""  Rekognition developer-guide.Is the  problem  with the code  or something    else ?""",Anticipation,results,1,105,111
0,53799908,"""I am using  this Java  Lambda  code provided by  AWS   to detect labels in a  video:When  checking the  results in cloudwatch  I receive an error:I  am using the root   account and have  set  up   AWS  as described in the  "" Create the AWS Toolkit for Eclipse Lambda Project""  Rekognition developer-guide.Is the  problem  with the code  or something    else ?""",Trust,labels,1,66,71
0,54039956,"""I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.I have to pass this image to Google Vision for our scanning.I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.How I can to use imagemagick for this problem ?I want use PNG because is probably better for the ocr, but I don t want a static resolution, but I want only limit the size at little less of 5MB.""",Trust,support,1,257,263
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Anticipation,expect,1,121,126
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Anticipation,warning,1,673,679
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Anticipation,WARNING,1,798,804
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Fear,uncertain,1,1002,1010
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Fear,command,1,103,109
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Surprise,uncertain,1,1002,1010
0,35755940,"""I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.When I pass an image to the command like so I expect to get back some json telling me what is in the image.However, I am getting this error instead.Lots going on here.But the Project APIisenabled.So this is part of the error message is erroneous.It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".I applied this fix ...After applying this fix, I get fewer errors ...It appears that this error message:""No handlers could be found for logger ""oauth2client.util"" is actually masking a more detailed warning/error message and that I can see the more detailed one by adding this code ...So no I am stuck on this error message:WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)It has been suggested that this error can be avoided by using named parameters instead of positional notation.However, I am uncertain exactly where I might make this change.I don't actually see the oauth2client.util:build() function in the code.Here is the google code (slightly modified):""",Trust,Label,1,17,21
0,54802917,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",Trust,poll,1,463,466
0,54802917,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",Trust,is supported,1,127,138
0,54802917,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",Joy,'s,1,247,248
0,54802917,"""I thought that I could by at of today from the docs it looks like I can't (). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (). Can someone confirm this?If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...many thanks!!""",Joy,good,1,252,255
0,50423259,"""I'm facing this issue today. Unable to access Microsoft FACE Api endpoint:Anyone facing the same issue today?Error:""",Sadness,Unable,1,30,35
0,49182513,"""I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings. But when I attempt to use the client, it doesn't work.The.The code I use:The exception I'm being thrown:I'm guessing there has to be something going on that i'm not aware of""",Disgust,doesn't work,1,165,176
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Trust,certain,1,180,186
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Trust,valid,1,392,396
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Trust,enabling,1,407,414
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Joy,would be appreciated,1,779,798
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Joy,like,1,314,317
0,51642038,"""I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on, the response I get back is:I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard.Below is a segment of the code where I make the request:Here is the request body (imageRequests as it's called in my code above) that is sent to the API:Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.Any help would be appreciated.""",Anticipation,would be appreciated,1,779,798
0,56093211,"""I have encountered several cases that a call to object_localization, on Google Vision API, returns empty, whereas a cell to label_detection with the same image returns many object with hight level of certainty.Example:This image from Google Street ViewThe API can identify the billboard with high certaintyBut then I try it with object localization and... nothingWhen I try it with other images I get a partial localization, like in this image:I do get both labels and object localization annotations:Labels:Object localization annotations:So, it appears that there's no fundamental problem with my API configuration, but is there anything I can do different to make it work?""",Trust,to label,1,122,129
0,56093211,"""I have encountered several cases that a call to object_localization, on Google Vision API, returns empty, whereas a cell to label_detection with the same image returns many object with hight level of certainty.Example:This image from Google Street ViewThe API can identify the billboard with high certaintyBut then I try it with object localization and... nothingWhen I try it with other images I get a partial localization, like in this image:I do get both labels and object localization annotations:Labels:Object localization annotations:So, it appears that there's no fundamental problem with my API configuration, but is there anything I can do different to make it work?""",Trust,labels,1,459,464
0,56093211,"""I have encountered several cases that a call to object_localization, on Google Vision API, returns empty, whereas a cell to label_detection with the same image returns many object with hight level of certainty.Example:This image from Google Street ViewThe API can identify the billboard with high certaintyBut then I try it with object localization and... nothingWhen I try it with other images I get a partial localization, like in this image:I do get both labels and object localization annotations:Labels:Object localization annotations:So, it appears that there's no fundamental problem with my API configuration, but is there anything I can do different to make it work?""",Trust,Labels,1,502,507
0,49510330,"""I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.Detailed error:""",Anticipation,have synchronized,1,235,251
0,55982785,"""I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?""",Anticipation,training,2,30,37
1,55982785,"""I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?""",Anticipation,training,2,60,67
0,51319671,"""This is my gem file. But when i use bundle install i see this output.""",Joy,gem,1,12,14
0,35952647,"""I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.This is my api_request.js file:It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file.Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta.^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?""",Trust,decided,1,492,498
0,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Anticipation,attempted,1,169,177
0,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Anticipation,would be appreciated,1,449,468
0,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Disgust,is not supported,1,134,149
0,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Joy,would be appreciated,1,449,468
0,51429447,"""I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '' even though the bucket is based in another region. Any help or advice would be appreciated.EDIT: I thought it may be relevant to mention, I am running this on Python.""",Sadness,Unfortunately,1,103,115
0,38634409,"""I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for exampleThe problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.I will be very greatful for sharing some info.""",Trust,sharing,1,558,564
0,49589030,"""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.""",Sadness,Unable,1,88,93
0,49589030,"""Google vision is throwing me the following error on ruby on rails which had me baffled.Unable to convert ""image_path"" to an ImageHowever, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.""",Surprise,baffled,1,80,86
0,46287956,"""I have created a collection in was CLI like so:I would like to rename that collection to another string. I can't find how to do that. Any suggestion?""",Joy,would like,1,50,59
0,54444114,"""i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying. If i use a local image file, then it works perfectly fine.     Can someone please help me ? Thanks""",Anticipation,order,1,115,119
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Trust,authenticate,1,103,114
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Trust,authenticated,1,168,180
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Trust,credentials,1,1094,1104
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Trust,the best,1,226,233
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Anger,stupid,1,417,422
0,50159443,"""My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.I've searched Google and Stack Overflow and the best leads I found were:Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:I keep receiving this error,Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.In, I put (I overwrote some contents in this post for security):and in, I put:also, tried:I have also tried changing these variable names in both files instead ofwithandbased on this Google page.Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!""",Joy,gem,1,676,678
0,50258562,"""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.""",Trust,certain,1,49,55
0,50258562,"""I'm having trouble figuring out how to access a certain folder within a bucket in s3 using PythonLet's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:""myBucket/subfolder/images/""In /images/ folder there are:I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.""",Trust,labels,1,314,319
0,50630045,"""I need to recognize image with Google Vision API. Among the examples, they use following construction:I need to do similar, but my image comes from:Which returns numpy array, not bytes. I tried:Which converts array to bytes, but returns different bytes apparently, since it gives different result.So how to make my image array similar to one which I get byfunction""",Anticipation,result,1,291,296
0,49819964,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""",Joy,would like,2,122,131
1,49819964,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""",Joy,would like,2,267,276
0,49819964,"""So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this outputThis is my codeI don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.""",Anticipation,in,1,300,301
0,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Anticipation,prediction,1,335,344
0,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Anticipation,training,1,396,403
0,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Joy,does not seem,1,75,87
0,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Joy,well,1,97,100
0,54521080,"""I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.""",Trust,labels,1,191,196
0,38417738,"""I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,}Can anyone help me out?""",Trust,labels,1,298,303
0,55555575,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""",Trust,could assist,1,260,279
0,55555575,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""",Trust,instructions,1,303,314
0,55555575,"""I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?The instructions tell me to change {body} to""",Anticipation,instructions,1,303,314
0,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Trust,CREDENTIALS,2,192,202
1,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Trust,CREDENTIALS,2,533,543
0,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Trust,credentials,1,262,272
0,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Fear,command,1,216,222
0,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Joy,am,1,242,243
0,51899558,"""I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here.I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.Here are some of the things that I've done but didn't manage to work stillReinstalled google-cloud-texttospeechEnsure that my environment variables are set by using env & set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to checkRestarted my raspberry pi""",Joy,positive,1,245,252
0,49416747,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""",Anticipation,am attempting,1,3,15
0,49416747,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""",Anticipation,expected,1,346,353
0,49416747,"""I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.The example curl command iswhereis replaced with an actual api keyAs this is just hitting a URL I expected I should be able to useHoweverreturnsWhat am I missing in mycommands?  Do I need to specify some sort of headers or something?Documentation link""",Fear,missing,1,402,408
0,47000735,"""I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".I did everything like in. But it doe not work :(So, what I have:1) app.yaml:2) appengine_config.py3) requirements.txt4) lib folders with all libs from5) main.py that use google cloud vision lib.Error:Could you help me to understand where I was wrong?""",Sadness,:(,1,155,156
0,50874265,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",Trust,labels,2,160,165
1,50874265,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",Trust,labels,2,244,249
0,50874265,"""I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:ResourceGoogleVision.pythe labels variable is of typeAs you can see i am using message to json function on the labels response. But i am getting the above error.Is there a way to convert the result to a json format?""",Anticipation,result,1,324,329
0,44467350,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""",Anticipation,results,1,143,149
0,44467350,"""JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:How can i add it to the json code in a valid way. I keep getting syntax errors!!""",Trust,valid,1,273,277
0,47296283,"""I'm trying to authenticate google vision api with electron and it seems like the google vision api can't find the required libraries so it gives me these errors.I added the node module usingand this is how I'm trying to initialize it""",Trust,to authenticate,1,12,26
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Anticipation,expect,2,819,824
1,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Anticipation,expect,2,1638,1643
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Sadness,bottom,1,590,595
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Sadness,console,1,1182,1188
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Trust,purely,1,395,400
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Trust,criteria,1,2111,2118
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Anger,silly,1,1165,1169
0,55846066,"""Current BehaviourI'm usingwith iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.I've also raised anon their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.Expected BehaviourI expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the codeWhat have I tried to fix it?Disable theas this was a fix for Android, no luck here.Manually set the.Manually setto center of screen.Change theto 0.2 and slowly increase to the point of where it starts to look silly.Setto just console.log something as this was another fix for android.UpdateUpdate to master branch atHow can I recreate it?Create new react-native project/setto use the front camera.set(It's on by default anyway, this just ensures it.setTry to scan a Code39 / Code128 -Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.Software Used & VersionsiOS: 12.1.4react-native-camera: ^2.1.1 / 2.6.0react-native: 0.57.7react: 16.6.1CodeI render the camera in aand I've put my code below.Relevant Package CodeI found some code that seems relevant:atMore specifically just this section here:Ifit will just return providing it doesn't meet any of the other criteria.""",Disgust,silly,1,1165,1169
0,50168647,"""I am relatively new to Python and trying to implement a Multiprocessing module for my for loop.I have an array of Image url's stored in img_urls which I need to download and apply some Google vision.This is my runAll() methodI am getting this as the warning when I run it and python crashes""",Sadness,crashes,1,284,290
0,50160997,"""I'm working on a project in which I need to analyze an image using Google's Vision API and post the response to a Dynamodb table.I have successfully implemented the Vision API, but not able to convert its response into Python Dictionary.Here's what I have tried:Here's the Vision api implementation:And Here's theFunction:Now, It doesn't return any error but theis not posted in Database table because it's not the correct form of the object, the problem here is thetype of response returns from Google's API.""",Joy,successfully,1,137,148
0,46287595,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""",Joy,would like,1,152,161
0,46287595,"""I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:How do I tell it to index all images in the ""name"" bucket?no luck.""",Trust,good,1,49,52
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Sadness,stripped-down,1,94,106
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Sadness,fail,1,649,652
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Sadness,fails,1,884,888
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Anticipation,expect,1,276,281
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Anticipation,result,1,1118,1123
0,51731727,"""I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.Here's a stripped-down JavaFX app that is supposed to do that:This looks to me as though I followed the ScalaFX documentation'sfor creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the->binding, but it manifests less often.Why do my properties sometimes fail to call the bindings when they change?edit 1JavaFX stores the listeners associated with bindings as a, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.""",Trust,associated,1,726,735
0,53564632,"""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.""",Fear,concerned,1,251,259
0,53564632,"""How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.""",Sadness,concerned,1,251,259
0,53123056,"""I need to return the message sent by Rekognition to SNS but I get this error in  CloudWatch:Code:And is this the correct way of implementing Rekognition stored video in AWS Lambda with python I didn't find any examples on it.Update:The steps my app needs to take are:In the frontend, the user triggers a lambda function with API gateway which sends a file to s3When the file arrives trigger the same lambda function to apply video recognition and send jobId to SNSwhen SNS receives a message trigger the same lambda function to get the label data and return the data back to the user with API gateway""",Trust,label,1,537,541
0,37786967,"""I am trying to create a Watson Visual Recognition Create Classifier using v3 of the rest API following the documentationwhich states:However, using a ""positive"" zip file of 48MB containing 594 images (max size of an image is 144Kb) and a ""negative"" zip file of  16MB containing 218 images (max size of an image is 114Kb) but I keep getting the error:In response to:I've kept trying reducing the file size by deleting images within the zips and re-trying but I'm well below the stated limits.Anyone got any idea?Thanks""",Joy,'m,1,460,461
0,37786967,"""I am trying to create a Watson Visual Recognition Create Classifier using v3 of the rest API following the documentationwhich states:However, using a ""positive"" zip file of 48MB containing 594 images (max size of an image is 144Kb) and a ""negative"" zip file of  16MB containing 218 images (max size of an image is 114Kb) but I keep getting the error:In response to:I've kept trying reducing the file size by deleting images within the zips and re-trying but I'm well below the stated limits.Anyone got any idea?Thanks""",Joy,well,1,463,466
0,52119949,"""When I runcontaining various vulgar fraction symbols through the, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?""",Disgust,vulgar,1,30,35
0,50182544,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""",Disgust,did not work,1,412,423
0,50182544,"""I am pretty new this area and I started firebase cloud function 2 days ago.Sorry, I am still a student so I might not understand clearly some documentation.I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.my cloud functionI am using firebase cloud function and Google Vision API.actually I tried to pass the parameter like thisMy client side coeand it did not work. I always got null return when I trigger the function.So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?p.s: when I tried the code with node the_code.js it works.""",Joy,like,1,378,381
0,53381742,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""",Anticipation,I wonder,1,1026,1033
0,53381742,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""",Disgust,did not work,1,359,370
0,53381742,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""",Joy,successful,1,556,565
0,53381742,"""I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question:But it did not work for me for the reasons described below, which is why I'm asking a question of my own.Here is the problem:I am making the following post request to the specified urlThe POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)However, when I make a get request on the urlI get the following error:Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one requestThanks for the help.""",Trust,the best,1,1045,1052
0,48548552,"""I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.""",Fear,missing,1,190,196
0,37690111,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",Joy,like,1,826,829
0,37690111,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",Sadness,unable,1,509,514
0,37690111,"""I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.When I executed it (hundreds of delete requests in parallel), I received a. After that, all of my delete requests (even individual ones) received an.My questions are:Is there a better way to delete all classifiers that is not doing it one by one?Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?This is the 429 error that I received in the multiple delete requestsEdit:I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that.I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.This is the JS function that I'm using to delete a single classifier-EditThe occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer""",Surprise,result,1,1512,1517
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Trust,valid,1,275,279
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Trust,credentials,1,209,219
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Trust,credential,1,296,305
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Trust,CREDENTIALS,1,361,371
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Anticipation,result,1,128,133
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Anticipation,Expected,1,222,229
0,48219196,"""I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.""error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential""For more information- I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.""",Sadness,invalid,1,186,192
0,45455138,"""we are using google vision ocr for gathering text from receipts.In some cases the receipt have some text written in vertical  , like vat information and some other.The question is that  google vision read efficiently only the text in the main orientation (horizontal by example) and discards all the text written in the same receipt in vertical orientation instead in horizontal.Is there a parameter to set up for tell google vision to acquire also the text in vertical orientation?I have put online an example with an image with text in two orientations .Text recognized from g-vision :Horizontal text lineText I've expected to be recognized:Horizontal text lineVertical text line""",Anticipation,'ve expected,1,614,625
0,39212656,"""I am trying to implement and add google vision services to my project using the below github sample code link.Running into this error in ImageText and Word java classes wherewhere AutoValue_ImageText type can not be resolved andwhere AutoValue_Word type cannot be resolved.please help! i can not even fix these syntax errors to see if this code even complies properly.thank you in advance""",Anticipation,in,1,379,380
0,50018491,"""I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here ().I copied and adjust it to what I want to achieve. I installed their package using composer.So here is my code:So when I run my code it throws this error:Now am wondering what is the next step for me, also what will be my*Please, if this question needs more explanation let me know in the comment instead of downvoting.Thanks.""",Joy,to achieve,1,215,224
